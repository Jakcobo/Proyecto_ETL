# Project ETL with Apache Airflow and Kafka

> A modular **ETL** (Extractâ€“Transformâ€“Load) solution combining batch processing and real-time streaming via **Kafka**, orchestrated with **Apache Airflow**, and data quality validation using **Great Expectations**.

---

## ğŸ“– Table of Contents

1. [Overview](#overview)
2. [Repository Structure](#repository-structure)
3. [Installation & Configuration](#installation--configuration)

   1. [Airflow Virtual Environment](#airflow-virtual-environment)
   2. [Dependencies](#dependencies)
4. [Docker Compose Deployment](#docker-compose-deployment)
5. [Apache Airflow](#apache-airflow)

   1. [Main DAGs](#main-dags)
6. [Data Validation (Great Expectations)](#data-validationâ€“great-expectations)
7. [Batch Processing (Batch ETL)](#batch-processingâ€“batch-etl)
8. [Real-Time Processing (Kafka Streaming)](#real-time-processingâ€“kafka-streaming)
9. [Jupyter Notebooks](#jupyter-notebooks)
10. [Cache Cleaning](#cache-cleaning)

---

## ğŸ“Œ Overview

This project implements a complete data pipeline that:

* **Extract**: Fetches raw data from various sources (CSV, API, Kafka).
* **Transform**: Cleans and enriches data using business rules and quality checks.
* **Load**: Inserts processed data into a dimensional model in **PostgreSQL**.
* **Orchestrate**: Manages task dependencies and scheduling with **Apache Airflow**.
* **Stream**: Processes data in real time using **Kafka** (producer/consumer).
* **Validate**: Verifies data quality with **Great Expectations**.

---

## ğŸ—‚ Repository Structure

```text
.
â”œâ”€â”€ .gitignore
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ clean_pycache.sh
â”œâ”€â”€ create_airflow_venv.txt
â”œâ”€â”€ dashboard_project01_final_.pdf
â”œâ”€â”€ md_03_project.pdf
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ producer.py
â”‚   â”œâ”€â”€ consumer.py
â”‚   â”œâ”€â”€ etl_utils.py
â”‚   â””â”€â”€ â€¦
â”œâ”€â”€ src_nico/
â”œâ”€â”€ scripts_ge/
â”‚   â””â”€â”€ run_validations.py
â”œâ”€â”€ great_expectations/
â”‚   â”œâ”€â”€ great_expectations.yml
â”‚   â””â”€â”€ suites/
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â”œâ”€â”€ etl_dag.py
â”‚       â””â”€â”€ kafka_streaming_dag.py
â”œâ”€â”€ Notebooks/
â”‚   â”œâ”€â”€ 01_DataLoad.ipynb
â”‚   â”œâ”€â”€ 02_EDA.ipynb
â”‚   â””â”€â”€ 03_StreamEDA.ipynb
â”œâ”€â”€ data/
â””â”€â”€ .vscode/
```

* **`.gitignore`**: Files and folders to ignore (envs, caches, credentials).
* **`docker-compose.yml`**: Defines Docker services (Zookeeper, Kafka, PostgreSQL, Airflow).
* **`clean_pycache.sh`**: Bash script to remove `__pycache__` directories and `.pyc` files.
* **`create_airflow_venv.txt`**: Steps to create and activate a virtual environment for Airflow.
* **`requirements.txt`**: Python dependencies (`pandas`, `sqlalchemy`, `kafka-python`, `apache-airflow`, `great_expectations`, etc.).
* **`src/`**: Main ETL and streaming code (producer, consumer, utilities).
* **`src_nico/`**: Experimental or alternative ETL modules.
* **`scripts_ge/`**: Scripts to run Great Expectations validations.
* **`great_expectations/`**: GE configuration, suites, and checkpoints.
* **`airflow/dags/`**: DAG definitions for batch and streaming workflows.
* **`Notebooks/`**: Jupyter Notebooks for EDA and pipeline testing.
* **`data/`**: Example input data (CSV, JSON).
* **`.vscode/`**: VS Code settings (linter, debugger, workspace).

---
## Create a folder named env/ with a file called .env:

```bashr
{
  "user": "your_user",
  "password": "your_password",
  "host": "your_host",
  "port": "your_port",
  "database": "airbnb"
}
```
## âš™ï¸ Installation & Configuration

### Airflow Virtual Environment

```bash
python3 -m venv venv_airflow
source venv_airflow/bin/activate
```

### Dependencies

With the virtual environment active, install Airflow and other packages:

```bash
pip install "apache-airflow==2.X.X" \
  --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.X.X/constraints-3.8.txt"

pip install -r requirements.txt
```

---

## ğŸ³ Docker Compose Deployment

Bring up all services:

```bash
docker-compose up -d
```

* The `etl-network` connects Kafka, PostgreSQL, and Airflow.
* Volumes persist database data and logs.

---

## ğŸ›© Apache Airflow

1. **Initialize the metadata database**

   ```bash
   airflow db init
   ```
2. **Create an admin user**

   ```bash
   airflow users create \
     --username admin \
     --firstname Admin \
     --lastname User \
     --role Admin \
     --email admin@example.com
   ```
3. **Start scheduler and webserver**

   ```bash
   airflow scheduler &
   airflow webserver --port 8080 &
   ```

### Main DAGs

* **`etl_dag.py`**
  Orchestrates batch extraction, transformation, and loading into the dimensional model.

* **`kafka_streaming_dag.py`**
  Manages continuous consumption from Kafka and incremental loading.

---

## âœ… Data Validation (Great Expectations)

* **Configuration**: `great_expectations/great_expectations.yml` and expectation suites.
* **Runner**: `scripts_ge/run_validations.py` executes all suites and generates an HTML report.

---

## ğŸ—ƒ Batch Processing (Batch ETL)

Defined in `airflow/dags/etl_dag.py`, includes:

1. **Extract**: Reads CSV/JSON files from `data/`.
2. **Transform**: Cleans, normalizes, and enriches data.
3. **Load**: Inserts records into dimension and fact tables in PostgreSQL.

---

## ğŸš€ Real-Time Processing (Kafka Streaming)

Core modules in `src/`:

* **`producer.py`**: Publishes messages to `etl_topic`.
* **`consumer.py`**: Consumes messages, transforms, and loads incrementally.
* **`etl_utils.py`**: Shared utilities for serialization, validation, and DB connections.

---

## ğŸ““ Jupyter Notebooks

* **`01_DataLoad.ipynb`** â€“ Extraction and loading demos.
* **`02_EDA.ipynb`** â€“ Batch data exploratory analysis.
* **`03_StreamEDA.ipynb`** â€“ Real-time visualization from Kafka.

---

## ğŸ§¹ Cache Cleaning

To remove compiled files:

```bash
bash clean_pycache.sh
```

## Authors
### `Jakcobo`
GitHub: github.com/Jakcobo
### `y4xulSC`
GitHub: github.com/y4xulSC
### `Mrsasayo`
GitHub: github.com/Mrsasayo
