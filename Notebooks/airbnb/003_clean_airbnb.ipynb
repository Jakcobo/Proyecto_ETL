{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 991,
   "id": "2a71cbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 1: Importación de Librerías\n",
    "import pandas as pd\n",
    "import psycopg2 \n",
    "from sqlalchemy import create_engine, text\n",
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "id": "acfcbc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:14,620 - INFO - Inicio del notebook de Limpieza de Datos (003_data_cleaning.ipynb).\n",
      "2025-05-17 02:01:14,621 - INFO - Configuraciones de Pandas para visualización aplicadas.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging configurado. Los logs se guardarán en /home/nicolas/Escritorio/proyecto ETL/develop/Notebooks/airbnb/003_clean_airbnb.log\n",
      "Celda 2: Configuraciones básicas de logging y Pandas realizadas.\n"
     ]
    }
   ],
   "source": [
    "# Celda 2: Configuraciones Básicas\n",
    "# El log se guardará en el mismo directorio que este notebook.\n",
    "LOG_FILE_PATH = '/home/nicolas/Escritorio/proyecto ETL/develop/Notebooks/airbnb/003_clean_airbnb.log'\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(LOG_FILE_PATH), # Nombre de archivo de log actualizado\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logging.info(\"Inicio del notebook de Limpieza de Datos (003_data_cleaning.ipynb).\")\n",
    "print(f\"Logging configurado. Los logs se guardarán en {LOG_FILE_PATH}\")\n",
    "\n",
    "# Configuraciones de Pandas para mejor visualización\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "logging.info(\"Configuraciones de Pandas para visualización aplicadas.\")\n",
    "print(\"Celda 2: Configuraciones básicas de logging y Pandas realizadas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 993,
   "id": "83545015",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:14,631 - INFO - Ruta del archivo .env: /home/nicolas/Escritorio/proyecto ETL/develop/env/.env\n",
      "2025-05-17 02:01:14,632 - INFO - Nombre de la tabla en PostgreSQL a leer: raw_airbnb\n",
      "2025-05-17 02:01:14,634 - INFO - Archivo .env encontrado y cargado desde /home/nicolas/Escritorio/proyecto ETL/develop/env/.env\n",
      "2025-05-17 02:01:14,635 - INFO - Variables de entorno para PostgreSQL cargadas correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Celda 3: Carga de Variables de Entorno para PostgreSQL\n",
    "ENV_FILE_PATH = '/home/nicolas/Escritorio/proyecto ETL/develop/env/.env'\n",
    "TABLE_NAME_RAW = 'raw_airbnb' # Tabla de donde leeremos los datos brutos\n",
    "\n",
    "logging.info(f\"Ruta del archivo .env: {ENV_FILE_PATH}\")\n",
    "logging.info(f\"Nombre de la tabla en PostgreSQL a leer: {TABLE_NAME_RAW}\")\n",
    "\n",
    "# Cargar variables de entorno\n",
    "if os.path.exists(ENV_FILE_PATH):\n",
    "    load_dotenv(ENV_FILE_PATH)\n",
    "    logging.info(f\"Archivo .env encontrado y cargado desde {ENV_FILE_PATH}\")\n",
    "else:\n",
    "    logging.error(f\"Archivo .env NO encontrado en {ENV_FILE_PATH}. Asegúrate de que la ruta es correcta.\")\n",
    "    raise FileNotFoundError(f\"Archivo .env no encontrado en {ENV_FILE_PATH}\")\n",
    "\n",
    "POSTGRES_USER = os.getenv('POSTGRES_USER')\n",
    "POSTGRES_PASSWORD = os.getenv('POSTGRES_PASSWORD')\n",
    "POSTGRES_HOST = os.getenv('POSTGRES_HOST')\n",
    "POSTGRES_PORT = os.getenv('POSTGRES_PORT')\n",
    "POSTGRES_DATABASE = os.getenv('POSTGRES_DATABASE')\n",
    "\n",
    "if not all([POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_HOST, POSTGRES_PORT, POSTGRES_DATABASE]):\n",
    "    logging.error(\"Una o más variables de entorno de PostgreSQL no están definidas. Revisa tu archivo .env y la carga.\")\n",
    "    raise ValueError(\"Faltan credenciales de PostgreSQL. Verifica el archivo .env y su carga.\")\n",
    "else:\n",
    "    logging.info(\"Variables de entorno para PostgreSQL cargadas correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "id": "117671c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:14,705 - INFO - Intentando conectar a la base de datos PostgreSQL y extraer datos.\n",
      "2025-05-17 02:01:14,707 - INFO - Consultando la tabla completa 'raw_airbnb'...\n",
      "WARNING:  la base de datos «airbnb» tiene una discordancia de versión de ordenamiento (“collation”)\n",
      "DETAIL:  La base de datos fue creada usando la versión de ordenamiento 2.31, pero el sistema operativo provee la versión 2.35.\n",
      "HINT:  Reconstruya todos los objetos en esta base de datos que usen el ordenamiento por omisión y ejecute ALTER DATABASE airbnb REFRESH COLLATION VERSION, o construya PostgreSQL con la versión correcta de la biblioteca.\n",
      "2025-05-17 02:01:15,766 - INFO - Datos extraídos exitosamente de la tabla 'raw_airbnb'.\n",
      "2025-05-17 02:01:15,766 - INFO - El DataFrame df_raw tiene 102599 filas y 26 columnas.\n",
      "2025-05-17 02:01:15,767 - INFO - Conexiones del motor de SQLAlchemy dispuestas (cerradas).\n",
      "2025-05-17 02:01:15,804 - INFO - Copia de df_raw creada como df_cleaning para el proceso de limpieza.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Celda 4: Datos extraídos de 'raw_airbnb'. Filas: 102599, Columnas: 26\n",
      "Conexiones de DB dispuestas.\n",
      "Copia df_cleaning creada a partir de df_raw.\n"
     ]
    }
   ],
   "source": [
    "# Celda 4: Extracción de Datos desde PostgreSQL\n",
    "df_raw = pd.DataFrame() # DataFrame para los datos brutos\n",
    "engine = None\n",
    "\n",
    "try:\n",
    "    logging.info(\"Intentando conectar a la base de datos PostgreSQL y extraer datos.\")\n",
    "    DATABASE_URL = f\"postgresql+psycopg2://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DATABASE}\"\n",
    "    engine = create_engine(DATABASE_URL)\n",
    "    \n",
    "    logging.info(f\"Consultando la tabla completa '{TABLE_NAME_RAW}'...\")\n",
    "    df_raw = pd.read_sql_table(TABLE_NAME_RAW, con=engine)\n",
    "    \n",
    "    logging.info(f\"Datos extraídos exitosamente de la tabla '{TABLE_NAME_RAW}'.\")\n",
    "    logging.info(f\"El DataFrame df_raw tiene {df_raw.shape[0]} filas y {df_raw.shape[1]} columnas.\")\n",
    "    print(f\"Celda 4: Datos extraídos de '{TABLE_NAME_RAW}'. Filas: {df_raw.shape[0]}, Columnas: {df_raw.shape[1]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logging.error(f\"Error al conectar a PostgreSQL o extraer datos de la tabla '{TABLE_NAME_RAW}': {e}\")\n",
    "    print(f\"Celda 4: Error al conectar a PostgreSQL o extraer datos - {e}\")\n",
    "finally:\n",
    "    if engine:\n",
    "        engine.dispose()\n",
    "        logging.info(\"Conexiones del motor de SQLAlchemy dispuestas (cerradas).\")\n",
    "        print(\"Conexiones de DB dispuestas.\")\n",
    "\n",
    "# Crear una copia para trabajar, manteniendo df_raw intacto\n",
    "df_cleaning = pd.DataFrame() # Predefinir por si df_raw está vacío\n",
    "if not df_raw.empty:\n",
    "    df_cleaning = df_raw.copy()\n",
    "    logging.info(\"Copia de df_raw creada como df_cleaning para el proceso de limpieza.\")\n",
    "    print(\"Copia df_cleaning creada a partir de df_raw.\")\n",
    "else:\n",
    "    logging.warning(\"df_raw está vacío, por lo que df_cleaning también lo estará. La limpieza no podrá proceder.\")\n",
    "    print(\"df_raw está vacío. No se puede proceder con la limpieza.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 995,
   "id": "28140d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:15,812 - INFO - Celda 5: Iniciando eliminación de filas duplicadas.\n",
      "2025-05-17 02:01:15,813 - INFO - Número de filas antes de eliminar duplicados: 102599\n",
      "2025-05-17 02:01:16,073 - INFO - Número de filas duplicadas encontradas en df_cleaning: 541\n",
      "2025-05-17 02:01:16,363 - INFO - Se eliminaron 541 filas duplicadas.\n",
      "2025-05-17 02:01:16,363 - INFO - Número de filas después de eliminar duplicados: 102058\n",
      "2025-05-17 02:01:16,364 - INFO - Índice del DataFrame df_cleaning reseteado.\n"
     ]
    }
   ],
   "source": [
    "# Celda 5: Eliminación de Filas Duplicadas\n",
    "logging.info(\"Celda 5: Iniciando eliminación de filas duplicadas.\")\n",
    "if not df_cleaning.empty:\n",
    "    initial_rows = len(df_cleaning)\n",
    "    logging.info(f\"Número de filas antes de eliminar duplicados: {initial_rows}\")\n",
    "    \n",
    "    # Verificar cuántos duplicados hay (esto ya lo hicimos en EDA, pero es bueno reconfirmar)\n",
    "    num_duplicados_actual = df_cleaning.duplicated().sum()\n",
    "    logging.info(f\"Número de filas duplicadas encontradas en df_cleaning: {num_duplicados_actual}\")\n",
    "\n",
    "    if num_duplicados_actual > 0:\n",
    "        # Eliminar duplicados, manteniendo la primera aparición por defecto\n",
    "        df_cleaning.drop_duplicates(inplace=True)\n",
    "        final_rows = len(df_cleaning)\n",
    "        rows_dropped = initial_rows - final_rows\n",
    "        logging.info(f\"Se eliminaron {rows_dropped} filas duplicadas.\")\n",
    "        logging.info(f\"Número de filas después de eliminar duplicados: {final_rows}\")\n",
    "    else:\n",
    "        logging.info(\"No se encontraron filas duplicadas para eliminar.\")\n",
    "    \n",
    "    # Resetear el índice después de eliminar filas\n",
    "    df_cleaning.reset_index(drop=True, inplace=True)\n",
    "    logging.info(\"Índice del DataFrame df_cleaning reseteado.\")\n",
    "else:\n",
    "    logging.warning(\"El DataFrame df_cleaning está vacío. No se pueden eliminar duplicados.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 996,
   "id": "e07f89b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:16,370 - INFO - Celda 4.1: Iniciando normalización de nombres de columnas en df_cleaning.\n",
      "2025-05-17 02:01:16,371 - INFO - Nombres de  columnas  originales: ['id', 'NAME', 'host id', 'host_identity_verified', 'host name', 'neighbourhood group', 'neighbourhood', 'lat', 'long', 'country', 'country code', 'instant_bookable', 'cancellation_policy', 'room type', 'Construction year', 'price', 'service fee', 'minimum nights', 'number of reviews', 'last review', 'reviews per month', 'review rate number', 'calculated host listings count', 'availability 365', 'house_rules', 'license']\n",
      "2025-05-17 02:01:16,372 - INFO - Nombres de columnas normalizados: ['id', 'name', 'host_id', 'host_identity_verified', 'host_name', 'neighbourhood_group', 'neighbourhood', 'lat', 'long', 'country', 'country_code', 'instant_bookable', 'cancellation_policy', 'room_type', 'construction_year', 'price', 'service_fee', 'minimum_nights', 'number_of_reviews', 'last_review', 'reviews_per_month', 'review_rate_number', 'calculated_host_listings_count', 'availability_365', 'house_rules', 'license']\n"
     ]
    }
   ],
   "source": [
    "# Celda 4.1: Normalización de Nombres de Columnas\n",
    "logging.info(\"Celda 4.1: Iniciando normalización de nombres de columnas en df_cleaning.\")\n",
    "\n",
    "if not df_cleaning.empty:\n",
    "    original_columns = df_cleaning.columns.tolist()\n",
    "    logging.info(f\"Nombres de  columnas  originales: {original_columns}\")\n",
    "    \n",
    "    # Normalizar: convertir a minúsculas y reemplazar espacios con guiones bajos\n",
    "    normalized_columns = [col.lower().replace(' ', '_') for col in original_columns]\n",
    "    \n",
    "    # También es buena idea reemplazar múltiples guiones bajos por uno solo, por si acaso\n",
    "    normalized_columns = [col.replace('__', '_') for col in normalized_columns]\n",
    "    \n",
    "    # Renombrar las columnas en el DataFrame\n",
    "    df_cleaning.columns = normalized_columns\n",
    "    \n",
    "    logging.info(f\"Nombres de columnas normalizados: {df_cleaning.columns.tolist()}\")\n",
    "\n",
    "\n",
    "else:\n",
    "    logging.warning(\"El DataFrame df_cleaning está vacío. No se pueden normalizar los nombres de las columnas.\")\n",
    "    print(\"El DataFrame df_cleaning está vacío. No se pueden normalizar los nombres de las columnas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 997,
   "id": "ae0a662c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:16,392 - INFO - Analizando valores únicos para columnas con 2 o menos valores únicos en df_cleaning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:16,455 - INFO - Valores únicos para 'host_identity_verified': ['unconfirmed' 'verified' None] (Total: 3)\n",
      "2025-05-17 02:01:16,602 - INFO - Valores únicos para 'instant_bookable': ['false' 'true' None] (Total: 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columna: 'host_identity_verified'\n",
      "  Número de valores únicos (incluyendo NaN si existe): 3\n",
      "  Valores únicos: ['unconfirmed' 'verified' None]\n",
      "  Conteo de cada valor único (incluyendo NaN):\n",
      "| host_identity_verified   |   count |\n",
      "|:-------------------------|--------:|\n",
      "| unconfirmed              |   50944 |\n",
      "| verified                 |   50825 |\n",
      "|                          |     289 |\n",
      "\n",
      "Columna: 'instant_bookable'\n",
      "  Número de valores únicos (incluyendo NaN si existe): 3\n",
      "  Valores únicos: ['false' 'true' None]\n",
      "  Conteo de cada valor único (incluyendo NaN):\n",
      "| instant_bookable   |   count |\n",
      "|:-------------------|--------:|\n",
      "| false              |   51186 |\n",
      "| true               |   50767 |\n",
      "|                    |     105 |\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Analizando valores únicos para columnas con 2 o menos valores únicos en df_cleaning.\")\n",
    "\n",
    "if not df_cleaning.empty:\n",
    "    for column in df_cleaning.columns:\n",
    "        unique_values = df_cleaning[column].unique()\n",
    "        num_unique_values = df_cleaning[column].nunique(dropna=False) # Contar NaN como un valor único si está presente\n",
    "\n",
    "        if num_unique_values == 3:\n",
    "            print(f\"\\nColumna: '{column}'\")\n",
    "            print(f\"  Número de valores únicos (incluyendo NaN si existe): {num_unique_values}\")\n",
    "            print(f\"  Valores únicos: {unique_values}\")\n",
    "            print(\"  Conteo de cada valor único (incluyendo NaN):\")\n",
    "            print(df_cleaning[column].value_counts(dropna=False).to_markdown())\n",
    "            logging.info(f\"Valores únicos para '{column}': {unique_values} (Total: {num_unique_values})\")\n",
    "        # else:\n",
    "            # logging.debug(f\"Columna '{column}' tiene {num_unique_values} valores únicos. No se muestra.\")\n",
    "else:\n",
    "    logging.warning(\"El DataFrame df_cleaning está vacío. No se puede analizar valores únicos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 998,
   "id": "09b94004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:16,757 - INFO - Celda 7: Iniciando transformación de tipos de datos - Columnas Booleanas.\n",
      "2025-05-17 02:01:16,759 - INFO - Procesando 'host_identity_verified'.\n",
      "2025-05-17 02:01:16,797 - INFO - Columna 'is_host_unconfirmed' creada. True si original es 'unconfirmed', False en otros casos.\n",
      "2025-05-17 02:01:16,836 - INFO - Columna 'is_host_verified' creada. True si original es 'verified', False en otros casos.\n",
      "2025-05-17 02:01:16,837 - INFO - Procesando 'instant_bookable'.\n",
      "2025-05-17 02:01:16,878 - INFO - Columna 'is_instant_bookable_false_policy' creada. True si original es 'FALSE', False en otros casos.\n",
      "2025-05-17 02:01:16,921 - INFO - Columna 'is_instant_bookable_true_policy' creada. True si original es 'TRUE', False en otros casos.\n",
      "2025-05-17 02:01:16,971 - INFO - Columnas originales ['host_identity_verified', 'instant_bookable'] eliminadas después de crear sus derivados booleanos.\n",
      "2025-05-17 02:01:16,971 - INFO - Transformaciones de tipo booleano completadas.\n"
     ]
    }
   ],
   "source": [
    "# Celda 7: Transformación de Tipos de Datos - BOOLEANAS\n",
    "logging.info(\"Celda 7: Iniciando transformación de tipos de datos - Columnas Booleanas.\")\n",
    "\n",
    "if not df_cleaning.empty:\n",
    "    try:\n",
    "\n",
    "        # --- host_identity_verified ---\n",
    "        # Columna original: 'host_identity_verified' (puede ser 'verified', 'unconfirmed', NaN, u otros strings)\n",
    "        if 'host_identity_verified' in df_cleaning.columns:\n",
    "            logging.info(\"Procesando 'host_identity_verified'.\")\n",
    "            \n",
    "            # Nueva columna: 'is_host_unconfirmed'\n",
    "            # True si host_identity_verified es 'unconfirmed', False para 'verified' o NaN/otros.\n",
    "            df_cleaning['is_host_unconfirmed'] = (df_cleaning['host_identity_verified'].astype(str).str.lower() == 'unconfirmed')\n",
    "            logging.info(\"Columna 'is_host_unconfirmed' creada. True si original es 'unconfirmed', False en otros casos.\")\n",
    "\n",
    "            # Nueva columna: 'is_host_verified'\n",
    "            # True si host_identity_verified es 'verified', False para 'unconfirmed' o NaN/otros.\n",
    "            df_cleaning['is_host_verified'] = (df_cleaning['host_identity_verified'].astype(str).str.lower() == 'verified')\n",
    "            logging.info(\"Columna 'is_host_verified' creada. True si original es 'verified', False en otros casos.\")\n",
    "\n",
    "        else:\n",
    "            logging.warning(\"Columna 'host_identity_verified' no encontrada. No se crearon 'is_host_unconfirmed' ni 'is_host_verified'.\")\n",
    "\n",
    "        # --- instant_bookable ---\n",
    "        # Columna original: 'instant_bookable' (puede ser 'TRUE', 'FALSE', NaN, u otros strings)\n",
    "        if 'instant_bookable' in df_cleaning.columns:\n",
    "            logging.info(\"Procesando 'instant_bookable'.\")\n",
    "\n",
    "            # Nueva columna: 'is_instant_bookable_false' (Tu lógica: si original es 'FALSE' -> True, sino False)\n",
    "            # Aquí \"False\" se refiere al valor string 'FALSE' en la columna original.\n",
    "            df_cleaning['is_instant_bookable_false_policy'] = (df_cleaning['instant_bookable'].astype(str).str.upper() == 'FALSE')\n",
    "            logging.info(\"Columna 'is_instant_bookable_false_policy' creada. True si original es 'FALSE', False en otros casos.\")\n",
    "            \n",
    "            # Nueva columna: 'is_instant_bookable_true' (Tu lógica: si original es 'TRUE' -> True, sino False)\n",
    "            df_cleaning['is_instant_bookable_true_policy'] = (df_cleaning['instant_bookable'].astype(str).str.upper() == 'TRUE')\n",
    "            logging.info(\"Columna 'is_instant_bookable_true_policy' creada. True si original es 'TRUE', False en otros casos.\")\n",
    "\n",
    "        else:\n",
    "            logging.warning(\"Columna 'instant_bookable' no encontrada. No se crearon 'is_instant_bookable_false_policy' ni 'is_instant_bookable_true_policy'.\")\n",
    "\n",
    "        # Eliminar las columnas originales después de la transformación\n",
    "        cols_to_drop_after_bool_transform = []\n",
    "        if 'host_identity_verified' in df_cleaning.columns:\n",
    "            cols_to_drop_after_bool_transform.append('host_identity_verified')\n",
    "        if 'instant_bookable' in df_cleaning.columns:\n",
    "            cols_to_drop_after_bool_transform.append('instant_bookable')\n",
    "        \n",
    "        if cols_to_drop_after_bool_transform:\n",
    "            df_cleaning.drop(columns=cols_to_drop_after_bool_transform, inplace=True)\n",
    "            logging.info(f\"Columnas originales {cols_to_drop_after_bool_transform} eliminadas después de crear sus derivados booleanos.\")        \n",
    "        logging.info(\"Transformaciones de tipo booleano completadas.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Ocurrió un error durante la transformación de tipos booleanos: {e}\")\n",
    "else:\n",
    "    logging.warning(\"El DataFrame df_cleaning está vacío. No se pueden realizar transformaciones de tipo booleano.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 999,
   "id": "f38de923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:16,978 - INFO - Analizando valores únicos para columnas con 2 o menos valores únicos en df_cleaning.\n",
      "2025-05-17 02:01:17,069 - INFO - Valores únicos para 'neighbourhood_group': ['Brooklyn' 'Manhattan' 'brookln' 'manhatan' 'Queens' None 'Staten Island'\n",
      " 'Bronx'] (Total: 8)\n",
      "2025-05-17 02:01:17,172 - INFO - Valores únicos para 'cancellation_policy': ['strict' 'moderate' 'flexible' None] (Total: 4)\n",
      "2025-05-17 02:01:17,225 - INFO - Valores únicos para 'room_type': ['Private room' 'Entire home/apt' 'Shared room' 'Hotel room'] (Total: 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columna: 'neighbourhood_group'\n",
      "  Número de valores únicos (incluyendo NaN si existe): 8\n",
      "  Valores únicos: ['Brooklyn' 'Manhattan' 'brookln' 'manhatan' 'Queens' None 'Staten Island'\n",
      " 'Bronx']\n",
      "  Conteo de cada valor único (incluyendo NaN):\n",
      "| neighbourhood_group   |   count |\n",
      "|:----------------------|--------:|\n",
      "| Manhattan             |   43557 |\n",
      "| Brooklyn              |   41630 |\n",
      "| Queens                |   13197 |\n",
      "| Bronx                 |    2694 |\n",
      "| Staten Island         |     949 |\n",
      "|                       |      29 |\n",
      "| manhatan              |       1 |\n",
      "| brookln               |       1 |\n",
      "\n",
      "Columna: 'cancellation_policy'\n",
      "  Número de valores únicos (incluyendo NaN si existe): 4\n",
      "  Valores únicos: ['strict' 'moderate' 'flexible' None]\n",
      "  Conteo de cada valor único (incluyendo NaN):\n",
      "| cancellation_policy   |   count |\n",
      "|:----------------------|--------:|\n",
      "| moderate              |   34162 |\n",
      "| strict                |   33929 |\n",
      "| flexible              |   33891 |\n",
      "|                       |      76 |\n",
      "\n",
      "Columna: 'room_type'\n",
      "  Número de valores únicos (incluyendo NaN si existe): 4\n",
      "  Valores únicos: ['Private room' 'Entire home/apt' 'Shared room' 'Hotel room']\n",
      "  Conteo de cada valor único (incluyendo NaN):\n",
      "| room_type       |   count |\n",
      "|:----------------|--------:|\n",
      "| Entire home/apt |   53429 |\n",
      "| Private room    |   46306 |\n",
      "| Shared room     |    2208 |\n",
      "| Hotel room      |     115 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:17,300 - INFO - Valores únicos para 'review_rate_number': [ 4.  5.  3. nan  2.  1.] (Total: 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columna: 'review_rate_number'\n",
      "  Número de valores únicos (incluyendo NaN si existe): 6\n",
      "  Valores únicos: [ 4.  5.  3. nan  2.  1.]\n",
      "  Conteo de cada valor único (incluyendo NaN):\n",
      "|   review_rate_number |   count |\n",
      "|---------------------:|--------:|\n",
      "|                    5 |   23251 |\n",
      "|                    4 |   23200 |\n",
      "|                    3 |   23130 |\n",
      "|                    2 |   22972 |\n",
      "|                    1 |    9186 |\n",
      "|                  nan |     319 |\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Analizando valores únicos para columnas con 2 o menos valores únicos en df_cleaning.\")\n",
    "\n",
    "if not df_cleaning.empty:\n",
    "    for column in df_cleaning.columns:\n",
    "        unique_values = df_cleaning[column].unique()\n",
    "        num_unique_values = df_cleaning[column].nunique(dropna=False) # Contar NaN como un valor único si está presente\n",
    "\n",
    "        if num_unique_values >= 3 and num_unique_values <=20:\n",
    "            print(f\"\\nColumna: '{column}'\")\n",
    "            print(f\"  Número de valores únicos (incluyendo NaN si existe): {num_unique_values}\")\n",
    "            print(f\"  Valores únicos: {unique_values}\")\n",
    "            print(\"  Conteo de cada valor único (incluyendo NaN):\")\n",
    "            print(df_cleaning[column].value_counts(dropna=False).to_markdown())\n",
    "            logging.info(f\"Valores únicos para '{column}': {unique_values} (Total: {num_unique_values})\")\n",
    "        # else:\n",
    "            # logging.debug(f\"Columna '{column}' tiene {num_unique_values} valores únicos. No se muestra.\")\n",
    "else:\n",
    "    logging.warning(\"El DataFrame df_cleaning está vacío. No se puede analizar valores únicos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1000,
   "id": "2c0b01b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:17,338 - INFO - Celda 8: Iniciando transformación de tipos de datos - Columnas Categóricas.\n",
      "2025-05-17 02:01:17,338 - INFO - Columnas a convertir a categórico: ['neighbourhood_group', 'cancellation_policy', 'room_type', 'review_rate_number', 'neighbourhood']\n",
      "2025-05-17 02:01:17,434 - INFO - Columna 'neighbourhood_group' (tipo original: object) convertida a category.\n",
      "2025-05-17 02:01:17,523 - INFO - Columna 'cancellation_policy' (tipo original: object) convertida a category.\n",
      "2025-05-17 02:01:17,615 - INFO - Columna 'room_type' (tipo original: object) convertida a category.\n",
      "2025-05-17 02:01:17,617 - INFO - Columna 'review_rate_number' (tipo original: float64) convertida a category.\n",
      "2025-05-17 02:01:17,618 - INFO - La columna 'review_rate_number' contiene valores NaN/pd.NA además de sus categorías.\n",
      "2025-05-17 02:01:17,710 - INFO - Columna 'neighbourhood' (tipo original: object) convertida a category.\n",
      "2025-05-17 02:01:17,711 - INFO - Transformaciones de tipo categórico completadas.\n"
     ]
    }
   ],
   "source": [
    "# Celda 8: Transformación de Tipos de Datos - CATEGÓRICAS\n",
    "logging.info(\"Celda 8: Iniciando transformación de tipos de datos - Columnas Categóricas.\")\n",
    "\n",
    "if not df_cleaning.empty:\n",
    "    try:\n",
    "        cols_to_category = [\n",
    "            'neighbourhood_group',\n",
    "            'cancellation_policy',\n",
    "            'room_type',\n",
    "            'review_rate_number',\n",
    "            'neighbourhood'\n",
    "        ]\n",
    "        \n",
    "        logging.info(f\"Columnas a convertir a categórico: {cols_to_category}\")\n",
    "        \n",
    "        for col_name in cols_to_category:\n",
    "            if col_name in df_cleaning.columns:\n",
    "                original_dtype = df_cleaning[col_name].dtype\n",
    "                if pd.api.types.is_object_dtype(original_dtype) or pd.api.types.is_string_dtype(original_dtype):\n",
    "                    df_cleaning[col_name] = df_cleaning[col_name].astype(str).str.strip().replace({'nan': pd.NA, '': pd.NA})\n",
    "                elif pd.api.types.is_numeric_dtype(original_dtype) and col_name == 'review_rate_number':\n",
    "                    pass\n",
    "\n",
    "                # Convertir a tipo 'category'\n",
    "                df_cleaning[col_name] = df_cleaning[col_name].astype('category')\n",
    "                \n",
    "                logging.info(f\"Columna '{col_name}' (tipo original: {original_dtype}) convertida a category.\")\n",
    "                if df_cleaning[col_name].isna().any():\n",
    "                    logging.info(f\"La columna '{col_name}' contiene valores NaN/pd.NA además de sus categorías.\")\n",
    "\n",
    "            else:\n",
    "                logging.warning(f\"Columna '{col_name}' no encontrada en df_cleaning. No se pudo convertir a category.\")\n",
    "        \n",
    "        logging.info(\"Transformaciones de tipo categórico completadas.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Ocurrió un error durante la transformación de tipos categóricos: {e}\")\n",
    "else:\n",
    "    logging.warning(\"El DataFrame df_cleaning está vacío. No se pueden realizar transformaciones de tipo categórico.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1001,
   "id": "ef113cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:17,718 - INFO - Celda 8.1: Calculando el conteo de valores únicos para columnas no categóricas y no booleanas.\n",
      "2025-05-17 02:01:17,897 - INFO - Tabla resumen de conteo de valores únicos generada y mostrada.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Conteo de Valores Únicos Totales (Excluyendo Categóricas y Booleanas) ---\n",
      "| Columna                        |   Total_Valores_Unicos | Tipo_Dato   |\n",
      "|:-------------------------------|-----------------------:|:------------|\n",
      "| id                             |                 102058 | int64       |\n",
      "| host_id                        |                 102057 | int64       |\n",
      "| name                           |                  61282 | object      |\n",
      "| lat                            |                  21992 | float64     |\n",
      "| long                           |                  17775 | float64     |\n",
      "| host_name                      |                  13191 | object      |\n",
      "| last_review                    |                   2478 | object      |\n",
      "| house_rules                    |                   1977 | object      |\n",
      "| price                          |                   1152 | object      |\n",
      "| reviews_per_month              |                   1017 | float64     |\n",
      "| number_of_reviews              |                    477 | float64     |\n",
      "| availability_365               |                    439 | float64     |\n",
      "| service_fee                    |                    232 | object      |\n",
      "| minimum_nights                 |                    154 | float64     |\n",
      "| calculated_host_listings_count |                     79 | float64     |\n",
      "| construction_year              |                     21 | float64     |\n",
      "| country                        |                      2 | object      |\n",
      "| country_code                   |                      2 | object      |\n",
      "| license                        |                      2 | object      |\n"
     ]
    }
   ],
   "source": [
    "# Celda 8.1: Conteo de Valores Únicos para Columnas No Categóricas y No Booleanas\n",
    "logging.info(\"Celda 8.1: Calculando el conteo de valores únicos para columnas no categóricas y no booleanas.\")\n",
    "print(\"\\n--- Conteo de Valores Únicos Totales (Excluyendo Categóricas y Booleanas) ---\")\n",
    "\n",
    "if not df_cleaning.empty:\n",
    "    unique_counts_summary = []\n",
    "    \n",
    "    columns_to_check = df_cleaning.select_dtypes(exclude=['category', 'bool', 'boolean']).columns\n",
    "    \n",
    "    if not columns_to_check.empty:\n",
    "        for column in columns_to_check:\n",
    "            try:\n",
    "                # nunique(dropna=False) cuenta NaN/NaT como un valor único si está presente\n",
    "                num_unique = df_cleaning[column].nunique(dropna=False)\n",
    "                unique_counts_summary.append({'Columna': column, 'Total_Valores_Unicos': num_unique, 'Tipo_Dato': df_cleaning[column].dtype})\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error al calcular valores únicos para la columna '{column}': {e}\")\n",
    "                unique_counts_summary.append({'Columna': column, 'Total_Valores_Unicos': 'Error', 'Tipo_Dato': df_cleaning[column].dtype})\n",
    "\n",
    "        if unique_counts_summary:\n",
    "            df_unique_summary = pd.DataFrame(unique_counts_summary)\n",
    "            df_unique_summary_sorted = df_unique_summary.sort_values(by='Total_Valores_Unicos', ascending=False) # Opcional: ordenar\n",
    "            \n",
    "            print(df_unique_summary_sorted.to_markdown(index=False))\n",
    "            logging.info(\"Tabla resumen de conteo de valores únicos generada y mostrada.\")\n",
    "        else:\n",
    "            print(\"No se procesaron columnas para el resumen de valores únicos (podría ser un error o no hay columnas que cumplan el criterio).\")\n",
    "            logging.info(\"No se procesaron columnas para el resumen de valores únicos.\")\n",
    "            \n",
    "    else:\n",
    "        print(\"No hay columnas que no sean de tipo 'category' o 'boolean'/'bool' para analizar.\")\n",
    "        logging.info(\"No se encontraron columnas no categóricas/booleanas para el conteo de únicos.\")\n",
    "        \n",
    "else:\n",
    "    logging.warning(\"El DataFrame df_cleaning está vacío. No se puede generar el resumen de valores únicos.\")\n",
    "    print(\"El DataFrame df_cleaning está vacío.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1002,
   "id": "2e69a832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:17,908 - INFO - Celda 9: Iniciando transformación de tipos de datos - Numéricos y Fecha.\n",
      "2025-05-17 02:01:17,909 - INFO - Convirtiendo 'last_review' a datetime.\n",
      "2025-05-17 02:01:17,947 - INFO - Limpiando y convirtiendo 'price' a float como 'price'.\n",
      "2025-05-17 02:01:18,048 - INFO - Limpiando y convirtiendo 'service_fee' a float como 'service_fee'.\n",
      "2025-05-17 02:01:18,149 - INFO - Convirtiendo 'number_of_reviews' a Int64 (nullable integer).\n",
      "2025-05-17 02:01:18,155 - INFO - Convirtiendo 'availability_365' a Int64 (nullable integer).\n",
      "2025-05-17 02:01:18,161 - INFO - Convirtiendo 'minimum_nights' a Int64 (nullable integer).\n",
      "2025-05-17 02:01:18,166 - INFO - Convirtiendo 'calculated_host_listings_count' a Int64 (nullable integer).\n",
      "2025-05-17 02:01:18,171 - INFO - Convirtiendo 'construction_year' a Int64 (nullable integer).\n",
      "2025-05-17 02:01:18,177 - INFO - Transformaciones de tipos numéricos y de fecha completadas.\n"
     ]
    }
   ],
   "source": [
    "# Celda 9: Transformación de Tipos de Datos - Numéricos y Fecha\n",
    "logging.info(\"Celda 9: Iniciando transformación de tipos de datos - Numéricos y Fecha.\")\n",
    "\n",
    "if not df_cleaning.empty:\n",
    "    try:\n",
    "        # --- Transformación de Fecha ---\n",
    "        col_last_review = 'last_review' # Nombre normalizado\n",
    "        if col_last_review in df_cleaning.columns:\n",
    "            logging.info(f\"Convirtiendo '{col_last_review}' a datetime.\")\n",
    "            original_non_nat = df_cleaning[col_last_review].notna().sum()\n",
    "            df_cleaning[col_last_review] = pd.to_datetime(df_cleaning[col_last_review], format='%m/%d/%Y', errors='coerce')\n",
    "            coerced_nat_count = df_cleaning[col_last_review].isna().sum() - (len(df_cleaning) - original_non_nat)\n",
    "            if coerced_nat_count > 0:\n",
    "                logging.warning(f\"Columna '{col_last_review}': {coerced_nat_count} valores no pudieron ser convertidos a datetime y se establecieron como NaT.\")\n",
    "        else:\n",
    "            logging.warning(f\"Columna '{col_last_review}' no encontrada.\")\n",
    "\n",
    "        # --- Transformaciones Numéricas (desde Object a Float) ---\n",
    "        cols_object_to_float = {\n",
    "            'price': 'price', # El nombre normalizado ya es 'price'\n",
    "            'service_fee': 'service_fee' # El nombre normalizado ya es 'service_fee'\n",
    "        }\n",
    "\n",
    "        for original_col_name, target_col_name in cols_object_to_float.items():\n",
    "            if original_col_name in df_cleaning.columns:\n",
    "                logging.info(f\"Limpiando y convirtiendo '{original_col_name}' a float como '{target_col_name}'.\")\n",
    "                original_non_nan_text = df_cleaning[original_col_name].notna().sum()\n",
    "                \n",
    "                # Limpieza de strings\n",
    "                cleaned_series = df_cleaning[original_col_name].astype(str) \\\n",
    "                    .str.replace('$', '', regex=False) \\\n",
    "                    .str.replace(',', '', regex=False) \\\n",
    "                    .str.strip()\n",
    "                \n",
    "                # Convertir a numérico\n",
    "                df_cleaning[target_col_name] = pd.to_numeric(cleaned_series, errors='coerce')\n",
    "                \n",
    "                coerced_nan_count_text = df_cleaning[target_col_name].isna().sum() - (len(df_cleaning) - original_non_nan_text)\n",
    "                if coerced_nan_count_text > 0:\n",
    "                     logging.warning(f\"Columna '{target_col_name}': {coerced_nan_count_text} valores no pudieron ser convertidos a float y se establecieron como NaN después de la limpieza.\")\n",
    "            else:\n",
    "                logging.warning(f\"Columna original '{original_col_name}' no encontrada para convertir a float.\")\n",
    "\n",
    "        # --- Transformaciones Numéricas (desde Float64 a Int64) ---\n",
    "        cols_float_to_int64 = [\n",
    "            'number_of_reviews', \n",
    "            'availability_365',\n",
    "            'minimum_nights',\n",
    "            'calculated_host_listings_count',\n",
    "            'construction_year'\n",
    "        ]\n",
    "\n",
    "        for col_name in cols_float_to_int64:\n",
    "            if col_name in df_cleaning.columns:\n",
    "                if pd.api.types.is_float_dtype(df_cleaning[col_name]) or pd.api.types.is_integer_dtype(df_cleaning[col_name]): # Aceptar si ya es Int64\n",
    "                    logging.info(f\"Convirtiendo '{col_name}' a Int64 (nullable integer).\")\n",
    "                    df_cleaning[col_name] = df_cleaning[col_name].astype('Int64')\n",
    "                else:\n",
    "                    logging.warning(f\"Columna '{col_name}' no es de tipo float o int. Tipo actual: {df_cleaning[col_name].dtype}. No se convirtió a Int64.\")\n",
    "            else:\n",
    "                logging.warning(f\"Columna '{col_name}' no encontrada para convertir a Int64.\")\n",
    "\n",
    "        logging.info(\"Transformaciones de tipos numéricos y de fecha completadas.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Ocurrió un error durante la transformación de tipos numéricos y de fecha: {e}\")\n",
    "else:\n",
    "    logging.warning(\"El DataFrame df_cleaning está vacío. No se pueden realizar transformaciones numéricas ni de fecha.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "id": "9e5ff453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:18,184 - INFO - Celda 6: Intentando eliminar la columna 'license' (normalizada).\n",
      "2025-05-17 02:01:18,217 - INFO - Columna 'license' eliminada exitosamente.\n",
      "2025-05-17 02:01:18,218 - INFO - Dimensiones de df_cleaning después de eliminar 'license': (102058, 27)\n"
     ]
    }
   ],
   "source": [
    "# Celda 9: Eliminación de la Columna 'license'\n",
    "logging.info(\"Celda 6: Intentando eliminar la columna 'license' (normalizada).\")\n",
    "if not df_cleaning.empty:\n",
    "\n",
    "    column_to_drop_normalized = 'license'\n",
    "    \n",
    "    if column_to_drop_normalized in df_cleaning.columns:\n",
    "        df_cleaning.drop(columns=[column_to_drop_normalized], inplace=True)\n",
    "        logging.info(f\"Columna '{column_to_drop_normalized}' eliminada exitosamente.\")\n",
    "        logging.info(f\"Dimensiones de df_cleaning después de eliminar '{column_to_drop_normalized}': {df_cleaning.shape}\")\n",
    "    else:\n",
    "        logging.warning(f\"La columna '{column_to_drop_normalized}' no se encontró en df_cleaning. No se realizó ninguna eliminación.\")\n",
    "else:\n",
    "    logging.warning(\"El DataFrame df_cleaning está vacío. No se puede eliminar la columna.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "id": "d685bcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:18,226 - INFO - Celda 10.1: Identificando valores fuera de rango en 'availability_365'.\n",
      "2025-05-17 02:01:18,228 - INFO - Analizando columna 'availability_365' (Tipo actual: Int64).\n",
      "2025-05-17 02:01:18,240 - INFO - Para 'availability_365':\n",
      "2025-05-17 02:01:18,241 - INFO -   - Número de valores < 0: 431\n",
      "2025-05-17 02:01:18,242 - INFO -     Valores únicos encontrados < 0: [-9, -10, -2, -1, -6, -4, -8, -5, -3, -7]\n",
      "2025-05-17 02:01:18,243 - INFO -   - Número de valores > 365: 2754\n",
      "2025-05-17 02:01:18,244 - INFO -     Valores únicos encontrados > 365 (hasta 20): [374, 375, 372, 383, 411, 416, 405, 393, 400, 417]\n"
     ]
    }
   ],
   "source": [
    "# Celda 10.1: Identificación de Valores Fuera de Rango en 'availability_365'\n",
    "logging.info(\"Celda 10.1: Identificando valores fuera de rango en 'availability_365'.\")\n",
    "\n",
    "if not df_cleaning.empty:\n",
    "    col_name = 'availability_365' # Nombre ya normalizado\n",
    "    \n",
    "    if col_name in df_cleaning.columns:\n",
    "        # Verificar si la columna es numérica, si no, intentar convertir (aunque ya debería serlo por Celda 9)\n",
    "        if not pd.api.types.is_numeric_dtype(df_cleaning[col_name]):\n",
    "            logging.warning(f\"La columna '{col_name}' no es de tipo numérico (tipo: {df_cleaning[col_name].dtype}). Intentando convertir a Int64.\")\n",
    "            try:\n",
    "                df_cleaning[col_name] = pd.to_numeric(df_cleaning[col_name], errors='coerce').astype('Int64')\n",
    "                logging.info(f\"Columna '{col_name}' convertida a Int64 para el análisis.\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"No se pudo convertir '{col_name}' a Int64. Omitiendo análisis para esta columna. Error: {e}\")\n",
    "        \n",
    "        # Proceder solo si la columna es numérica (o se pudo convertir)\n",
    "        if pd.api.types.is_numeric_dtype(df_cleaning[col_name]):\n",
    "            logging.info(f\"Analizando columna '{col_name}' (Tipo actual: {df_cleaning[col_name].dtype}).\")\n",
    "\n",
    "            # Identificar valores < 0\n",
    "            valores_menores_a_cero = df_cleaning[df_cleaning[col_name] < 0][col_name]\n",
    "            count_menores_a_cero = len(valores_menores_a_cero)\n",
    "            \n",
    "            # Límite superior realista (365, ya que el máximo original era muy alto)\n",
    "            limite_superior_realista = 365 \n",
    "            valores_mayores_limite = df_cleaning[df_cleaning[col_name] > limite_superior_realista][col_name]\n",
    "            count_mayores_limite = len(valores_mayores_limite)\n",
    "\n",
    "            logging.info(f\"Para '{col_name}':\")\n",
    "            logging.info(f\"  - Número de valores < 0: {count_menores_a_cero}\")\n",
    "            if count_menores_a_cero > 0:\n",
    "                unique_negatives = valores_menores_a_cero.unique().tolist()\n",
    "                logging.info(f\"    Valores únicos encontrados < 0: {unique_negatives}\")\n",
    "            \n",
    "            logging.info(f\"  - Número de valores > {limite_superior_realista}: {count_mayores_limite}\")\n",
    "            if count_mayores_limite > 0:\n",
    "                unique_positives_outliers = valores_mayores_limite.unique()[:10].tolist() # Mostrar hasta 20 únicos\n",
    "                logging.info(f\"    Valores únicos encontrados > {limite_superior_realista} (hasta 20): {unique_positives_outliers}\")\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "    else:\n",
    "        logging.warning(f\"Columna '{col_name}' no encontrada en df_cleaning.\")\n",
    "else:\n",
    "    logging.warning(\"El DataFrame df_cleaning está vacío. No se puede procesar 'availability_365'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1005,
   "id": "a4975644",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:18,251 - INFO - Celda 10.2: Aplicando corrección (clipping) a 'availability_365'.\n",
      "2025-05-17 02:01:18,256 - INFO - Valores en 'availability_365' corregidos (clipping entre 1 y 365).\n",
      "2025-05-17 02:01:18,259 - INFO - Nuevo rango para 'availability_365' después de la corrección: Min=1, Max=365\n"
     ]
    }
   ],
   "source": [
    "# Celda 10.2: Aplicación de Corrección (Clipping) a 'availability_365' y Verificación\n",
    "logging.info(\"Celda 10.2: Aplicando corrección (clipping) a 'availability_365'.\")\n",
    "\n",
    "if not df_cleaning.empty:\n",
    "    col_name = 'availability_365'\n",
    "    \n",
    "    if col_name in df_cleaning.columns and pd.api.types.is_numeric_dtype(df_cleaning[col_name]):\n",
    "        limite_superior_realista = 365\n",
    "\n",
    "        df_cleaning[col_name] = df_cleaning[col_name].clip(lower=1, upper=limite_superior_realista)\n",
    "        \n",
    "        logging.info(f\"Valores en '{col_name}' corregidos (clipping entre 1 y {limite_superior_realista}).\")\n",
    "\n",
    "        new_min = df_cleaning[col_name].min()\n",
    "        new_max = df_cleaning[col_name].max()\n",
    "        \n",
    "        logging.info(f\"Nuevo rango para '{col_name}' después de la corrección: Min={new_min}, Max={new_max}\")\n",
    "        \n",
    "    elif col_name not in df_cleaning.columns:\n",
    "        logging.warning(f\"Columna '{col_name}' no encontrada en df_cleaning para aplicar corrección.\")\n",
    "    elif not pd.api.types.is_numeric_dtype(df_cleaning[col_name]):\n",
    "        logging.warning(f\"Columna '{col_name}' no es de tipo numérico. No se aplicó la corrección de clipping.\")\n",
    "else:\n",
    "    logging.warning(\"El DataFrame df_cleaning está vacío. No se puede aplicar corrección a 'availability_365'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1006,
   "id": "9041162e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:18,269 - INFO - Celda 11.1: Identificando valores fuera de rango en 'minimum_nights'.\n",
      "2025-05-17 02:01:18,270 - INFO - Analizando columna 'minimum_nights' (Tipo actual: Int64).\n",
      "2025-05-17 02:01:18,273 - INFO - Para 'minimum_nights':\n",
      "2025-05-17 02:01:18,274 - INFO -   - Número de valores < 1: 13\n",
      "2025-05-17 02:01:18,276 - INFO -     Valores únicos encontrados < 1: [-10, -5, -1, -12, -2, -3, -1223, -365, -200, -125]\n",
      "2025-05-17 02:01:18,277 - INFO -   - Número de valores > 365: 35\n",
      "2025-05-17 02:01:18,277 - INFO -     Valores únicos encontrados > 365 (hasta 20): [371, 366, 399, 452, 3455, 398, 370, 1000, 1250, 500]\n"
     ]
    }
   ],
   "source": [
    "# Celda 11.1: Identificación de Valores Fuera de Rango en 'minimum_nights'\n",
    "logging.info(\"Celda 11.1: Identificando valores fuera de rango en 'minimum_nights'.\")\n",
    "\n",
    "if not df_cleaning.empty:\n",
    "    col_name_mn = 'minimum_nights' # Nombre ya normalizado\n",
    "    \n",
    "    if col_name_mn in df_cleaning.columns:\n",
    "        # Verificar si la columna es numérica (ya debería ser Int64 por Celda 9)\n",
    "        if not pd.api.types.is_numeric_dtype(df_cleaning[col_name_mn]):\n",
    "            logging.warning(f\"La columna '{col_name_mn}' no es de tipo numérico (tipo: {df_cleaning[col_name_mn].dtype}). Intentando convertir a Int64.\")\n",
    "            try:\n",
    "                df_cleaning[col_name_mn] = pd.to_numeric(df_cleaning[col_name_mn], errors='coerce').astype('Int64')\n",
    "                logging.info(f\"Columna '{col_name_mn}' convertida a Int64 para el análisis.\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"No se pudo convertir '{col_name_mn}' a Int64. Omitiendo análisis para esta columna. Error: {e}\")\n",
    "        \n",
    "        # Proceder solo si la columna es numérica\n",
    "        if pd.api.types.is_numeric_dtype(df_cleaning[col_name_mn]):\n",
    "            logging.info(f\"Analizando columna '{col_name_mn}' (Tipo actual: {df_cleaning[col_name_mn].dtype}).\")\n",
    "\n",
    "            limite_inferior_mn = 1\n",
    "            valores_menores_limite_inf = df_cleaning[df_cleaning[col_name_mn] < limite_inferior_mn][col_name_mn]\n",
    "            count_menores_limite_inf = len(valores_menores_limite_inf)\n",
    "            \n",
    "            limite_superior_mn = 365 \n",
    "            valores_mayores_limite_sup = df_cleaning[df_cleaning[col_name_mn] > limite_superior_mn][col_name_mn]\n",
    "            count_mayores_limite_sup = len(valores_mayores_limite_sup)\n",
    "\n",
    "            logging.info(f\"Para '{col_name_mn}':\")\n",
    "            logging.info(f\"  - Número de valores < {limite_inferior_mn}: {count_menores_limite_inf}\")\n",
    "            if count_menores_limite_inf > 0:\n",
    "                unique_negatives_or_zero = valores_menores_limite_inf.unique().tolist()\n",
    "                logging.info(f\"    Valores únicos encontrados < {limite_inferior_mn}: {unique_negatives_or_zero}\")\n",
    "            \n",
    "            logging.info(f\"  - Número de valores > {limite_superior_mn}: {count_mayores_limite_sup}\")\n",
    "            if count_mayores_limite_sup > 0:\n",
    "                unique_positives_outliers_mn = valores_mayores_limite_sup.unique()[:10].tolist() # Mostrar hasta 20 únicos\n",
    "                logging.info(f\"    Valores únicos encontrados > {limite_superior_mn} (hasta 20): {unique_positives_outliers_mn}\")\n",
    "        else:\n",
    "            pass # Mensaje de error ya manejado en el bloque try-except de conversión\n",
    "            \n",
    "    else:\n",
    "        logging.warning(f\"Columna '{col_name_mn}' no encontrada en df_cleaning.\")\n",
    "else:\n",
    "    logging.warning(\"El DataFrame df_cleaning está vacío. No se puede procesar '{col_name_mn}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1007,
   "id": "096044dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:18,286 - INFO - Celda 11.2: Aplicando corrección (clipping) a 'minimum_nights'.\n",
      "2025-05-17 02:01:18,294 - INFO - Valores en 'minimum_nights' corregidos (clipping entre 1 y 365).\n",
      "2025-05-17 02:01:18,295 - INFO - Nuevo rango para 'minimum_nights' después de la corrección: Min=1, Max=365\n"
     ]
    }
   ],
   "source": [
    "# Celda 11.2: Aplicación de Corrección (Clipping) a 'minimum_nights' y Verificación\n",
    "logging.info(\"Celda 11.2: Aplicando corrección (clipping) a 'minimum_nights'.\")\n",
    "\n",
    "if not df_cleaning.empty:\n",
    "    col_name_mn = 'minimum_nights' # Nombre ya normalizado\n",
    "    \n",
    "    if col_name_mn in df_cleaning.columns and pd.api.types.is_numeric_dtype(df_cleaning[col_name_mn]):\n",
    "        limite_inferior_mn = 1\n",
    "        limite_superior_mn = 365 \n",
    "        \n",
    "        # Aplicar clip:\n",
    "        df_cleaning[col_name_mn] = df_cleaning[col_name_mn].clip(lower=limite_inferior_mn, upper=limite_superior_mn)\n",
    "        \n",
    "        logging.info(f\"Valores en '{col_name_mn}' corregidos (clipping entre {limite_inferior_mn} y {limite_superior_mn}).\")\n",
    "\n",
    "        # Verificar después de la corrección\n",
    "        new_min_mn = df_cleaning[col_name_mn].min()\n",
    "        new_max_mn = df_cleaning[col_name_mn].max()\n",
    "        \n",
    "        logging.info(f\"Nuevo rango para '{col_name_mn}' después de la corrección: Min={new_min_mn}, Max={new_max_mn}\")\n",
    "        \n",
    "    elif col_name_mn not in df_cleaning.columns:\n",
    "        logging.warning(f\"Columna '{col_name_mn}' no encontrada en df_cleaning para aplicar corrección.\")\n",
    "    elif not pd.api.types.is_numeric_dtype(df_cleaning[col_name_mn]): # Este caso debería ser cubierto por la Celda 11.1 si la conversión falló\n",
    "        logging.warning(f\"Columna '{col_name_mn}' no es de tipo numérico. No se aplicó la corrección de clipping.\")\n",
    "else:\n",
    "    logging.warning(\"El DataFrame df_cleaning está vacío. No se puede aplicar corrección a '{col_name_mn}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1008,
   "id": "d3735402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                                |   count |           mean |           std |            min |            25% |            50% |            75% |            max |\n",
      "|:-------------------------------|--------:|---------------:|--------------:|---------------:|---------------:|---------------:|---------------:|---------------:|\n",
      "| id                             |  102058 |    2.91844e+07 |   1.62717e+07 |    1.00125e+06 |    1.50929e+07 |    2.91844e+07 |    4.32759e+07 |    5.73674e+07 |\n",
      "| host_id                        |  102058 |    4.92674e+10 |   2.85374e+10 |    1.23601e+08 |    2.45992e+10 |    4.91287e+10 |    7.40062e+10 |    9.87631e+10 |\n",
      "| lat                            |  102050 |   40.7281      |   0.0558524   |   40.4998      |   40.6887      |   40.7223      |   40.7628      |   40.917       |\n",
      "| long                           |  102050 |  -73.9497      |   0.0495016   |  -74.2498      |  -73.9826      |  -73.9544      |  -73.9323      |  -73.7052      |\n",
      "| construction_year              |  101844 | 2012.49        |   5.76584     | 2003           | 2007           | 2012           | 2017           | 2022           |\n",
      "| price                          |  101811 |  625.356       | 331.673       |   50           |  340           |  625           |  913           | 1200           |\n",
      "| service_fee                    |  101785 |  125.039       |  66.3259      |   10           |   68           |  125           |  183           |  240           |\n",
      "| minimum_nights                 |  101658 |    7.96793     |  18.3028      |    1           |    2           |    3           |    5           |  365           |\n",
      "| number_of_reviews              |  101875 |   27.5179      |  49.5717      |    0           |    1           |    7           |   31           | 1024           |\n",
      "| reviews_per_month              |   86240 |    1.37541     |   1.74802     |    0.01        |    0.22        |    0.74        |    2.01        |   90           |\n",
      "| calculated_host_listings_count |  101739 |    7.93694     |  32.2664      |    1           |    1           |    1           |    2           |  332           |\n",
      "| availability_365               |  101610 |  140.439       | 133.191       |    1           |    3           |   96           |  268           |  365           |\n"
     ]
    }
   ],
   "source": [
    "# Mostrar el resumen estadístico de las columnas numéricas de df_cleaning como tabla markdown\n",
    "numeric_summary = df_cleaning.select_dtypes(include=np.number).describe().T\n",
    "print(numeric_summary.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "id": "154d7a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:18,411 - INFO - Celda 12.1: Iniciando imputación de nulos para 'lat' y 'long'.\n",
      "2025-05-17 02:01:18,413 - INFO - Nulos en 'lat' ANTES de la imputación: 8\n",
      "2025-05-17 02:01:18,435 - INFO - Nulos en 'lat' DESPUÉS de la imputación: 0\n",
      "2025-05-17 02:01:18,436 - INFO - Nulos en 'long' ANTES de la imputación: 8\n",
      "2025-05-17 02:01:18,456 - INFO - Nulos en 'long' DESPUÉS de la imputación: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se imputaron 8 valores en 'lat'.\n",
      "Se imputaron 8 valores en 'long'.\n",
      "\n",
      "Nuevo rango para 'lat': Min=40.4998, Max=40.9170\n",
      "Nuevo rango para 'long': Min=-74.2498, Max=-73.7052\n"
     ]
    }
   ],
   "source": [
    "# Celda 12.1: Imputación de Nulos en 'lat' y 'long'\n",
    "logging.info(\"Celda 12.1: Iniciando imputación de nulos para 'lat' y 'long'.\")\n",
    "\n",
    "if not df_cleaning.empty:\n",
    "    cols_geo = ['lat', 'long']\n",
    "    \n",
    "    for col_geo in cols_geo:\n",
    "        if col_geo not in df_cleaning.columns:\n",
    "            logging.warning(f\"Columna '{col_geo}' no encontrada. Omitiendo imputación para esta columna.\")\n",
    "            continue\n",
    "\n",
    "        nulos_antes = df_cleaning[col_geo].isnull().sum()\n",
    "        logging.info(f\"Nulos en '{col_geo}' ANTES de la imputación: {nulos_antes}\")\n",
    "\n",
    "        if nulos_antes == 0:\n",
    "            logging.info(f\"No hay nulos que imputar en '{col_geo}'.\")\n",
    "            continue\n",
    "\n",
    "        for index in df_cleaning[df_cleaning[col_geo].isnull()].index:\n",
    "            current_neighbourhood = df_cleaning.loc[index, 'neighbourhood']\n",
    "            current_neighbourhood_group = df_cleaning.loc[index, 'neighbourhood_group']\n",
    "            \n",
    "            imputed_value = pd.NA\n",
    "\n",
    "            # Estrategia 1: Usar 'neighbourhood'\n",
    "            if pd.notna(current_neighbourhood):\n",
    "                mean_val_neighbourhood = df_cleaning[\n",
    "                    (df_cleaning['neighbourhood'] == current_neighbourhood) & \n",
    "                    (df_cleaning.index != index) &\n",
    "                    (df_cleaning[col_geo].notna()) \n",
    "                ][col_geo].mean()\n",
    "                \n",
    "                if pd.notna(mean_val_neighbourhood):\n",
    "                    imputed_value = mean_val_neighbourhood\n",
    "                    logging.debug(f\"Imputando '{col_geo}' para índice {index} con media de neighbourhood '{current_neighbourhood}': {imputed_value}\")\n",
    "                else:\n",
    "                    logging.warning(f\"No se pudo calcular la media de '{col_geo}' para neighbourhood '{current_neighbourhood}' (índice {index}). Intentando con neighbourhood_group.\")\n",
    "            \n",
    "            # Estrategia 2: Usar 'neighbourhood_group' (si la Estrategia 1 falló o 'neighbourhood' era nulo)\n",
    "            if pd.isna(imputed_value) and pd.notna(current_neighbourhood_group):\n",
    "                mean_val_group = df_cleaning[\n",
    "                    (df_cleaning['neighbourhood_group'] == current_neighbourhood_group) &\n",
    "                    (df_cleaning.index != index) &\n",
    "                    (df_cleaning[col_geo].notna())\n",
    "                ][col_geo].mean()\n",
    "                \n",
    "                if pd.notna(mean_val_group):\n",
    "                    imputed_value = mean_val_group\n",
    "                    logging.debug(f\"Imputando '{col_geo}' para índice {index} con media de neighbourhood_group '{current_neighbourhood_group}': {imputed_value}\")\n",
    "                else:\n",
    "                    logging.warning(f\"No se pudo calcular la media de '{col_geo}' para neighbourhood_group '{current_neighbourhood_group}' (índice {index}).\")\n",
    "\n",
    "            # Aplicar el valor imputado (si se encontró alguno)\n",
    "            if pd.notna(imputed_value):\n",
    "                df_cleaning.loc[index, col_geo] = imputed_value\n",
    "            else:\n",
    "                logging.warning(f\"No se pudo imputar '{col_geo}' para el índice {index} usando neighbourhood o neighbourhood_group. El valor permanece nulo.\")\n",
    "        \n",
    "        nulos_despues = df_cleaning[col_geo].isnull().sum()\n",
    "        logging.info(f\"Nulos en '{col_geo}' DESPUÉS de la imputación: {nulos_despues}\")\n",
    "        if nulos_antes > nulos_despues:\n",
    "            print(f\"Se imputaron {nulos_antes - nulos_despues} valores en '{col_geo}'.\")\n",
    "        elif nulos_antes == nulos_despues and nulos_antes > 0:\n",
    "            print(f\"No se pudieron imputar los nulos restantes en '{col_geo}' con la estrategia actual.\")\n",
    "\n",
    "    # Verificar rangos de lat/long después de la imputación (opcional, pero bueno)\n",
    "    if 'lat' in df_cleaning.columns and df_cleaning['lat'].notna().any():\n",
    "        print(f\"\\nNuevo rango para 'lat': Min={df_cleaning['lat'].min():.4f}, Max={df_cleaning['lat'].max():.4f}\")\n",
    "    if 'long' in df_cleaning.columns and df_cleaning['long'].notna().any():\n",
    "        print(f\"Nuevo rango para 'long': Min={df_cleaning['long'].min():.4f}, Max={df_cleaning['long'].max():.4f}\")\n",
    "\n",
    "else:\n",
    "    logging.warning(\"El DataFrame df_cleaning está vacío. No se puede realizar la imputación de 'lat'/'long'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "id": "07abff52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:18,466 - INFO - Celda 12.4: Iniciando imputación de nulos para 'country' y 'country_code'.\n",
      "2025-05-17 02:01:18,477 - INFO - Nulos en 'country' ANTES de la imputación: 532\n",
      "2025-05-17 02:01:18,499 - INFO - La moda para 'country' es: 'United States'\n",
      "2025-05-17 02:01:18,527 - INFO - Nulos en 'country' DESPUÉS de la imputación: 0\n",
      "/tmp/ipykernel_1305523/1937374830.py:43: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if not pd.api.types.is_categorical_dtype(df_cleaning[col_name]):\n",
      "2025-05-17 02:01:18,544 - INFO - Convirtiendo 'country' a tipo 'category' después de la imputación.\n",
      "2025-05-17 02:01:18,569 - INFO - Nulos en 'country_code' ANTES de la imputación: 131\n",
      "2025-05-17 02:01:18,595 - INFO - La moda para 'country_code' es: 'US'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Celda 12.4: Imputación de Nulos en 'country' y 'country_code' ---\n",
      "\n",
      "Nulos en 'country' ANTES de la imputación: 532\n",
      "La moda para 'country' es: 'United States'\n",
      "Nulos en 'country' DESPUÉS de la imputación: 0\n",
      "Se imputaron 532 valores en 'country' con 'United States'.\n",
      "Columna 'country' convertida a tipo 'category'. Nuevo tipo: category\n",
      "\n",
      "Nulos en 'country_code' ANTES de la imputación: 131\n",
      "La moda para 'country_code' es: 'US'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:18,623 - INFO - Nulos en 'country_code' DESPUÉS de la imputación: 0\n",
      "/tmp/ipykernel_1305523/1937374830.py:43: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if not pd.api.types.is_categorical_dtype(df_cleaning[col_name]):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulos en 'country_code' DESPUÉS de la imputación: 0\n",
      "Se imputaron 131 valores en 'country_code' con 'US'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:18,637 - INFO - Convirtiendo 'country_code' a tipo 'category' después de la imputación.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna 'country_code' convertida a tipo 'category'. Nuevo tipo: category\n"
     ]
    }
   ],
   "source": [
    "# Celda 12.4: Imputación de Nulos en 'country' y 'country_code'\n",
    "logging.info(\"Celda 12.4: Iniciando imputación de nulos para 'country' y 'country_code'.\")\n",
    "print(\"\\n--- Celda 12.4: Imputación de Nulos en 'country' y 'country_code' ---\")\n",
    "\n",
    "if not df_cleaning.empty:\n",
    "    cols_to_impute_mode = ['country', 'country_code'] # Nombres ya normalizados\n",
    "    \n",
    "    for col_name in cols_to_impute_mode:\n",
    "        if col_name not in df_cleaning.columns:\n",
    "            logging.warning(f\"Columna '{col_name}' no encontrada. Omitiendo imputación para esta columna.\")\n",
    "            print(f\"Advertencia: Columna '{col_name}' no encontrada. Se omite su imputación.\")\n",
    "            continue\n",
    "\n",
    "        nulos_antes_cc = df_cleaning[col_name].isnull().sum()\n",
    "        logging.info(f\"Nulos en '{col_name}' ANTES de la imputación: {nulos_antes_cc}\")\n",
    "        print(f\"\\nNulos en '{col_name}' ANTES de la imputación: {nulos_antes_cc}\")\n",
    "\n",
    "        if nulos_antes_cc == 0:\n",
    "            logging.info(f\"No hay nulos que imputar en '{col_name}'.\")\n",
    "            print(f\"No hay nulos que imputar en '{col_name}'.\")\n",
    "            continue\n",
    "\n",
    "        # Calcular la moda de la columna (excluyendo NaNs para el cálculo de la moda)\n",
    "        moda_col = df_cleaning[col_name].dropna().mode() \n",
    "        \n",
    "        if not moda_col.empty:\n",
    "            valor_moda = moda_col.iloc[0] # Tomar la primera moda si hay varias\n",
    "            logging.info(f\"La moda para '{col_name}' es: '{valor_moda}'\")\n",
    "            print(f\"La moda para '{col_name}' es: '{valor_moda}'\")\n",
    "            \n",
    "            # Imputar los valores nulos con la moda\n",
    "            df_cleaning[col_name].fillna(valor_moda, inplace=True)\n",
    "            imputed_count_cc = nulos_antes_cc - df_cleaning[col_name].isnull().sum() # Contar cuántos se imputaron\n",
    "            \n",
    "            nulos_despues_cc = df_cleaning[col_name].isnull().sum()\n",
    "            logging.info(f\"Nulos en '{col_name}' DESPUÉS de la imputación: {nulos_despues_cc}\")\n",
    "            print(f\"Nulos en '{col_name}' DESPUÉS de la imputación: {nulos_despues_cc}\")\n",
    "            if imputed_count_cc > 0:\n",
    "                print(f\"Se imputaron {imputed_count_cc} valores en '{col_name}' con '{valor_moda}'.\")\n",
    "            \n",
    "            # Opcional: Convertir a tipo 'category' después de la imputación si no lo es ya\n",
    "            # (Ya deberían serlo por la Celda 8 si tenían pocos valores únicos o se incluyeron allí)\n",
    "            if not pd.api.types.is_categorical_dtype(df_cleaning[col_name]):\n",
    "                if df_cleaning[col_name].nunique(dropna=False) <= 10: # Umbral de ejemplo\n",
    "                     logging.info(f\"Convirtiendo '{col_name}' a tipo 'category' después de la imputación.\")\n",
    "                     df_cleaning[col_name] = df_cleaning[col_name].astype('category')\n",
    "                     print(f\"Columna '{col_name}' convertida a tipo 'category'. Nuevo tipo: {df_cleaning[col_name].dtype}\")\n",
    "        else:\n",
    "            logging.warning(f\"No se pudo determinar la moda para '{col_name}' (la columna podría ser completamente nula o tener problemas). No se imputaron valores.\")\n",
    "            print(f\"Advertencia: No se pudo determinar la moda para '{col_name}'. No se imputaron valores.\")\n",
    "            \n",
    "else:\n",
    "    logging.warning(\"El DataFrame df_cleaning está vacío. No se puede realizar la imputación.\")\n",
    "    print(\"El DataFrame df_cleaning está vacío.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "id": "284428bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:18,667 - INFO - Celda 12.5: Eliminando la columna 'host_name'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Celda 12.5: Eliminación de la Columna 'host_name' ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:18,679 - INFO - Columna 'host_name' encontrada. Nulos antes de eliminar: 404.\n",
      "2025-05-17 02:01:18,689 - INFO - Columna 'host_name' eliminada exitosamente.\n",
      "2025-05-17 02:01:18,690 - INFO - Dimensiones de df_cleaning después de eliminar 'host_name': (102058, 26)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna 'host_name' encontrada. Nulos antes de eliminar: 404.\n",
      "Columna 'host_name' eliminada exitosamente.\n",
      "Nuevas dimensiones de df_cleaning: (102058, 26)\n"
     ]
    }
   ],
   "source": [
    "# Celda 12.5: Eliminación de la Columna 'host_name'\n",
    "logging.info(\"Celda 12.5: Eliminando la columna 'host_name'.\")\n",
    "print(\"\\n--- Celda 12.5: Eliminación de la Columna 'host_name' ---\")\n",
    "\n",
    "if not df_cleaning.empty:\n",
    "    col_to_drop_hostname = 'host_name' # Nombre ya normalizado\n",
    "    \n",
    "    if col_to_drop_hostname in df_cleaning.columns:\n",
    "        nulos_en_hostname_antes = df_cleaning[col_to_drop_hostname].isnull().sum()\n",
    "        logging.info(f\"Columna '{col_to_drop_hostname}' encontrada. Nulos antes de eliminar: {nulos_en_hostname_antes}.\")\n",
    "        print(f\"Columna '{col_to_drop_hostname}' encontrada. Nulos antes de eliminar: {nulos_en_hostname_antes}.\")\n",
    "\n",
    "        df_cleaning.drop(columns=[col_to_drop_hostname], inplace=True)\n",
    "        \n",
    "        logging.info(f\"Columna '{col_to_drop_hostname}' eliminada exitosamente.\")\n",
    "        print(f\"Columna '{col_to_drop_hostname}' eliminada exitosamente.\")\n",
    "        logging.info(f\"Dimensiones de df_cleaning después de eliminar '{col_to_drop_hostname}': {df_cleaning.shape}\")\n",
    "        print(f\"Nuevas dimensiones de df_cleaning: {df_cleaning.shape}\")\n",
    "    else:\n",
    "        logging.warning(f\"La columna '{col_to_drop_hostname}' no se encontró en df_cleaning. No se realizó ninguna eliminación.\")\n",
    "        print(f\"La columna '{col_to_drop_hostname}' no se encontró. No se eliminó nada.\")\n",
    "else:\n",
    "    logging.warning(\"El DataFrame df_cleaning está vacío. No se puede eliminar la columna 'host_name'.\")\n",
    "    print(\"El DataFrame df_cleaning está vacío.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "id": "4ceaca44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:18,699 - INFO - Celda 12.7.1: Identificando fechas anómalas (posteriores a 2024) en 'last_review'.\n",
      "2025-05-17 02:01:18,701 - INFO - Límite superior para fechas válidas en 'last_review': 2024-12-31\n",
      "2025-05-17 02:01:18,703 - INFO - Número de fechas en 'last_review' posteriores a 2024-12-31: 4\n",
      "2025-05-17 02:01:18,704 - INFO - Ejemplos de fechas anómalas: [Timestamp('2025-06-26 00:00:00'), Timestamp('2058-06-16 00:00:00'), Timestamp('2026-03-28 00:00:00'), Timestamp('2040-06-16 00:00:00')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Celda 12.7.1: Identificación de Fechas Anómalas en 'last_review' ---\n",
      "Límite superior considerado para fechas válidas en 'last_review': 2024-12-31\n",
      "Número de fechas en 'last_review' encontradas posteriores a 2024-12-31: 4\n",
      "Algunas fechas anómalas encontradas (hasta 10 ejemplos):\n",
      "<DatetimeArray>\n",
      "['2025-06-26 00:00:00', '2058-06-16 00:00:00', '2026-03-28 00:00:00', '2040-06-16 00:00:00']\n",
      "Length: 4, dtype: datetime64[ns]\n",
      "\n",
      "Estadísticas descriptivas de fechas (antes de corregir anomalías):\n",
      "|       | last_review                   |\n",
      "|:------|:------------------------------|\n",
      "| count | 86226                         |\n",
      "| mean  | 2019-06-11 02:12:22.996311808 |\n",
      "| min   | 2012-07-11 00:00:00           |\n",
      "| 25%   | 2018-10-27 00:00:00           |\n",
      "| 50%   | 2019-06-13 00:00:00           |\n",
      "| 75%   | 2019-07-05 00:00:00           |\n",
      "| max   | 2058-06-16 00:00:00           |\n"
     ]
    }
   ],
   "source": [
    "# Celda 12.7.1: Identificación de Fechas Anómalas en 'last_review' (posteriores a 2024)\n",
    "logging.info(\"Celda 12.7.1: Identificando fechas anómalas (posteriores a 2024) en 'last_review'.\")\n",
    "print(\"\\n--- Celda 12.7.1: Identificación de Fechas Anómalas en 'last_review' ---\")\n",
    "\n",
    "if not df_cleaning.empty:\n",
    "    col_lr = 'last_review' # Nombre ya normalizado\n",
    "    \n",
    "    if col_lr not in df_cleaning.columns:\n",
    "        logging.warning(f\"Columna '{col_lr}' no encontrada. Omitiendo análisis.\")\n",
    "        print(f\"Advertencia: Columna '{col_lr}' no encontrada.\")\n",
    "    elif not pd.api.types.is_datetime64_any_dtype(df_cleaning[col_lr]):\n",
    "        logging.warning(f\"Columna '{col_lr}' no es de tipo datetime (Tipo: {df_cleaning[col_lr].dtype}). No se pueden identificar fechas anómalas.\")\n",
    "        print(f\"Advertencia: Columna '{col_lr}' no es de tipo datetime.\")\n",
    "    else:\n",
    "        limite_fecha_superior = pd.Timestamp('2024-12-31 23:59:59') \n",
    "\n",
    "        logging.info(f\"Límite superior para fechas válidas en '{col_lr}': {limite_fecha_superior.strftime('%Y-%m-%d')}\")\n",
    "        print(f\"Límite superior considerado para fechas válidas en '{col_lr}': {limite_fecha_superior.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "        fechas_anomalas_lr = df_cleaning.loc[\n",
    "            df_cleaning[col_lr].notna() & (df_cleaning[col_lr] > limite_fecha_superior), col_lr\n",
    "        ]\n",
    "        count_fechas_anomalas_lr = len(fechas_anomalas_lr)\n",
    "\n",
    "        logging.info(f\"Número de fechas en '{col_lr}' posteriores a {limite_fecha_superior.strftime('%Y-%m-%d')}: {count_fechas_anomalas_lr}\")\n",
    "        print(f\"Número de fechas en '{col_lr}' encontradas posteriores a {limite_fecha_superior.strftime('%Y-%m-%d')}: {count_fechas_anomalas_lr}\")\n",
    "\n",
    "        if count_fechas_anomalas_lr > 0:\n",
    "            print(\"Algunas fechas anómalas encontradas (hasta 10 ejemplos):\")\n",
    "            print(fechas_anomalas_lr.unique()[:10])\n",
    "            logging.info(f\"Ejemplos de fechas anómalas: {fechas_anomalas_lr.unique()[:10].tolist()}\")\n",
    "        \n",
    "        print(\"\\nEstadísticas descriptivas de fechas (antes de corregir anomalías):\")\n",
    "        if df_cleaning[col_lr].notna().any():\n",
    "            # CORRECCIÓN AQUÍ: Se elimina datetime_is_numeric=True\n",
    "            print(df_cleaning[col_lr].describe().to_markdown()) \n",
    "        else:\n",
    "            print(\"No hay fechas no nulas para describir.\")\n",
    "else:\n",
    "    logging.warning(\"El DataFrame df_cleaning está vacío.\")\n",
    "    print(\"El DataFrame df_cleaning está vacío.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "id": "6b1911c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:18,718 - INFO - Celda 12.7.2: Corrigiendo fechas anómalas en 'last_review' reemplazándolas con NaT.\n",
      "2025-05-17 02:01:18,723 - INFO - 4 fechas anómalas en 'last_review' han sido reemplazadas con NaT.\n",
      "2025-05-17 02:01:18,725 - INFO - Verificación: No quedan fechas anómalas (posteriores al límite).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Celda 12.7.2: Corrección de Fechas Anómalas en 'last_review' ---\n",
      "4 fechas anómalas en 'last_review' han sido reemplazadas con NaT.\n",
      "Verificación: No quedan fechas anómalas (posteriores al límite).\n",
      "\n",
      "Estadísticas descriptivas de fechas (después de corregir anomalías y antes de imputar NaTs):\n",
      "|       | last_review                   |\n",
      "|:------|:------------------------------|\n",
      "| count | 86222                         |\n",
      "| mean  | 2019-06-10 18:47:53.419776768 |\n",
      "| min   | 2012-07-11 00:00:00           |\n",
      "| 25%   | 2018-10-27 00:00:00           |\n",
      "| 50%   | 2019-06-13 00:00:00           |\n",
      "| 75%   | 2019-07-05 00:00:00           |\n",
      "| max   | 2024-08-15 00:00:00           |\n"
     ]
    }
   ],
   "source": [
    "# Celda 12.7.2: Corrección de Fechas Anómalas en 'last_review' (reemplazar con NaT)\n",
    "logging.info(\"Celda 12.7.2: Corrigiendo fechas anómalas en 'last_review' reemplazándolas con NaT.\")\n",
    "print(\"\\n--- Celda 12.7.2: Corrección de Fechas Anómalas en 'last_review' ---\")\n",
    "\n",
    "if not df_cleaning.empty:\n",
    "    col_lr = 'last_review'\n",
    "    \n",
    "    if col_lr in df_cleaning.columns and pd.api.types.is_datetime64_any_dtype(df_cleaning[col_lr]):\n",
    "        limite_fecha_superior = pd.Timestamp('2024-12-31 23:59:59')\n",
    "        \n",
    "        indices_anomalos = df_cleaning[\n",
    "            df_cleaning[col_lr].notna() & (df_cleaning[col_lr] > limite_fecha_superior)\n",
    "        ].index\n",
    "        \n",
    "        count_a_corregir = len(indices_anomalos)\n",
    "\n",
    "        if count_a_corregir > 0:\n",
    "            df_cleaning.loc[indices_anomalos, col_lr] = pd.NaT\n",
    "            logging.info(f\"{count_a_corregir} fechas anómalas en '{col_lr}' han sido reemplazadas con NaT.\")\n",
    "            print(f\"{count_a_corregir} fechas anómalas en '{col_lr}' han sido reemplazadas con NaT.\")\n",
    "            \n",
    "            fechas_aun_anomalas_indices = df_cleaning[\n",
    "                df_cleaning[col_lr].notna() & (df_cleaning[col_lr] > limite_fecha_superior)\n",
    "            ].index\n",
    "            \n",
    "            if len(fechas_aun_anomalas_indices) == 0: # Comprobar si la longitud de los índices es 0\n",
    "                logging.info(\"Verificación: No quedan fechas anómalas (posteriores al límite).\")\n",
    "                print(\"Verificación: No quedan fechas anómalas (posteriores al límite).\")\n",
    "            else:\n",
    "                logging.warning(\"Advertencia: Todavía se detectan fechas anómalas después de la corrección. Revisar lógica.\")\n",
    "                print(\"Advertencia: Todavía se detectan fechas anómalas después de la corrección. Revisar lógica.\")\n",
    "        else:\n",
    "            logging.info(f\"No se encontraron fechas anómalas en '{col_lr}' para corregir (posteriores a {limite_fecha_superior.strftime('%Y-%m-%d')}).\")\n",
    "            print(f\"No se encontraron fechas anómalas en '{col_lr}' para corregir.\")\n",
    "            \n",
    "        print(\"\\nEstadísticas descriptivas de fechas (después de corregir anomalías y antes de imputar NaTs):\")\n",
    "        if df_cleaning[col_lr].notna().any():\n",
    "             # CORRECCIÓN AQUÍ: Se elimina datetime_is_numeric=True\n",
    "             print(df_cleaning[col_lr].describe().to_markdown())\n",
    "        else:\n",
    "            print(f\"Todos los valores en '{col_lr}' son ahora NaT o la columna está vacía.\")\n",
    "\n",
    "    elif col_lr not in df_cleaning.columns:\n",
    "        logging.warning(f\"Columna '{col_lr}' no encontrada.\")\n",
    "        print(f\"Columna '{col_lr}' no encontrada.\")\n",
    "    else: # No es datetime\n",
    "        logging.warning(f\"Columna '{col_lr}' no es de tipo datetime.\")\n",
    "        print(f\"Columna '{col_lr}' no es de tipo datetime.\")\n",
    "else:\n",
    "    logging.warning(\"El DataFrame df_cleaning está vacío.\")\n",
    "    print(\"El DataFrame df_cleaning está vacío.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "id": "bcb9abe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:18,739 - INFO - Celda 12.7.3: Iniciando imputación de NaT's para 'last_review' con la media (corregida).\n",
      "2025-05-17 02:01:18,742 - INFO - Total de NaT's en 'last_review' ANTES de la imputación con media: 15836\n",
      "2025-05-17 02:01:18,744 - INFO - La fecha media (corregida) calculada para 'last_review' es: 2019-06-10 18:47:53.419777024\n",
      "2025-05-17 02:01:18,745 - INFO - NaT's en 'last_review' DESPUÉS de la imputación con media: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Celda 12.7.3: Imputación de NaT's en 'last_review' con Media Corregida ---\n",
      "Total de NaT's en 'last_review' ANTES de la imputación con media: 15836\n",
      "La fecha media (corregida) calculada para 'last_review' es: 2019-06-10 18:47:53\n",
      "NaT's en 'last_review' DESPUÉS de la imputación con media: 0\n",
      "Se imputaron 15836 valores NaT en 'last_review' con la fecha media (corregida).\n",
      "\n",
      "Nuevo rango FINAL para 'last_review' después de la imputación:\n",
      "  Fecha mínima: 2012-07-11\n",
      "  Fecha máxima: 2024-08-15\n",
      "\n",
      "Estadísticas descriptivas FINALES para 'last_review':\n",
      "|       | last_review                   |\n",
      "|:------|:------------------------------|\n",
      "| count | 102058                        |\n",
      "| mean  | 2019-06-10 18:47:53.419776768 |\n",
      "| min   | 2012-07-11 00:00:00           |\n",
      "| 25%   | 2019-01-02 00:00:00           |\n",
      "| 50%   | 2019-06-10 18:47:53.419777024 |\n",
      "| 75%   | 2019-07-01 00:00:00           |\n",
      "| max   | 2024-08-15 00:00:00           |\n"
     ]
    }
   ],
   "source": [
    "# Celda 12.7.3: Imputación de NaT's en 'last_review' con la Media (Corregida)\n",
    "logging.info(\"Celda 12.7.3: Iniciando imputación de NaT's para 'last_review' con la media (corregida).\")\n",
    "print(\"\\n--- Celda 12.7.3: Imputación de NaT's en 'last_review' con Media Corregida ---\")\n",
    "\n",
    "if not df_cleaning.empty:\n",
    "    col_lr = 'last_review'\n",
    "    \n",
    "    if col_lr in df_cleaning.columns and pd.api.types.is_datetime64_any_dtype(df_cleaning[col_lr]):\n",
    "        nulos_antes_lr_imputacion = df_cleaning[col_lr].isnull().sum()\n",
    "        logging.info(f\"Total de NaT's en '{col_lr}' ANTES de la imputación con media: {nulos_antes_lr_imputacion}\")\n",
    "        print(f\"Total de NaT's en '{col_lr}' ANTES de la imputación con media: {nulos_antes_lr_imputacion}\")\n",
    "\n",
    "        if nulos_antes_lr_imputacion == 0:\n",
    "            logging.info(f\"No hay NaT's que imputar en '{col_lr}'.\")\n",
    "            print(f\"No hay NaT's que imputar en '{col_lr}'.\")\n",
    "        else:\n",
    "            mean_date_lr_corrected = df_cleaning[col_lr].dropna().mean()\n",
    "            \n",
    "            if pd.notna(mean_date_lr_corrected):\n",
    "                logging.info(f\"La fecha media (corregida) calculada para '{col_lr}' es: {mean_date_lr_corrected}\")\n",
    "                print(f\"La fecha media (corregida) calculada para '{col_lr}' es: {mean_date_lr_corrected.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "                \n",
    "                df_cleaning[col_lr].fillna(mean_date_lr_corrected, inplace=True)\n",
    "                imputed_count_lr_final = nulos_antes_lr_imputacion - df_cleaning[col_lr].isnull().sum()\n",
    "                \n",
    "                nulos_despues_lr_imputacion = df_cleaning[col_lr].isnull().sum()\n",
    "                logging.info(f\"NaT's en '{col_lr}' DESPUÉS de la imputación con media: {nulos_despues_lr_imputacion}\")\n",
    "                print(f\"NaT's en '{col_lr}' DESPUÉS de la imputación con media: {nulos_despues_lr_imputacion}\")\n",
    "                if imputed_count_lr_final > 0:\n",
    "                    print(f\"Se imputaron {imputed_count_lr_final} valores NaT en '{col_lr}' con la fecha media (corregida).\")\n",
    "            else:\n",
    "                logging.warning(f\"No se pudo calcular la fecha media (corregida) para '{col_lr}'. No se imputaron valores.\")\n",
    "                print(f\"Advertencia: No se pudo calcular la fecha media (corregida) para '{col_lr}'. No se imputaron valores.\")\n",
    "            \n",
    "            if df_cleaning[col_lr].notna().any():\n",
    "                min_date_final = df_cleaning[col_lr].min()\n",
    "                max_date_final = df_cleaning[col_lr].max()\n",
    "                print(f\"\\nNuevo rango FINAL para '{col_lr}' después de la imputación:\")\n",
    "                print(f\"  Fecha mínima: {min_date_final.strftime('%Y-%m-%d')}\")\n",
    "                print(f\"  Fecha máxima: {max_date_final.strftime('%Y-%m-%d')}\")\n",
    "                print(\"\\nEstadísticas descriptivas FINALES para 'last_review':\")\n",
    "                # CORRECCIÓN AQUÍ: Se elimina datetime_is_numeric=True\n",
    "                print(df_cleaning[col_lr].describe().to_markdown())\n",
    "\n",
    "    elif col_lr not in df_cleaning.columns:\n",
    "        logging.warning(f\"Columna '{col_lr}' no encontrada.\")\n",
    "        print(f\"Columna '{col_lr}' no encontrada.\")\n",
    "    else: # No es datetime\n",
    "        logging.warning(f\"Columna '{col_lr}' no es de tipo datetime.\")\n",
    "        print(f\"Columna '{col_lr}' no es de tipo datetime.\")\n",
    "else:\n",
    "    logging.warning(\"El DataFrame df_cleaning está vacío.\")\n",
    "    print(\"El DataFrame df_cleaning está vacío.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1015,
   "id": "8c51e71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna 'house_rules' eliminada.\n"
     ]
    }
   ],
   "source": [
    "if 'house_rules' in df_cleaning.columns:\n",
    "    df_cleaning.drop(columns=['house_rules'], inplace=True)\n",
    "    print(\"Columna 'house_rules' eliminada.\")\n",
    "else:\n",
    "    print(\"La columna 'house_rules' ya no existe en df_cleaning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1016,
   "id": "73fa9343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:18,776 - INFO - Celda 12.8: Iniciando imputación de nulos para 'reviews_per_month' con estrategia personalizada.\n",
      "2025-05-17 02:01:18,778 - INFO - Nulos en 'reviews_per_month' ANTES de la imputación: 15818\n",
      "2025-05-17 02:01:18,780 - INFO - Estadísticas de 'reviews_per_month' (existentes): Media=1.38, StdDev=1.75, MinObs=0.01, MaxObs=90.00\n",
      "2025-05-17 02:01:18,781 - INFO - Parámetros de imputación: MediaImp=1.38, StdImp=1.24, LimInf=1.00, LimSup=90.00\n",
      "2025-05-17 02:01:18,785 - INFO - Nulos en 'reviews_per_month' DESPUÉS de la imputación: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Celda 12.8: Imputación de Nulos en 'reviews_per_month' ---\n",
      "Nulos en 'reviews_per_month' ANTES de la imputación: 15818\n",
      "Estadísticas de 'reviews_per_month' (existentes): Media=1.38, StdDev=1.75, MinObs=0.01, MaxObs=90.00\n",
      "Parámetros de imputación: MediaImp=1.38, StdImp=1.24, LimInf=1.00, LimSup=90.00\n",
      "Nulos en 'reviews_per_month' DESPUÉS de la imputación: 0\n",
      "Se imputaron 15818 valores en 'reviews_per_month'.\n",
      "\n",
      "Estadísticas descriptivas actualizadas para 'reviews_per_month':\n",
      "|       |   reviews_per_month |\n",
      "|:------|--------------------:|\n",
      "| count |        102058       |\n",
      "| mean  |             1.42627 |\n",
      "| std   |             1.64542 |\n",
      "| min   |             0.01    |\n",
      "| 25%   |             0.28    |\n",
      "| 50%   |             1       |\n",
      "| 75%   |             2.06    |\n",
      "| max   |            90       |\n"
     ]
    }
   ],
   "source": [
    "# Celda 12.8: Imputación de Nulos en 'reviews_per_month' con Estrategia Personalizada\n",
    "logging.info(\"Celda 12.8: Iniciando imputación de nulos para 'reviews_per_month' con estrategia personalizada.\")\n",
    "print(\"\\n--- Celda 12.8: Imputación de Nulos en 'reviews_per_month' ---\")\n",
    "\n",
    "if not df_cleaning.empty:\n",
    "    col_rpm = 'reviews_per_month' # Nombre ya normalizado\n",
    "    \n",
    "    if col_rpm not in df_cleaning.columns:\n",
    "        logging.warning(f\"Columna '{col_rpm}' no encontrada. Omitiendo imputación.\")\n",
    "        print(f\"Advertencia: Columna '{col_rpm}' no encontrada. Se omite su imputación.\")\n",
    "    elif not pd.api.types.is_numeric_dtype(df_cleaning[col_rpm]): # Ya debería ser float64\n",
    "        logging.warning(f\"Columna '{col_rpm}' no es de tipo numérico (Tipo: {df_cleaning[col_rpm].dtype}). No se puede imputar.\")\n",
    "        print(f\"Advertencia: Columna '{col_rpm}' no es de tipo numérico. No se puede imputar.\")\n",
    "    else:\n",
    "        nulos_antes_rpm = df_cleaning[col_rpm].isnull().sum()\n",
    "        logging.info(f\"Nulos en '{col_rpm}' ANTES de la imputación: {nulos_antes_rpm}\")\n",
    "        print(f\"Nulos en '{col_rpm}' ANTES de la imputación: {nulos_antes_rpm}\")\n",
    "\n",
    "        if nulos_antes_rpm == 0:\n",
    "            logging.info(f\"No hay nulos que imputar en '{col_rpm}'.\")\n",
    "            print(f\"No hay nulos que imputar en '{col_rpm}'.\")\n",
    "        else:\n",
    "            # 1. Calcular estadísticas de los datos existentes no nulos\n",
    "            existing_values_rpm = df_cleaning[col_rpm].dropna()\n",
    "            if existing_values_rpm.empty:\n",
    "                logging.warning(f\"No hay valores no nulos en '{col_rpm}' para calcular estadísticas. No se puede imputar.\")\n",
    "                print(f\"Advertencia: No hay valores no nulos en '{col_rpm}' para calcular estadísticas. No se puede imputar.\")\n",
    "            else:\n",
    "                mean_rpm = existing_values_rpm.mean()\n",
    "                std_rpm = existing_values_rpm.std()\n",
    "                min_observed_rpm = existing_values_rpm.min() # Mínimo observado no nulo\n",
    "                max_observed_rpm = existing_values_rpm.max() # Máximo observado no nulo\n",
    "\n",
    "                logging.info(f\"Estadísticas de '{col_rpm}' (existentes): Media={mean_rpm:.2f}, StdDev={std_rpm:.2f}, MinObs={min_observed_rpm:.2f}, MaxObs={max_observed_rpm:.2f}\")\n",
    "                print(f\"Estadísticas de '{col_rpm}' (existentes): Media={mean_rpm:.2f}, StdDev={std_rpm:.2f}, MinObs={min_observed_rpm:.2f}, MaxObs={max_observed_rpm:.2f}\")\n",
    "\n",
    "                # 2. Definir parámetros para la imputación\n",
    "                # Centro de la distribución para la imputación\n",
    "                imputation_mean = mean_rpm \n",
    "                # Dispersión: reducir la desviación estándar (si varianza/2 -> std/sqrt(2))\n",
    "                # Si la desviación estándar es 0 (todos los valores existentes son iguales), usar un valor pequeño para evitar error en np.random.normal\n",
    "                imputation_std = (std_rpm / np.sqrt(2)) if std_rpm > 0 else 0.1 \n",
    "                \n",
    "                # Límites para los valores imputados\n",
    "                imputation_lower_limit = 1.0 # Como solicitaste, mínimo 1\n",
    "                # Límite superior: Podríamos usar el máximo observado o algo basado en la media y std\n",
    "                # Ser conservador y usar el máximo observado para no introducir valores demasiado extremos.\n",
    "                imputation_upper_limit = max_observed_rpm \n",
    "                # Asegurar que el límite superior no sea menor que el inferior (caso extremo)\n",
    "                if imputation_upper_limit < imputation_lower_limit:\n",
    "                    imputation_upper_limit = imputation_lower_limit + imputation_std # Un pequeño margen si max_observed es muy bajo\n",
    "\n",
    "                logging.info(f\"Parámetros de imputación: MediaImp={imputation_mean:.2f}, StdImp={imputation_std:.2f}, LimInf={imputation_lower_limit:.2f}, LimSup={imputation_upper_limit:.2f}\")\n",
    "                print(f\"Parámetros de imputación: MediaImp={imputation_mean:.2f}, StdImp={imputation_std:.2f}, LimInf={imputation_lower_limit:.2f}, LimSup={imputation_upper_limit:.2f}\")\n",
    "\n",
    "                # 3. Generar valores aleatorios para los nulos\n",
    "                num_nulos_to_impute = nulos_antes_rpm\n",
    "                np.random.seed(42) # Establece la semilla para reproducibilidad\n",
    "\n",
    "                random_imputed_values = np.random.normal(loc=imputation_mean, scale=imputation_std, size=num_nulos_to_impute)\n",
    "                \n",
    "                # 4. Aplicar límites (clipping) a los valores generados\n",
    "                clipped_imputed_values = np.clip(random_imputed_values, imputation_lower_limit, imputation_upper_limit)\n",
    "                \n",
    "                # Imputar los nulos\n",
    "                # Es importante asignar los valores generados solo a las filas que tienen NaN\n",
    "                nan_indices_rpm = df_cleaning[df_cleaning[col_rpm].isnull()].index\n",
    "                if len(nan_indices_rpm) == len(clipped_imputed_values): # Asegurar que las longitudes coinciden\n",
    "                    df_cleaning.loc[nan_indices_rpm, col_rpm] = clipped_imputed_values\n",
    "                    imputed_count_rpm = num_nulos_to_impute # Asumimos que todos se imputaron\n",
    "                else:\n",
    "                    # Esto no debería ocurrir si num_nulos_to_impute se calculó correctamente\n",
    "                    logging.error(\"Error: Discrepancia en el número de nulos y valores generados para imputar.\")\n",
    "                    print(\"Error: Discrepancia en el número de nulos y valores generados.\")\n",
    "                    imputed_count_rpm = 0\n",
    "\n",
    "\n",
    "                nulos_despues_rpm = df_cleaning[col_rpm].isnull().sum()\n",
    "                logging.info(f\"Nulos en '{col_rpm}' DESPUÉS de la imputación: {nulos_despues_rpm}\")\n",
    "                print(f\"Nulos en '{col_rpm}' DESPUÉS de la imputación: {nulos_despues_rpm}\")\n",
    "                if imputed_count_rpm > 0: # O nulos_antes_rpm > nulos_despues_rpm\n",
    "                    print(f\"Se imputaron {nulos_antes_rpm - nulos_despues_rpm} valores en '{col_rpm}'.\")\n",
    "                \n",
    "                # Verificar estadísticas después de la imputación\n",
    "                print(f\"\\nEstadísticas descriptivas actualizadas para '{col_rpm}':\")\n",
    "                print(df_cleaning[col_rpm].describe().to_markdown())\n",
    "else:\n",
    "    logging.warning(\"El DataFrame df_cleaning está vacío. No se puede realizar la imputación de '{col_rpm}'.\")\n",
    "    print(\"El DataFrame df_cleaning está vacío.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1017,
   "id": "c5399fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:18,798 - INFO - Celda 12.9: Iniciando imputación de nulos para 'availability_365' con la mediana.\n",
      "2025-05-17 02:01:18,800 - INFO - Nulos en 'availability_365' ANTES de la imputación con mediana: 448\n",
      "2025-05-17 02:01:18,802 - INFO - La mediana calculada para 'availability_365' es: 96.0 (se usará como entero: 96)\n",
      "2025-05-17 02:01:18,804 - INFO - Nulos en 'availability_365' DESPUÉS de la imputación: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Celda 12.9: Imputación de Nulos en 'availability_365' con Mediana ---\n",
      "Nulos en 'availability_365' ANTES de la imputación con mediana: 448\n",
      "La mediana calculada para 'availability_365' es: 96.00 (se usará como entero: 96)\n",
      "Nulos en 'availability_365' DESPUÉS de la imputación: 0\n",
      "Se imputaron 448 valores nulos en 'availability_365' con la mediana (96).\n",
      "\n",
      "Estadísticas descriptivas actualizadas para 'availability_365':\n",
      "|       |   availability_365 |\n",
      "|:------|-------------------:|\n",
      "| count |         102058     |\n",
      "| mean  |            140.244 |\n",
      "| std   |            132.931 |\n",
      "| min   |              1     |\n",
      "| 25%   |              3     |\n",
      "| 50%   |             96     |\n",
      "| 75%   |            268     |\n",
      "| max   |            365     |\n"
     ]
    }
   ],
   "source": [
    "# Celda 12.9: Imputación de Nulos en 'availability_365' con la Mediana\n",
    "logging.info(\"Celda 12.9: Iniciando imputación de nulos para 'availability_365' con la mediana.\")\n",
    "print(\"\\n--- Celda 12.9: Imputación de Nulos en 'availability_365' con Mediana ---\")\n",
    "\n",
    "if not df_cleaning.empty:\n",
    "    col_avail = 'availability_365' # Nombre ya normalizado\n",
    "    \n",
    "    if col_avail not in df_cleaning.columns:\n",
    "        logging.warning(f\"Columna '{col_avail}' no encontrada. Omitiendo imputación.\")\n",
    "        print(f\"Advertencia: Columna '{col_avail}' no encontrada. Se omite su imputación.\")\n",
    "    elif not pd.api.types.is_numeric_dtype(df_cleaning[col_avail]): # Ya debería ser Int64\n",
    "        logging.warning(f\"Columna '{col_avail}' no es de tipo numérico (Tipo: {df_cleaning[col_avail].dtype}). No se puede imputar con la mediana.\")\n",
    "        print(f\"Advertencia: Columna '{col_avail}' no es de tipo numérico. No se puede imputar con la mediana.\")\n",
    "    else:\n",
    "        nulos_antes_avail = df_cleaning[col_avail].isnull().sum()\n",
    "        logging.info(f\"Nulos en '{col_avail}' ANTES de la imputación con mediana: {nulos_antes_avail}\")\n",
    "        print(f\"Nulos en '{col_avail}' ANTES de la imputación con mediana: {nulos_antes_avail}\")\n",
    "\n",
    "        if nulos_antes_avail == 0:\n",
    "            logging.info(f\"No hay nulos que imputar en '{col_avail}'.\")\n",
    "            print(f\"No hay nulos que imputar en '{col_avail}'.\")\n",
    "        else:\n",
    "            # Calcular la mediana de los valores existentes (no nulos)\n",
    "            # La columna ya fue clipeada entre 0 y 365.\n",
    "            median_avail = df_cleaning[col_avail].dropna().median()\n",
    "            \n",
    "            if pd.notna(median_avail):\n",
    "                # La mediana de una columna Int64 puede ser float si hay un número par de elementos.\n",
    "                # Como la columna es Int64, redondeamos la mediana y la convertimos a entero\n",
    "                # para mantener la consistencia del tipo de dato.\n",
    "                median_avail_int = int(round(median_avail))\n",
    "\n",
    "                logging.info(f\"La mediana calculada para '{col_avail}' es: {median_avail} (se usará como entero: {median_avail_int})\")\n",
    "                print(f\"La mediana calculada para '{col_avail}' es: {median_avail:.2f} (se usará como entero: {median_avail_int})\")\n",
    "                \n",
    "                # Imputar los valores nulos (pd.NA) con la mediana entera\n",
    "                df_cleaning[col_avail].fillna(median_avail_int, inplace=True)\n",
    "                imputed_count_avail = nulos_antes_avail - df_cleaning[col_avail].isnull().sum()\n",
    "                \n",
    "                nulos_despues_avail = df_cleaning[col_avail].isnull().sum()\n",
    "                logging.info(f\"Nulos en '{col_avail}' DESPUÉS de la imputación: {nulos_despues_avail}\")\n",
    "                print(f\"Nulos en '{col_avail}' DESPUÉS de la imputación: {nulos_despues_avail}\")\n",
    "                if imputed_count_avail > 0:\n",
    "                    print(f\"Se imputaron {imputed_count_avail} valores nulos en '{col_avail}' con la mediana ({median_avail_int}).\")\n",
    "            else:\n",
    "                logging.warning(f\"No se pudo calcular la mediana para '{col_avail}' (quizás todos los valores son nulos). No se imputaron valores.\")\n",
    "                print(f\"Advertencia: No se pudo calcular la mediana para '{col_avail}'. No se imputaron valores.\")\n",
    "                \n",
    "            # Verificar estadísticas después de la imputación\n",
    "            print(f\"\\nEstadísticas descriptivas actualizadas para '{col_avail}':\")\n",
    "            print(df_cleaning[col_avail].describe().to_markdown())\n",
    "else:\n",
    "    logging.warning(\"El DataFrame df_cleaning está vacío. No se puede realizar la imputación de '{col_avail}'.\")\n",
    "    print(\"El DataFrame df_cleaning está vacío.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1018,
   "id": "07aea2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:18,818 - INFO - Celda 12.10: Iniciando imputación probabilística de nulos para 'minimum_nights'.\n",
      "2025-05-17 02:01:18,820 - INFO - Nulos en 'minimum_nights' ANTES de la imputación: 400\n",
      "2025-05-17 02:01:18,821 - INFO - Valores para imputación en 'minimum_nights': [1, 2, 3, 4, 5, 6, 7, 8]\n",
      "2025-05-17 02:01:18,822 - INFO - Probabilidades asociadas: [0.2916666666666667, 0.125, 0.125, 0.125, 0.08333333333333333, 0.08333333333333333, 0.08333333333333333, 0.08333333333333333]\n",
      "2025-05-17 02:01:18,826 - INFO - Nulos en 'minimum_nights' DESPUÉS de la imputación: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Celda 12.10: Imputación Probabilística de Nulos en 'minimum_nights' ---\n",
      "Nulos en 'minimum_nights' ANTES de la imputación: 400\n",
      "Valores para imputación en 'minimum_nights': [1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Probabilidades asociadas: [0.292, 0.125, 0.125, 0.125, 0.083, 0.083, 0.083, 0.083]\n",
      "Nulos en 'minimum_nights' DESPUÉS de la imputación: 0\n",
      "Se imputaron 400 valores en 'minimum_nights'.\n",
      "\n",
      "Estadísticas descriptivas actualizadas para 'minimum_nights':\n",
      "|       |   minimum_nights |\n",
      "|:------|-----------------:|\n",
      "| count |     102058       |\n",
      "| mean  |          7.95094 |\n",
      "| std   |         18.2695  |\n",
      "| min   |          1       |\n",
      "| 25%   |          2       |\n",
      "| 50%   |          3       |\n",
      "| 75%   |          5       |\n",
      "| max   |        365       |\n",
      "\n",
      "Distribución de valores imputados en 'minimum_nights' (para los que eran nulos):\n",
      "|   minimum_nights |   proportion |\n",
      "|-----------------:|-------------:|\n",
      "|                1 |   0.248996   |\n",
      "|                2 |   0.230732   |\n",
      "|                3 |   0.157655   |\n",
      "|                4 |   0.065257   |\n",
      "|                5 |   0.0593486  |\n",
      "|                6 |   0.0153834  |\n",
      "|                7 |   0.0396441  |\n",
      "|                8 |   0.00272394 |\n",
      "|                9 |   0.00153834 |\n",
      "|               10 |   0.00909287 |\n"
     ]
    }
   ],
   "source": [
    "# Celda 12.10: Imputación Probabilística de Nulos en 'minimum_nights'\n",
    "logging.info(\"Celda 12.10: Iniciando imputación probabilística de nulos para 'minimum_nights'.\")\n",
    "print(\"\\n--- Celda 12.10: Imputación Probabilística de Nulos en 'minimum_nights' ---\")\n",
    "\n",
    "if not df_cleaning.empty:\n",
    "    col_mn = 'minimum_nights' # Nombre ya normalizado\n",
    "    \n",
    "    if col_mn not in df_cleaning.columns:\n",
    "        logging.warning(f\"Columna '{col_mn}' no encontrada. Omitiendo imputación.\")\n",
    "        print(f\"Advertencia: Columna '{col_mn}' no encontrada. Se omite su imputación.\")\n",
    "    elif not pd.api.types.is_numeric_dtype(df_cleaning[col_mn]): # Ya debería ser Int64\n",
    "        logging.warning(f\"Columna '{col_mn}' no es de tipo numérico (Tipo: {df_cleaning[col_mn].dtype}). No se puede imputar.\")\n",
    "        print(f\"Advertencia: Columna '{col_mn}' no es de tipo numérico. No se puede imputar.\")\n",
    "    else:\n",
    "        nulos_antes_mn = df_cleaning[col_mn].isnull().sum()\n",
    "        logging.info(f\"Nulos en '{col_mn}' ANTES de la imputación: {nulos_antes_mn}\")\n",
    "        print(f\"Nulos en '{col_mn}' ANTES de la imputación: {nulos_antes_mn}\")\n",
    "\n",
    "        if nulos_antes_mn == 0:\n",
    "            logging.info(f\"No hay nulos que imputar en '{col_mn}'.\")\n",
    "            print(f\"No hay nulos que imputar en '{col_mn}'.\")\n",
    "        else:\n",
    "            # 1. Definir los posibles valores y sus pesos/probabilidades\n",
    "            valores_imputacion_mn = np.arange(1, 9) # [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "            \n",
    "            # Pesos para dar mayor probabilidad al 1 y distribuir el resto\n",
    "            # Ejemplo: P(1) alta, el resto distribuidas.\n",
    "            # Suma de pesos = 10 + 7*2 = 24\n",
    "            # P(1) = 10/24 ~= 0.416\n",
    "            # P(2..8) = 2/24 ~= 0.083 cada uno\n",
    "            # Puedes ajustar estos pesos como desees.\n",
    "            pesos_mn = [7, 3, 3, 3, 2, 2, 2, 2] # Pesos para [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "            \n",
    "            # Normalizar los pesos para que sumen 1 (np.random.choice lo hace internamente si se le pasan pesos)\n",
    "            probabilidades_mn = np.array(pesos_mn) / np.sum(pesos_mn)\n",
    "\n",
    "            logging.info(f\"Valores para imputación en '{col_mn}': {valores_imputacion_mn.tolist()}\")\n",
    "            logging.info(f\"Probabilidades asociadas: {probabilidades_mn.tolist()}\")\n",
    "            print(f\"Valores para imputación en '{col_mn}': {valores_imputacion_mn.tolist()}\")\n",
    "            print(f\"Probabilidades asociadas: {[float(f'{p:.3f}') for p in probabilidades_mn]}\")\n",
    "\n",
    "\n",
    "            # 2. Generar valores aleatorios para los nulos\n",
    "            num_nulos_to_impute_mn = nulos_antes_mn\n",
    "            random_imputed_values_mn = np.random.choice(\n",
    "                valores_imputacion_mn, \n",
    "                size=num_nulos_to_impute_mn, \n",
    "                p=probabilidades_mn\n",
    "            )\n",
    "            \n",
    "            # 3. Imputar los nulos\n",
    "            nan_indices_mn = df_cleaning[df_cleaning[col_mn].isnull()].index\n",
    "            if len(nan_indices_mn) == len(random_imputed_values_mn):\n",
    "                df_cleaning.loc[nan_indices_mn, col_mn] = random_imputed_values_mn\n",
    "                imputed_count_mn = num_nulos_to_impute_mn\n",
    "            else:\n",
    "                logging.error(\"Error: Discrepancia en el número de nulos y valores generados para imputar en minimum_nights.\")\n",
    "                print(\"Error: Discrepancia en el número de nulos y valores generados en minimum_nights.\")\n",
    "                imputed_count_mn = 0\n",
    "            \n",
    "            nulos_despues_mn = df_cleaning[col_mn].isnull().sum()\n",
    "            logging.info(f\"Nulos en '{col_mn}' DESPUÉS de la imputación: {nulos_despues_mn}\")\n",
    "            print(f\"Nulos en '{col_mn}' DESPUÉS de la imputación: {nulos_despues_mn}\")\n",
    "            if imputed_count_mn > 0:\n",
    "                print(f\"Se imputaron {nulos_antes_mn - nulos_despues_mn} valores en '{col_mn}'.\")\n",
    "            \n",
    "            # Verificar estadísticas y distribución después de la imputación\n",
    "            print(f\"\\nEstadísticas descriptivas actualizadas para '{col_mn}':\")\n",
    "            print(df_cleaning[col_mn].describe().to_markdown())\n",
    "            print(f\"\\nDistribución de valores imputados en '{col_mn}' (para los que eran nulos):\")\n",
    "            # Para ver la distribución de los valores recién imputados:\n",
    "            if imputed_count_mn > 0 :\n",
    "                # Esto puede ser un poco más complejo de aislar si los índices se mezclan.\n",
    "                # Una forma es ver la distribución general ahora:\n",
    "                print(df_cleaning[col_mn].value_counts(normalize=True).sort_index().head(10).to_markdown()) # Mostrar los 10 primeros\n",
    "                \n",
    "else:\n",
    "    logging.warning(\"El DataFrame df_cleaning está vacío. No se puede realizar la imputación de '{col_mn}'.\")\n",
    "    print(\"El DataFrame df_cleaning está vacío.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1019,
   "id": "2d0b9b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:18,841 - INFO - Celda 12.11: Iniciando imputación de nulos para 'calculated_host_listings_count' con la media.\n",
      "2025-05-17 02:01:18,843 - INFO - Nulos en 'calculated_host_listings_count' ANTES de la imputación con media: 319\n",
      "2025-05-17 02:01:18,845 - INFO - La media calculada para 'calculated_host_listings_count' es: 7.936936671286331 (se usará como entero: 8)\n",
      "2025-05-17 02:01:18,846 - INFO - Nulos en 'calculated_host_listings_count' DESPUÉS de la imputación: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Celda 12.11: Imputación de Nulos en 'calculated_host_listings_count' con Media ---\n",
      "Nulos en 'calculated_host_listings_count' ANTES de la imputación con media: 319\n",
      "La media calculada para 'calculated_host_listings_count' es: 7.94 (se usará como entero: 8)\n",
      "Nulos en 'calculated_host_listings_count' DESPUÉS de la imputación: 0\n",
      "Se imputaron 319 valores nulos en 'calculated_host_listings_count' con la media (8).\n",
      "\n",
      "Estadísticas descriptivas actualizadas para 'calculated_host_listings_count':\n",
      "|       |   calculated_host_listings_count |\n",
      "|:------|---------------------------------:|\n",
      "| count |                     102058       |\n",
      "| mean  |                          7.93713 |\n",
      "| std   |                         32.2159  |\n",
      "| min   |                          1       |\n",
      "| 25%   |                          1       |\n",
      "| 50%   |                          1       |\n",
      "| 75%   |                          2       |\n",
      "| max   |                        332       |\n"
     ]
    }
   ],
   "source": [
    "# Celda 12.11: Imputación de Nulos en 'calculated_host_listings_count' con la Media\n",
    "logging.info(\"Celda 12.11: Iniciando imputación de nulos para 'calculated_host_listings_count' con la media.\")\n",
    "print(\"\\n--- Celda 12.11: Imputación de Nulos en 'calculated_host_listings_count' con Media ---\")\n",
    "\n",
    "if not df_cleaning.empty:\n",
    "    col_chlc = 'calculated_host_listings_count' # Nombre ya normalizado\n",
    "    \n",
    "    if col_chlc not in df_cleaning.columns:\n",
    "        logging.warning(f\"Columna '{col_chlc}' no encontrada. Omitiendo imputación.\")\n",
    "        print(f\"Advertencia: Columna '{col_chlc}' no encontrada. Se omite su imputación.\")\n",
    "    elif not pd.api.types.is_numeric_dtype(df_cleaning[col_chlc]): # Ya debería ser Int64\n",
    "        logging.warning(f\"Columna '{col_chlc}' no es de tipo numérico (Tipo: {df_cleaning[col_chlc].dtype}). No se puede imputar con la media.\")\n",
    "        print(f\"Advertencia: Columna '{col_chlc}' no es de tipo numérico. No se puede imputar con la media.\")\n",
    "    else:\n",
    "        nulos_antes_chlc = df_cleaning[col_chlc].isnull().sum()\n",
    "        logging.info(f\"Nulos en '{col_chlc}' ANTES de la imputación con media: {nulos_antes_chlc}\")\n",
    "        print(f\"Nulos en '{col_chlc}' ANTES de la imputación con media: {nulos_antes_chlc}\")\n",
    "\n",
    "        if nulos_antes_chlc == 0:\n",
    "            logging.info(f\"No hay nulos que imputar en '{col_chlc}'.\")\n",
    "            print(f\"No hay nulos que imputar en '{col_chlc}'.\")\n",
    "        else:\n",
    "            # Calcular la media de los valores existentes (no nulos)\n",
    "            mean_chlc = df_cleaning[col_chlc].dropna().mean()\n",
    "            \n",
    "            if pd.notna(mean_chlc):\n",
    "                # Redondear la media y convertir a entero para mantener la consistencia del tipo Int64\n",
    "                mean_chlc_int = int(round(mean_chlc))\n",
    "\n",
    "                logging.info(f\"La media calculada para '{col_chlc}' es: {mean_chlc} (se usará como entero: {mean_chlc_int})\")\n",
    "                print(f\"La media calculada para '{col_chlc}' es: {mean_chlc:.2f} (se usará como entero: {mean_chlc_int})\")\n",
    "                \n",
    "                # Imputar los valores nulos (pd.NA) con la media entera\n",
    "                df_cleaning[col_chlc].fillna(mean_chlc_int, inplace=True)\n",
    "                imputed_count_chlc = nulos_antes_chlc - df_cleaning[col_chlc].isnull().sum()\n",
    "                \n",
    "                nulos_despues_chlc = df_cleaning[col_chlc].isnull().sum()\n",
    "                logging.info(f\"Nulos en '{col_chlc}' DESPUÉS de la imputación: {nulos_despues_chlc}\")\n",
    "                print(f\"Nulos en '{col_chlc}' DESPUÉS de la imputación: {nulos_despues_chlc}\")\n",
    "                if imputed_count_chlc > 0:\n",
    "                    print(f\"Se imputaron {imputed_count_chlc} valores nulos en '{col_chlc}' con la media ({mean_chlc_int}).\")\n",
    "            else:\n",
    "                logging.warning(f\"No se pudo calcular la media para '{col_chlc}' (quizás todos los valores son nulos). No se imputaron valores.\")\n",
    "                print(f\"Advertencia: No se pudo calcular la media para '{col_chlc}'. No se imputaron valores.\")\n",
    "                \n",
    "            # Verificar estadísticas después de la imputación\n",
    "            print(f\"\\nEstadísticas descriptivas actualizadas para '{col_chlc}':\")\n",
    "            print(df_cleaning[col_chlc].describe().to_markdown())\n",
    "else:\n",
    "    logging.warning(\"El DataFrame df_cleaning está vacío. No se puede realizar la imputación de '{col_chlc}'.\")\n",
    "    print(\"El DataFrame df_cleaning está vacío.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "id": "90f21746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:18,862 - INFO - Celda 12.12: Iniciando imputación de nulos para 'review_rate_number' con moda condicionada.\n",
      "/tmp/ipykernel_1305523/2430475895.py:21: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  elif not (pd.api.types.is_categorical_dtype(df_cleaning[col_rrn]) or pd.api.types.is_numeric_dtype(df_cleaning[col_rrn])):\n",
      "2025-05-17 02:01:18,865 - INFO - Nulos en 'review_rate_number' ANTES de la imputación: 319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Celda 12.12: Imputación de Nulos en 'review_rate_number' ---\n",
      "Nulos en 'review_rate_number' ANTES de la imputación: 319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:20,728 - INFO - Nulos en 'review_rate_number' DESPUÉS de la imputación: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulos en 'review_rate_number' DESPUÉS de la imputación: 0\n",
      "Se intentó imputar 319 valores en 'review_rate_number'.\n",
      "\n",
      "Distribución de valores para 'review_rate_number' después de la imputación:\n",
      "|   review_rate_number |   proportion |\n",
      "|---------------------:|-------------:|\n",
      "|                    1 |    0.0900076 |\n",
      "|                    2 |    0.225088  |\n",
      "|                    3 |    0.227175  |\n",
      "|                    4 |    0.228458  |\n",
      "|                    5 |    0.229272  |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1305523/2430475895.py:88: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if not pd.api.types.is_categorical_dtype(df_cleaning[col_rrn]) and df_cleaning[col_rrn].notna().any():\n"
     ]
    }
   ],
   "source": [
    "# Celda 12.12: Imputación de Nulos en 'review_rate_number' con Moda Condicionada\n",
    "logging.info(\"Celda 12.12: Iniciando imputación de nulos para 'review_rate_number' con moda condicionada.\")\n",
    "print(\"\\n--- Celda 12.12: Imputación de Nulos en 'review_rate_number' ---\")\n",
    "\n",
    "if not df_cleaning.empty:\n",
    "    col_rrn = 'review_rate_number' # Nombre ya normalizado\n",
    "    col_group = 'neighbourhood_group'\n",
    "    col_nh = 'neighbourhood'\n",
    "    \n",
    "    # Verificar existencia de columnas\n",
    "    if col_rrn not in df_cleaning.columns:\n",
    "        logging.warning(f\"Columna '{col_rrn}' no encontrada. Omitiendo imputación.\")\n",
    "        print(f\"Advertencia: Columna '{col_rrn}' no encontrada.\")\n",
    "    elif col_group not in df_cleaning.columns:\n",
    "        logging.warning(f\"Columna de referencia '{col_group}' no encontrada. Estrategia principal no aplicable.\")\n",
    "        print(f\"Advertencia: Columna '{col_group}' no encontrada.\")\n",
    "    elif col_nh not in df_cleaning.columns:\n",
    "        logging.warning(f\"Columna de referencia '{col_nh}' no encontrada. Estrategia secundaria no aplicable.\")\n",
    "        print(f\"Advertencia: Columna '{col_nh}' no encontrada.\")\n",
    "    # Verificar que review_rate_number es categórica o se puede tratar como tal\n",
    "    elif not (pd.api.types.is_categorical_dtype(df_cleaning[col_rrn]) or pd.api.types.is_numeric_dtype(df_cleaning[col_rrn])):\n",
    "        logging.warning(f\"Columna '{col_rrn}' no es categórica ni numérica (Tipo: {df_cleaning[col_rrn].dtype}). Estrategia de moda no aplicable directamente.\")\n",
    "        print(f\"Advertencia: Columna '{col_rrn}' no es categórica ni numérica. No se puede imputar con moda fácilmente.\")\n",
    "    else:\n",
    "        nulos_antes_rrn = df_cleaning[col_rrn].isnull().sum()\n",
    "        logging.info(f\"Nulos en '{col_rrn}' ANTES de la imputación: {nulos_antes_rrn}\")\n",
    "        print(f\"Nulos en '{col_rrn}' ANTES de la imputación: {nulos_antes_rrn}\")\n",
    "\n",
    "        if nulos_antes_rrn == 0:\n",
    "            logging.info(f\"No hay nulos que imputar en '{col_rrn}'.\")\n",
    "            print(f\"No hay nulos que imputar en '{col_rrn}'.\")\n",
    "        else:\n",
    "            imputed_count_rrn = 0\n",
    "            # Iterar sobre las filas donde 'review_rate_number' es nulo\n",
    "            for index in df_cleaning[df_cleaning[col_rrn].isnull()].index:\n",
    "                current_group_val = df_cleaning.loc[index, col_group]\n",
    "                current_nh_val = df_cleaning.loc[index, col_nh]\n",
    "                \n",
    "                imputed_rrn_value = pd.NA\n",
    "\n",
    "                # Estrategia 1: Moda por 'neighbourhood_group'\n",
    "                if pd.notna(current_group_val):\n",
    "                    moda_by_group = df_cleaning[\n",
    "                        (df_cleaning[col_group] == current_group_val) & \n",
    "                        (df_cleaning[col_rrn].notna())\n",
    "                    ][col_rrn].mode()\n",
    "                    if not moda_by_group.empty:\n",
    "                        imputed_rrn_value = moda_by_group.iloc[0]\n",
    "                        logging.debug(f\"Imputando '{col_rrn}' para índice {index} con moda de '{col_group}' ({current_group_val}): {imputed_rrn_value}\")\n",
    "\n",
    "                # Estrategia 2: Moda por 'neighbourhood' (si la anterior falló o no aplicó)\n",
    "                if pd.isna(imputed_rrn_value) and pd.notna(current_nh_val):\n",
    "                    moda_by_nh = df_cleaning[\n",
    "                        (df_cleaning[col_nh] == current_nh_val) &\n",
    "                        (df_cleaning[col_rrn].notna())\n",
    "                    ][col_rrn].mode()\n",
    "                    if not moda_by_nh.empty:\n",
    "                        imputed_rrn_value = moda_by_nh.iloc[0]\n",
    "                        logging.debug(f\"Imputando '{col_rrn}' para índice {index} con moda de '{col_nh}' ({current_nh_val}): {imputed_rrn_value}\")\n",
    "                \n",
    "                # Estrategia 3: Moda Global (Fallback si las anteriores fallaron)\n",
    "                if pd.isna(imputed_rrn_value):\n",
    "                    moda_global_rrn = df_cleaning[col_rrn].dropna().mode()\n",
    "                    if not moda_global_rrn.empty:\n",
    "                        imputed_rrn_value = moda_global_rrn.iloc[0]\n",
    "                        logging.debug(f\"Imputando '{col_rrn}' para índice {index} con MODA GLOBAL: {imputed_rrn_value}\")\n",
    "                    else:\n",
    "                        logging.warning(f\"No se pudo calcular la moda global para '{col_rrn}'. El valor para el índice {index} permanece nulo.\")\n",
    "                \n",
    "                # Aplicar el valor imputado\n",
    "                if pd.notna(imputed_rrn_value):\n",
    "                    # Si la columna es categórica, el valor imputado debe ser una categoría existente o se añadirá.\n",
    "                    # Si el valor imputado no es del tipo correcto (ej. int cuando la categoría espera strings), convertir.\n",
    "                    # Como review_rate_number es 'category' de números, la moda será uno de esos números.\n",
    "                    df_cleaning.loc[index, col_rrn] = imputed_rrn_value\n",
    "                    imputed_count_rrn +=1\n",
    "            \n",
    "            nulos_despues_rrn = df_cleaning[col_rrn].isnull().sum()\n",
    "            logging.info(f\"Nulos en '{col_rrn}' DESPUÉS de la imputación: {nulos_despues_rrn}\")\n",
    "            print(f\"Nulos en '{col_rrn}' DESPUÉS de la imputación: {nulos_despues_rrn}\")\n",
    "            if imputed_count_rrn > 0:\n",
    "                print(f\"Se intentó imputar {imputed_count_rrn} valores en '{col_rrn}'.\")\n",
    "            \n",
    "            # Verificar estadísticas y distribución\n",
    "            print(f\"\\nDistribución de valores para '{col_rrn}' después de la imputación:\")\n",
    "            print(df_cleaning[col_rrn].value_counts(dropna=False, normalize=True).sort_index().to_markdown())\n",
    "            # Asegurar que el tipo sigue siendo category\n",
    "            if not pd.api.types.is_categorical_dtype(df_cleaning[col_rrn]) and df_cleaning[col_rrn].notna().any():\n",
    "                logging.info(f\"Re-convirtiendo '{col_rrn}' a tipo 'category' después de la imputación.\")\n",
    "                df_cleaning[col_rrn] = df_cleaning[col_rrn].astype('category')\n",
    "                print(f\"Columna '{col_rrn}' re-convertida a tipo 'category'. Nuevo tipo: {df_cleaning[col_rrn].dtype}\")\n",
    "\n",
    "else:\n",
    "    logging.warning(\"El DataFrame df_cleaning está vacío. No se puede realizar la imputación de '{col_rrn}'.\")\n",
    "    print(\"El DataFrame df_cleaning está vacío.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "id": "05b268c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:20,738 - INFO - Celda 12.13: Iniciando imputación de nulos para 'service_fee' con la mediana.\n",
      "2025-05-17 02:01:20,740 - INFO - Nulos en 'service_fee' ANTES de la imputación con mediana: 273\n",
      "2025-05-17 02:01:20,744 - INFO - La mediana calculada para 'service_fee' es: 125.00\n",
      "2025-05-17 02:01:20,746 - INFO - Nulos en 'service_fee' DESPUÉS de la imputación: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Celda 12.13: Imputación de Nulos en 'service_fee' con Mediana ---\n",
      "Nulos en 'service_fee' ANTES de la imputación con mediana: 273\n",
      "La mediana calculada para 'service_fee' es: 125.00\n",
      "Nulos en 'service_fee' DESPUÉS de la imputación: 0\n",
      "Se imputaron 273 valores nulos en 'service_fee' con la mediana (125.00).\n",
      "\n",
      "Estadísticas descriptivas actualizadas para 'service_fee':\n",
      "|       |   service_fee |\n",
      "|:------|--------------:|\n",
      "| count |   102058      |\n",
      "| mean  |      125.039  |\n",
      "| std   |       66.2371 |\n",
      "| min   |       10      |\n",
      "| 25%   |       68      |\n",
      "| 50%   |      125      |\n",
      "| 75%   |      182      |\n",
      "| max   |      240      |\n"
     ]
    }
   ],
   "source": [
    "# Celda 12.13: Imputación de Nulos en 'service_fee' con la Mediana\n",
    "logging.info(\"Celda 12.13: Iniciando imputación de nulos para 'service_fee' con la mediana.\")\n",
    "print(\"\\n--- Celda 12.13: Imputación de Nulos en 'service_fee' con Mediana ---\")\n",
    "\n",
    "if not df_cleaning.empty:\n",
    "    col_sf = 'service_fee' # Nombre ya normalizado\n",
    "    \n",
    "    if col_sf not in df_cleaning.columns:\n",
    "        logging.warning(f\"Columna '{col_sf}' no encontrada. Omitiendo imputación.\")\n",
    "        print(f\"Advertencia: Columna '{col_sf}' no encontrada. Se omite su imputación.\")\n",
    "    elif not pd.api.types.is_numeric_dtype(df_cleaning[col_sf]): # Ya debería ser float64\n",
    "        logging.warning(f\"Columna '{col_sf}' no es de tipo numérico (Tipo: {df_cleaning[col_sf].dtype}). No se puede imputar con la mediana.\")\n",
    "        print(f\"Advertencia: Columna '{col_sf}' no es de tipo numérico. No se puede imputar con la mediana.\")\n",
    "    else:\n",
    "        nulos_antes_sf = df_cleaning[col_sf].isnull().sum()\n",
    "        logging.info(f\"Nulos en '{col_sf}' ANTES de la imputación con mediana: {nulos_antes_sf}\")\n",
    "        print(f\"Nulos en '{col_sf}' ANTES de la imputación con mediana: {nulos_antes_sf}\")\n",
    "\n",
    "        if nulos_antes_sf == 0:\n",
    "            logging.info(f\"No hay nulos que imputar en '{col_sf}'.\")\n",
    "            print(f\"No hay nulos que imputar en '{col_sf}'.\")\n",
    "        else:\n",
    "            # Calcular la mediana de los valores existentes (no nulos)\n",
    "            median_sf = df_cleaning[col_sf].dropna().median()\n",
    "            \n",
    "            if pd.notna(median_sf):\n",
    "                logging.info(f\"La mediana calculada para '{col_sf}' es: {median_sf:.2f}\")\n",
    "                print(f\"La mediana calculada para '{col_sf}' es: {median_sf:.2f}\")\n",
    "                \n",
    "                # Imputar los valores nulos (NaN) con la mediana\n",
    "                df_cleaning[col_sf].fillna(median_sf, inplace=True)\n",
    "                imputed_count_sf = nulos_antes_sf - df_cleaning[col_sf].isnull().sum()\n",
    "                \n",
    "                nulos_despues_sf = df_cleaning[col_sf].isnull().sum()\n",
    "                logging.info(f\"Nulos en '{col_sf}' DESPUÉS de la imputación: {nulos_despues_sf}\")\n",
    "                print(f\"Nulos en '{col_sf}' DESPUÉS de la imputación: {nulos_despues_sf}\")\n",
    "                if imputed_count_sf > 0:\n",
    "                    print(f\"Se imputaron {imputed_count_sf} valores nulos en '{col_sf}' con la mediana ({median_sf:.2f}).\")\n",
    "            else:\n",
    "                logging.warning(f\"No se pudo calcular la mediana para '{col_sf}' (quizás todos los valores son nulos). No se imputaron valores.\")\n",
    "                print(f\"Advertencia: No se pudo calcular la mediana para '{col_sf}'. No se imputaron valores.\")\n",
    "                \n",
    "            # Verificar estadísticas después de la imputación\n",
    "            print(f\"\\nEstadísticas descriptivas actualizadas para '{col_sf}':\")\n",
    "            print(df_cleaning[col_sf].describe().to_markdown())\n",
    "else:\n",
    "    logging.warning(\"El DataFrame df_cleaning está vacío. No se puede realizar la imputación de '{col_sf}'.\")\n",
    "    print(\"El DataFrame df_cleaning está vacío.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "id": "46adaa43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:20,761 - INFO - Celda 12.14: Iniciando imputación de nulos para 'name' con 'Desconocido'.\n",
      "2025-05-17 02:01:20,769 - INFO - Nulos en 'name' ANTES de la imputación: 250\n",
      "2025-05-17 02:01:20,794 - INFO - Nulos en 'name' DESPUÉS de la imputación: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Celda 12.14: Imputación de Nulos en 'name' ---\n",
      "Nulos en 'name' ANTES de la imputación: 250\n",
      "Nulos en 'name' DESPUÉS de la imputación: 0\n",
      "Se imputaron 250 valores nulos en 'name' con 'Desconocido'.\n",
      "\n",
      "Frecuencia de valores en 'name' (mostrando 'Desconocido' si existe):\n",
      "| name                           |   count |\n",
      "|:-------------------------------|--------:|\n",
      "| Desconocido                    |     250 |\n",
      "| Home away from home            |      33 |\n",
      "| Hillside Hotel                 |      30 |\n",
      "| Water View King Bed Hotel Room |      30 |\n",
      "| Brooklyn Apartment             |      27 |\n",
      "  Conteo de 'Desconocido': 250\n",
      "Tipo de dato final para 'name': object\n"
     ]
    }
   ],
   "source": [
    "# Celda 12.14: Imputación de Nulos en 'name' con \"Desconocido\"\n",
    "logging.info(\"Celda 12.14: Iniciando imputación de nulos para 'name' con 'Desconocido'.\")\n",
    "print(\"\\n--- Celda 12.14: Imputación de Nulos en 'name' ---\")\n",
    "\n",
    "if not df_cleaning.empty:\n",
    "    col_name_listing = 'name' # Nombre ya normalizado\n",
    "    valor_imputacion_name = \"Desconocido\" # O \"No especificado\", \"Unknown\", etc.\n",
    "    \n",
    "    if col_name_listing not in df_cleaning.columns:\n",
    "        logging.warning(f\"Columna '{col_name_listing}' no encontrada. Omitiendo imputación.\")\n",
    "        print(f\"Advertencia: Columna '{col_name_listing}' no encontrada. Se omite su imputación.\")\n",
    "    # No se necesita verificar el tipo estrictamente como numérico, ya que fillna con string funciona en object/string\n",
    "    elif not (pd.api.types.is_object_dtype(df_cleaning[col_name_listing]) or pd.api.types.is_string_dtype(df_cleaning[col_name_listing]) or pd.api.types.is_categorical_dtype(df_cleaning[col_name_listing])):\n",
    "        logging.warning(f\"Columna '{col_name_listing}' no es de tipo object, string o category (Tipo: {df_cleaning[col_name_listing].dtype}). La imputación con string podría ser inesperada.\")\n",
    "        print(f\"Advertencia: Columna '{col_name_listing}' no es de un tipo de texto esperado. Revisar antes de imputar con '{valor_imputacion_name}'.\")\n",
    "    else:\n",
    "        nulos_antes_name = df_cleaning[col_name_listing].isnull().sum()\n",
    "        logging.info(f\"Nulos en '{col_name_listing}' ANTES de la imputación: {nulos_antes_name}\")\n",
    "        print(f\"Nulos en '{col_name_listing}' ANTES de la imputación: {nulos_antes_name}\")\n",
    "\n",
    "        if nulos_antes_name == 0:\n",
    "            logging.info(f\"No hay nulos que imputar en '{col_name_listing}'.\")\n",
    "            print(f\"No hay nulos que imputar en '{col_name_listing}'.\")\n",
    "        else:\n",
    "            # Imputar los valores nulos (pd.NA o None) con el string especificado\n",
    "            df_cleaning[col_name_listing].fillna(valor_imputacion_name, inplace=True)\n",
    "            imputed_count_name = nulos_antes_name - df_cleaning[col_name_listing].isnull().sum()\n",
    "            \n",
    "            nulos_despues_name = df_cleaning[col_name_listing].isnull().sum()\n",
    "            logging.info(f\"Nulos en '{col_name_listing}' DESPUÉS de la imputación: {nulos_despues_name}\")\n",
    "            print(f\"Nulos en '{col_name_listing}' DESPUÉS de la imputación: {nulos_despues_name}\")\n",
    "            if imputed_count_name > 0:\n",
    "                print(f\"Se imputaron {imputed_count_name} valores nulos en '{col_name_listing}' con '{valor_imputacion_name}'.\")\n",
    "            \n",
    "            # Verificar la frecuencia del valor imputado\n",
    "            print(f\"\\nFrecuencia de valores en '{col_name_listing}' (mostrando 'Desconocido' si existe):\")\n",
    "            if valor_imputacion_name in df_cleaning[col_name_listing].unique():\n",
    "                print(df_cleaning[col_name_listing].value_counts().head().to_markdown()) # Mostrar los más comunes\n",
    "                print(f\"  Conteo de '{valor_imputacion_name}': {df_cleaning[col_name_listing].value_counts().get(valor_imputacion_name, 0)}\")\n",
    "            else:\n",
    "                print(f\"El valor '{valor_imputacion_name}' no se encontró después de la imputación (esto sería inesperado si hubo nulos).\")\n",
    "                \n",
    "            # Asegurar que el tipo de dato sigue siendo apropiado (object o string)\n",
    "            # Si era category, fillna podría añadir la nueva categoría si no existía.\n",
    "            if not (pd.api.types.is_object_dtype(df_cleaning[col_name_listing]) or pd.api.types.is_string_dtype(df_cleaning[col_name_listing])):\n",
    "                if pd.api.types.is_categorical_dtype(df_cleaning[col_name_listing]):\n",
    "                     # Si era category y 'Desconocido' no era una categoría, se habrá añadido\n",
    "                     if valor_imputacion_name not in df_cleaning[col_name_listing].cat.categories:\n",
    "                         # Esto es informativo, fillna en category maneja esto.\n",
    "                         logging.info(f\"'{valor_imputacion_name}' fue añadido como nueva categoría a '{col_name_listing}'.\")\n",
    "                else: # Si cambió a otro tipo inesperado\n",
    "                    logging.warning(f\"El tipo de '{col_name_listing}' cambió a {df_cleaning[col_name_listing].dtype}. Convirtiendo a string.\")\n",
    "                    df_cleaning[col_name_listing] = df_cleaning[col_name_listing].astype(str)\n",
    "            print(f\"Tipo de dato final para '{col_name_listing}': {df_cleaning[col_name_listing].dtype}\")\n",
    "\n",
    "else:\n",
    "    logging.warning(\"El DataFrame df_cleaning está vacío. No se puede realizar la imputación de '{col_name_listing}'.\")\n",
    "    print(\"El DataFrame df_cleaning está vacío.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "id": "25b83355",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:20,911 - INFO - Celda 12.15: Iniciando imputación de nulos para 'price' con media condicionada por 'neighbourhood'.\n",
      "2025-05-17 02:01:20,913 - INFO - Nulos en 'price' ANTES de la imputación: 247\n",
      "2025-05-17 02:01:20,914 - INFO - Media global de 'price' (para fallback): 625.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Celda 12.15: Imputación de Nulos en 'price' ---\n",
      "Nulos en 'price' ANTES de la imputación: 247\n",
      "Media global de 'price' (para fallback): 625.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:21,367 - INFO - Nulos en 'price' DESPUÉS de la imputación: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulos en 'price' DESPUÉS de la imputación: 0\n",
      "Se imputaron 247 valores en 'price'.\n",
      "\n",
      "Estadísticas descriptivas actualizadas para 'price':\n",
      "|       |      price |\n",
      "|:------|-----------:|\n",
      "| count | 102058     |\n",
      "| mean  |    625.357 |\n",
      "| std   |    331.272 |\n",
      "| min   |     50     |\n",
      "| 25%   |    341     |\n",
      "| 50%   |    625     |\n",
      "| 75%   |    912     |\n",
      "| max   |   1200     |\n"
     ]
    }
   ],
   "source": [
    "# Celda 12.15: Imputación de Nulos en 'price' con Media Condicionada por 'neighbourhood'\n",
    "logging.info(\"Celda 12.15: Iniciando imputación de nulos para 'price' con media condicionada por 'neighbourhood'.\")\n",
    "print(\"\\n--- Celda 12.15: Imputación de Nulos en 'price' ---\")\n",
    "\n",
    "if not df_cleaning.empty:\n",
    "    col_price = 'price' # Nombre ya normalizado\n",
    "    col_nh_ref = 'neighbourhood' # Columna de referencia, ya normalizada\n",
    "    \n",
    "    if col_price not in df_cleaning.columns:\n",
    "        logging.warning(f\"Columna '{col_price}' no encontrada. Omitiendo imputación.\")\n",
    "        print(f\"Advertencia: Columna '{col_price}' no encontrada. Se omite su imputación.\")\n",
    "    elif col_nh_ref not in df_cleaning.columns:\n",
    "        logging.warning(f\"Columna de referencia '{col_nh_ref}' no encontrada. No se puede imputar '{col_price}' con esta estrategia principal.\")\n",
    "        print(f\"Advertencia: Columna '{col_nh_ref}' no encontrada.\")\n",
    "    elif not pd.api.types.is_numeric_dtype(df_cleaning[col_price]): # Ya debería ser float64\n",
    "        logging.warning(f\"Columna '{col_price}' no es de tipo numérico (Tipo: {df_cleaning[col_price].dtype}). No se puede imputar con la media.\")\n",
    "        print(f\"Advertencia: Columna '{col_price}' no es de tipo numérico. No se puede imputar con la media.\")\n",
    "    else:\n",
    "        nulos_antes_price = df_cleaning[col_price].isnull().sum()\n",
    "        logging.info(f\"Nulos en '{col_price}' ANTES de la imputación: {nulos_antes_price}\")\n",
    "        print(f\"Nulos en '{col_price}' ANTES de la imputación: {nulos_antes_price}\")\n",
    "\n",
    "        if nulos_antes_price == 0:\n",
    "            logging.info(f\"No hay nulos que imputar en '{col_price}'.\")\n",
    "            print(f\"No hay nulos que imputar en '{col_price}'.\")\n",
    "        else:\n",
    "            # Calcular la media global como fallback una sola vez\n",
    "            media_global_price = df_cleaning[col_price].dropna().mean()\n",
    "            if pd.notna(media_global_price):\n",
    "                 logging.info(f\"Media global de '{col_price}' (para fallback): {media_global_price:.2f}\")\n",
    "                 print(f\"Media global de '{col_price}' (para fallback): {media_global_price:.2f}\")\n",
    "            else:\n",
    "                 logging.warning(f\"No se pudo calcular la media global de '{col_price}'. El fallback podría no funcionar.\")\n",
    "                 print(f\"Advertencia: No se pudo calcular la media global de '{col_price}'.\")\n",
    "\n",
    "\n",
    "            imputed_count_price = 0\n",
    "            # Iterar sobre las filas donde 'price' es nulo\n",
    "            for index in df_cleaning[df_cleaning[col_price].isnull()].index:\n",
    "                current_neighbourhood_val = df_cleaning.loc[index, col_nh_ref]\n",
    "                \n",
    "                imputed_price_value = pd.NA # Valor por defecto\n",
    "\n",
    "                # Estrategia 1: Media por 'neighbourhood'\n",
    "                if pd.notna(current_neighbourhood_val):\n",
    "                    mean_price_neighbourhood = df_cleaning[\n",
    "                        (df_cleaning[col_nh_ref] == current_neighbourhood_val) &\n",
    "                        (df_cleaning[col_price].notna()) # Usar solo precios no nulos para la media\n",
    "                    ][col_price].mean()\n",
    "                    \n",
    "                    if pd.notna(mean_price_neighbourhood):\n",
    "                        imputed_price_value = mean_price_neighbourhood\n",
    "                        logging.debug(f\"Imputando '{col_price}' para índice {index} con media de '{col_nh_ref}' ({current_neighbourhood_val}): {imputed_price_value:.2f}\")\n",
    "                    else:\n",
    "                        logging.warning(f\"No se pudo calcular la media de '{col_price}' para '{col_nh_ref}' '{current_neighbourhood_val}' (índice {index}). Usando fallback si es posible.\")\n",
    "                \n",
    "                # Estrategia 2: Media Global (Fallback)\n",
    "                if pd.isna(imputed_price_value) and pd.notna(media_global_price):\n",
    "                    imputed_price_value = media_global_price\n",
    "                    logging.debug(f\"Imputando '{col_price}' para índice {index} con MEDIA GLOBAL: {imputed_price_value:.2f}\")\n",
    "                elif pd.isna(imputed_price_value) and pd.isna(media_global_price):\n",
    "                     logging.error(f\"No se pudo imputar '{col_price}' para el índice {index} ni con media de neighbourhood ni con media global (no calculable).\")\n",
    "\n",
    "\n",
    "                # Aplicar el valor imputado\n",
    "                if pd.notna(imputed_price_value):\n",
    "                    df_cleaning.loc[index, col_price] = imputed_price_value\n",
    "                    imputed_count_price +=1\n",
    "            \n",
    "            nulos_despues_price = df_cleaning[col_price].isnull().sum()\n",
    "            logging.info(f\"Nulos en '{col_price}' DESPUÉS de la imputación: {nulos_despues_price}\")\n",
    "            print(f\"Nulos en '{col_price}' DESPUÉS de la imputación: {nulos_despues_price}\")\n",
    "            if imputed_count_price > 0: # O nulos_antes_price > nulos_despues_price\n",
    "                print(f\"Se imputaron {nulos_antes_price - nulos_despues_price} valores en '{col_price}'.\")\n",
    "            elif nulos_antes_price == nulos_despues_price and nulos_antes_price > 0:\n",
    "                 print(f\"No se pudieron imputar los nulos restantes en '{col_price}' con la estrategia actual (posiblemente media global no calculable y neighbourhoods sin datos).\")\n",
    "\n",
    "            # Verificar estadísticas después de la imputación\n",
    "            print(f\"\\nEstadísticas descriptivas actualizadas para '{col_price}':\")\n",
    "            print(df_cleaning[col_price].describe().to_markdown())\n",
    "else:\n",
    "    logging.warning(\"El DataFrame df_cleaning está vacío. No se puede realizar la imputación de '{col_price}'.\")\n",
    "    print(\"El DataFrame df_cleaning está vacío.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1024,
   "id": "e26745c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:21,381 - INFO - Celda 12.16: Iniciando imputación de nulos para 'construction_year' con un valor placeholder.\n",
      "2025-05-17 02:01:21,382 - INFO - Nulos (pd.NA) en 'construction_year' ANTES de la imputación: 214\n",
      "2025-05-17 02:01:21,384 - INFO - Nulos en 'construction_year' DESPUÉS de la imputación: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Celda 12.16: Imputación de Nulos en 'construction_year' ---\n",
      "Nulos (pd.NA) en 'construction_year' ANTES de la imputación: 214\n",
      "Nulos en 'construction_year' DESPUÉS de la imputación: 0\n",
      "Se imputaron 214 valores nulos en 'construction_year' con '0'.\n",
      "\n",
      "Frecuencia del valor placeholder '0' en 'construction_year' después de la imputación: 214\n",
      "\n",
      "Estadísticas descriptivas actualizadas para 'construction_year':\n",
      "|       |   construction_year |\n",
      "|:------|--------------------:|\n",
      "| count |         102058      |\n",
      "| mean  |           2008.27   |\n",
      "| std   |             92.2383 |\n",
      "| min   |              0      |\n",
      "| 25%   |           2007      |\n",
      "| 50%   |           2012      |\n",
      "| 75%   |           2017      |\n",
      "| max   |           2022      |\n"
     ]
    }
   ],
   "source": [
    "# Celda 12.16: Imputación de Nulos en 'construction_year' con un valor placeholder (e.g., 0)\n",
    "logging.info(\"Celda 12.16: Iniciando imputación de nulos para 'construction_year' con un valor placeholder.\")\n",
    "print(\"\\n--- Celda 12.16: Imputación de Nulos en 'construction_year' ---\")\n",
    "\n",
    "if not df_cleaning.empty:\n",
    "    col_cy = 'construction_year' # Nombre ya normalizado\n",
    "    valor_placeholder_cy = 0  # Valor numérico para representar \"Desconocido\"\n",
    "                               # Podría ser -1, 999, o un año muy antiguo como 1000 si 0 es problemático.\n",
    "                               # Si 0 ya existe y tiene significado, elige otro.\n",
    "    \n",
    "    if col_cy not in df_cleaning.columns:\n",
    "        logging.warning(f\"Columna '{col_cy}' no encontrada. Omitiendo imputación.\")\n",
    "        print(f\"Advertencia: Columna '{col_cy}' no encontrada. Se omite su imputación.\")\n",
    "    elif not pd.api.types.is_numeric_dtype(df_cleaning[col_cy]): # Ya debería ser Int64\n",
    "        logging.warning(f\"Columna '{col_cy}' no es de tipo numérico (Tipo: {df_cleaning[col_cy].dtype}). No se puede imputar con {valor_placeholder_cy}.\")\n",
    "        print(f\"Advertencia: Columna '{col_cy}' no es de tipo numérico. No se puede imputar.\")\n",
    "    else:\n",
    "        nulos_antes_cy = df_cleaning[col_cy].isnull().sum()\n",
    "        logging.info(f\"Nulos (pd.NA) en '{col_cy}' ANTES de la imputación: {nulos_antes_cy}\")\n",
    "        print(f\"Nulos (pd.NA) en '{col_cy}' ANTES de la imputación: {nulos_antes_cy}\")\n",
    "\n",
    "        if nulos_antes_cy == 0:\n",
    "            logging.info(f\"No hay nulos que imputar en '{col_cy}'.\")\n",
    "            print(f\"No hay nulos que imputar en '{col_cy}'.\")\n",
    "        else:\n",
    "            # Verificar si el valor placeholder ya existe y cuántas veces\n",
    "            if valor_placeholder_cy in df_cleaning[col_cy].unique():\n",
    "                count_placeholder_antes = df_cleaning[df_cleaning[col_cy] == valor_placeholder_cy].shape[0]\n",
    "                logging.info(f\"El valor placeholder '{valor_placeholder_cy}' ya existe {count_placeholder_antes} veces en '{col_cy}'.\")\n",
    "                print(f\"Advertencia: El valor placeholder '{valor_placeholder_cy}' ya existe {count_placeholder_antes} veces en '{col_cy}'.\")\n",
    "\n",
    "            # Imputar los valores nulos (pd.NA) con el valor placeholder\n",
    "            df_cleaning[col_cy].fillna(valor_placeholder_cy, inplace=True)\n",
    "            imputed_count_cy = nulos_antes_cy - df_cleaning[col_cy].isnull().sum()\n",
    "            \n",
    "            nulos_despues_cy = df_cleaning[col_cy].isnull().sum()\n",
    "            logging.info(f\"Nulos en '{col_cy}' DESPUÉS de la imputación: {nulos_despues_cy}\")\n",
    "            print(f\"Nulos en '{col_cy}' DESPUÉS de la imputación: {nulos_despues_cy}\") # Debería ser 0\n",
    "            if imputed_count_cy > 0:\n",
    "                print(f\"Se imputaron {imputed_count_cy} valores nulos en '{col_cy}' con '{valor_placeholder_cy}'.\")\n",
    "            \n",
    "            # Verificar la frecuencia del valor imputado\n",
    "            count_placeholder_despues = df_cleaning[df_cleaning[col_cy] == valor_placeholder_cy].shape[0]\n",
    "            print(f\"\\nFrecuencia del valor placeholder '{valor_placeholder_cy}' en '{col_cy}' después de la imputación: {count_placeholder_despues}\")\n",
    "            \n",
    "            print(f\"\\nEstadísticas descriptivas actualizadas para '{col_cy}':\")\n",
    "            print(df_cleaning[col_cy].describe().to_markdown())\n",
    "else:\n",
    "    logging.warning(\"El DataFrame df_cleaning está vacío. No se puede realizar la imputación de '{col_cy}'.\")\n",
    "    print(\"El DataFrame df_cleaning está vacío.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1025,
   "id": "b2b3e787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:21,400 - INFO - Celda 12.17: Iniciando imputación de nulos para 'number_of_reviews' con la media.\n",
      "2025-05-17 02:01:21,402 - INFO - Nulos en 'number_of_reviews' ANTES de la imputación con media: 183\n",
      "2025-05-17 02:01:21,404 - INFO - La media calculada para 'number_of_reviews' es: 27.517948466257668 (se usará como entero: 28)\n",
      "2025-05-17 02:01:21,405 - INFO - Nulos en 'number_of_reviews' DESPUÉS de la imputación: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Celda 12.17: Imputación de Nulos en 'number_of_reviews' con Media ---\n",
      "Nulos en 'number_of_reviews' ANTES de la imputación con media: 183\n",
      "La media calculada para 'number_of_reviews' es: 27.52 (se usará como entero: 28)\n",
      "Nulos en 'number_of_reviews' DESPUÉS de la imputación: 0\n",
      "Se imputaron 183 valores nulos en 'number_of_reviews' con la media (28).\n",
      "\n",
      "Estadísticas descriptivas actualizadas para 'number_of_reviews':\n",
      "|       |   number_of_reviews |\n",
      "|:------|--------------------:|\n",
      "| count |         102058      |\n",
      "| mean  |             27.5188 |\n",
      "| std   |             49.5273 |\n",
      "| min   |              0      |\n",
      "| 25%   |              1      |\n",
      "| 50%   |              7      |\n",
      "| 75%   |             30      |\n",
      "| max   |           1024      |\n"
     ]
    }
   ],
   "source": [
    "# Celda 12.17: Imputación de Nulos en 'number_of_reviews' con la Media\n",
    "logging.info(\"Celda 12.17: Iniciando imputación de nulos para 'number_of_reviews' con la media.\")\n",
    "print(\"\\n--- Celda 12.17: Imputación de Nulos en 'number_of_reviews' con Media ---\")\n",
    "\n",
    "if not df_cleaning.empty:\n",
    "    col_nor = 'number_of_reviews' # Nombre ya normalizado\n",
    "    \n",
    "    if col_nor not in df_cleaning.columns:\n",
    "        logging.warning(f\"Columna '{col_nor}' no encontrada. Omitiendo imputación.\")\n",
    "        print(f\"Advertencia: Columna '{col_nor}' no encontrada. Se omite su imputación.\")\n",
    "    elif not pd.api.types.is_numeric_dtype(df_cleaning[col_nor]): # Ya debería ser Int64\n",
    "        logging.warning(f\"Columna '{col_nor}' no es de tipo numérico (Tipo: {df_cleaning[col_nor].dtype}). No se puede imputar con la media.\")\n",
    "        print(f\"Advertencia: Columna '{col_nor}' no es de tipo numérico. No se puede imputar con la media.\")\n",
    "    else:\n",
    "        nulos_antes_nor = df_cleaning[col_nor].isnull().sum()\n",
    "        logging.info(f\"Nulos en '{col_nor}' ANTES de la imputación con media: {nulos_antes_nor}\")\n",
    "        print(f\"Nulos en '{col_nor}' ANTES de la imputación con media: {nulos_antes_nor}\")\n",
    "\n",
    "        if nulos_antes_nor == 0:\n",
    "            logging.info(f\"No hay nulos que imputar en '{col_nor}'.\")\n",
    "            print(f\"No hay nulos que imputar en '{col_nor}'.\")\n",
    "        else:\n",
    "            # Calcular la media de los valores existentes (no nulos)\n",
    "            mean_nor = df_cleaning[col_nor].dropna().mean()\n",
    "            \n",
    "            if pd.notna(mean_nor):\n",
    "                # Redondear la media y convertir a entero para mantener la consistencia del tipo Int64\n",
    "                mean_nor_int = int(round(mean_nor))\n",
    "\n",
    "                logging.info(f\"La media calculada para '{col_nor}' es: {mean_nor} (se usará como entero: {mean_nor_int})\")\n",
    "                print(f\"La media calculada para '{col_nor}' es: {mean_nor:.2f} (se usará como entero: {mean_nor_int})\")\n",
    "                \n",
    "                # Imputar los valores nulos (pd.NA) con la media entera\n",
    "                df_cleaning[col_nor].fillna(mean_nor_int, inplace=True)\n",
    "                imputed_count_nor = nulos_antes_nor - df_cleaning[col_nor].isnull().sum()\n",
    "                \n",
    "                nulos_despues_nor = df_cleaning[col_nor].isnull().sum()\n",
    "                logging.info(f\"Nulos en '{col_nor}' DESPUÉS de la imputación: {nulos_despues_nor}\")\n",
    "                print(f\"Nulos en '{col_nor}' DESPUÉS de la imputación: {nulos_despues_nor}\")\n",
    "                if imputed_count_nor > 0:\n",
    "                    print(f\"Se imputaron {imputed_count_nor} valores nulos en '{col_nor}' con la media ({mean_nor_int}).\")\n",
    "            else:\n",
    "                logging.warning(f\"No se pudo calcular la media para '{col_nor}' (quizás todos los valores son nulos). No se imputaron valores.\")\n",
    "                print(f\"Advertencia: No se pudo calcular la media para '{col_nor}'. No se imputaron valores.\")\n",
    "                \n",
    "            # Verificar estadísticas después de la imputación\n",
    "            print(f\"\\nEstadísticas descriptivas actualizadas para '{col_nor}':\")\n",
    "            print(df_cleaning[col_nor].describe().to_markdown())\n",
    "else:\n",
    "    logging.warning(\"El DataFrame df_cleaning está vacío. No se puede realizar la imputación de '{col_nor}'.\")\n",
    "    print(\"El DataFrame df_cleaning está vacío.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1026,
   "id": "5c03b132",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:21,423 - INFO - Celda 12.18: Corrigiendo errores tipográficos en 'neighbourhood_group'.\n",
      "/tmp/ipykernel_1305523/322709299.py:13: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  elif not (pd.api.types.is_categorical_dtype(df_cleaning[col_ng]) or \\\n",
      "/tmp/ipykernel_1305523/322709299.py:20: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(df_cleaning[col_ng]):\n",
      "2025-05-17 02:01:21,427 - INFO - Valores únicos en 'neighbourhood_group' ANTES de la corrección: ['Bronx', 'Brooklyn', 'Manhattan', 'None', 'Queens', 'Staten Island', 'brookln', 'manhatan']\n",
      "/tmp/ipykernel_1305523/322709299.py:54: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(original_dtype_ng):\n",
      "2025-05-17 02:01:21,429 - INFO - Reemplazado 'brookln' por 'Brooklyn' en 'neighbourhood_group' (1 ocurrencias).\n",
      "/tmp/ipykernel_1305523/322709299.py:54: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(original_dtype_ng):\n",
      "2025-05-17 02:01:21,432 - INFO - Reemplazado 'manhatan' por 'Manhattan' en 'neighbourhood_group' (1 ocurrencias).\n",
      "2025-05-17 02:01:21,433 - INFO - Correcciones tipográficas aplicadas.\n",
      "/tmp/ipykernel_1305523/322709299.py:72: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(original_dtype_ng):\n",
      "2025-05-17 02:01:21,435 - INFO - 'neighbourhood_group' re-categorizada para optimizar categorías.\n",
      "/tmp/ipykernel_1305523/322709299.py:89: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(df_cleaning[col_ng]):\n",
      "2025-05-17 02:01:21,435 - INFO - Valores únicos en 'neighbourhood_group' DESPUÉS de la corrección: ['Bronx', 'Brooklyn', 'Manhattan', 'None', 'Queens', 'Staten Island']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Celda 12.18: Corrección de Errores Tipográficos en 'neighbourhood_group' ---\n",
      "Valores únicos en 'neighbourhood_group' ANTES de la corrección: ['Bronx', 'Brooklyn', 'Manhattan', 'None', 'Queens', 'Staten Island', 'brookln', 'manhatan']\n",
      "Reemplazado 'brookln' por 'Brooklyn' en 'neighbourhood_group' (1 ocurrencias).\n",
      "Reemplazado 'manhatan' por 'Manhattan' en 'neighbourhood_group' (1 ocurrencias).\n",
      "'neighbourhood_group' re-categorizada.\n",
      "\n",
      "Valores únicos en 'neighbourhood_group' DESPUÉS de la corrección: ['Bronx', 'Brooklyn', 'Manhattan', 'None', 'Queens', 'Staten Island']\n",
      "Tipo de dato final para 'neighbourhood_group': category\n",
      "\n",
      "Distribución de valores para 'neighbourhood_group' después de la corrección:\n",
      "| neighbourhood_group   |   count |\n",
      "|:----------------------|--------:|\n",
      "| Manhattan             |   43558 |\n",
      "| Brooklyn              |   41631 |\n",
      "| Queens                |   13197 |\n",
      "| Bronx                 |    2694 |\n",
      "| Staten Island         |     949 |\n",
      "| None                  |      29 |\n"
     ]
    }
   ],
   "source": [
    "# Celda 12.18: Corrección de Errores Tipográficos en 'neighbourhood_group'\n",
    "logging.info(\"Celda 12.18: Corrigiendo errores tipográficos en 'neighbourhood_group'.\")\n",
    "print(\"\\n--- Celda 12.18: Corrección de Errores Tipográficos en 'neighbourhood_group' ---\")\n",
    "\n",
    "if not df_cleaning.empty:\n",
    "    col_ng = 'neighbourhood_group' # Nombre ya normalizado\n",
    "    \n",
    "    if col_ng not in df_cleaning.columns:\n",
    "        logging.warning(f\"Columna '{col_ng}' no encontrada. Omitiendo corrección de errores tipográficos.\")\n",
    "        print(f\"Advertencia: Columna '{col_ng}' no encontrada. Se omite la corrección.\")\n",
    "    # Asegurarse de que es un tipo donde .replace con dict y .str (si se usa) tiene sentido.\n",
    "    # Ya debería ser 'category' o 'object'/'string'.\n",
    "    elif not (pd.api.types.is_categorical_dtype(df_cleaning[col_ng]) or \\\n",
    "              pd.api.types.is_object_dtype(df_cleaning[col_ng]) or \\\n",
    "              pd.api.types.is_string_dtype(df_cleaning[col_ng])):\n",
    "        logging.warning(f\"Columna '{col_ng}' no es de un tipo adecuado para reemplazo de strings (Tipo: {df_cleaning[col_ng].dtype}).\")\n",
    "        print(f\"Advertencia: Columna '{col_ng}' no es de un tipo adecuado para corrección de errores tipográficos.\")\n",
    "    else:\n",
    "        # Primero, veamos los valores únicos ANTES de la corrección para confirmar\n",
    "        if pd.api.types.is_categorical_dtype(df_cleaning[col_ng]):\n",
    "            unique_values_before_ng = df_cleaning[col_ng].cat.categories.tolist()\n",
    "            if df_cleaning[col_ng].isnull().any(): # Si hay NaNs en la categoría\n",
    "                 unique_values_before_ng.append(pd.NA) # o np.nan si prefieres consistencia en la lista\n",
    "        else: # object o string\n",
    "            unique_values_before_ng = df_cleaning[col_ng].unique().tolist()\n",
    "            \n",
    "        logging.info(f\"Valores únicos en '{col_ng}' ANTES de la corrección: {unique_values_before_ng}\")\n",
    "        print(f\"Valores únicos en '{col_ng}' ANTES de la corrección: {unique_values_before_ng}\")\n",
    "\n",
    "        # Definir el mapeo de correcciones\n",
    "        # Estos son ejemplos basados en el PDF. Debes verificar tus propios datos para identificar errores.\n",
    "        corrections_map_ng = {\n",
    "            'brookln': 'Brooklyn',\n",
    "            'manhatan': 'Manhattan',\n",
    "            # Añade más correcciones aquí si las identificas, por ejemplo:\n",
    "            # 'Manhatann': 'Manhattan',\n",
    "            # 'queenz': 'Queens'\n",
    "        }\n",
    "        \n",
    "        # Aplicar las correcciones\n",
    "        # Si la columna es de tipo 'category', reemplazar las categorías y luego los valores.\n",
    "        # Si es object/string, reemplazar directamente.\n",
    "        \n",
    "        original_dtype_ng = df_cleaning[col_ng].dtype\n",
    "        num_changes_made = 0\n",
    "\n",
    "        # Haremos el replace directamente. Si es categoría, los valores no mapeados se mantienen.\n",
    "        # Si una categoría es reemplazada por otra existente, se fusionan.\n",
    "        # Si una categoría es reemplazada por una nueva, la nueva categoría se añade (si la columna es category).\n",
    "        \n",
    "        for incorrect_val, correct_val in corrections_map_ng.items():\n",
    "            # Contar cuántas filas se verán afectadas por este reemplazo específico\n",
    "            # Necesitamos manejar el caso si la columna es string/object o category\n",
    "            if pd.api.types.is_categorical_dtype(original_dtype_ng):\n",
    "                if incorrect_val in df_cleaning[col_ng].cat.categories:\n",
    "                    count_before_replace = (df_cleaning[col_ng] == incorrect_val).sum()\n",
    "                else:\n",
    "                    count_before_replace = 0 # El valor incorrecto no es una categoría, no se reemplazará\n",
    "            else: # object/string\n",
    "                count_before_replace = (df_cleaning[col_ng] == incorrect_val).sum()\n",
    "\n",
    "            if count_before_replace > 0:\n",
    "                df_cleaning[col_ng] = df_cleaning[col_ng].replace({incorrect_val: correct_val})\n",
    "                num_changes_made += count_before_replace # Asumimos que todos los reemplazos ocurrieron\n",
    "                logging.info(f\"Reemplazado '{incorrect_val}' por '{correct_val}' en '{col_ng}' ({count_before_replace} ocurrencias).\")\n",
    "                print(f\"Reemplazado '{incorrect_val}' por '{correct_val}' en '{col_ng}' ({count_before_replace} ocurrencias).\")\n",
    "\n",
    "        if num_changes_made > 0:\n",
    "            logging.info(\"Correcciones tipográficas aplicadas.\")\n",
    "            # Si la columna era categórica, es bueno recategorizar para eliminar las categorías antiguas si ya no se usan\n",
    "            # y asegurar que el tipo de dato es el deseado.\n",
    "            if pd.api.types.is_categorical_dtype(original_dtype_ng):\n",
    "                # Obtener las categorías que realmente existen en los datos después del replace\n",
    "                current_values_in_col = df_cleaning[col_ng].dropna().unique()\n",
    "                df_cleaning[col_ng] = pd.Categorical(df_cleaning[col_ng], categories=sorted(current_values_in_col), ordered=False)\n",
    "                logging.info(f\"'{col_ng}' re-categorizada para optimizar categorías.\")\n",
    "                print(f\"'{col_ng}' re-categorizada.\")\n",
    "            elif pd.api.types.is_object_dtype(original_dtype_ng) or pd.api.types.is_string_dtype(original_dtype_ng):\n",
    "                # Si era object/string y queremos convertirla a category por primera vez o de nuevo\n",
    "                if df_cleaning[col_ng].nunique(dropna=False) <= 20: # Umbral de ejemplo\n",
    "                    df_cleaning[col_ng] = df_cleaning[col_ng].astype('category')\n",
    "                    logging.info(f\"'{col_ng}' convertida a category después de las correcciones.\")\n",
    "                    print(f\"'{col_ng}' convertida a category después de las correcciones.\")\n",
    "        else:\n",
    "            logging.info(f\"No se encontraron valores que coincidan con el mapa de correcciones en '{col_ng}'.\")\n",
    "            print(f\"No se aplicaron correcciones tipográficas en '{col_ng}' (no se encontraron los valores incorrectos especificados).\")\n",
    "\n",
    "        # Mostrar valores únicos DESPUÉS de la corrección\n",
    "        if pd.api.types.is_categorical_dtype(df_cleaning[col_ng]):\n",
    "            unique_values_after_ng = df_cleaning[col_ng].cat.categories.tolist()\n",
    "            if df_cleaning[col_ng].isnull().any():\n",
    "                 unique_values_after_ng.append(pd.NA)\n",
    "        else:\n",
    "            unique_values_after_ng = df_cleaning[col_ng].unique().tolist()\n",
    "            \n",
    "        logging.info(f\"Valores únicos en '{col_ng}' DESPUÉS de la corrección: {unique_values_after_ng}\")\n",
    "        print(f\"\\nValores únicos en '{col_ng}' DESPUÉS de la corrección: {unique_values_after_ng}\")\n",
    "        print(f\"Tipo de dato final para '{col_ng}': {df_cleaning[col_ng].dtype}\")\n",
    "        print(f\"\\nDistribución de valores para '{col_ng}' después de la corrección:\")\n",
    "        print(df_cleaning[col_ng].value_counts(dropna=False).to_markdown())\n",
    "else:\n",
    "    logging.warning(\"El DataFrame df_cleaning está vacío. No se puede realizar la corrección en '{col_ng}'.\")\n",
    "    print(\"El DataFrame df_cleaning está vacío.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1027,
   "id": "0b15bc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:21,446 - INFO - Celda 12.3: Verificando el estado actual de valores nulos por columna en df_cleaning.\n",
      "2025-05-17 02:01:21,460 - INFO - No se encontraron valores nulos en df_cleaning actualmente.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Excelente! No se encontraron valores nulos en el DataFrame df_cleaning en este momento.\n"
     ]
    }
   ],
   "source": [
    "# Celda 12.3: Verificación del Estado Actual de Nulos\n",
    "logging.info(\"Celda 12.3: Verificando el estado actual de valores nulos por columna en df_cleaning.\")\n",
    "\n",
    "if not df_cleaning.empty:\n",
    "    nulos_actual_counts = df_cleaning.isnull().sum()\n",
    "    nulos_actual_percentage = (nulos_actual_counts / len(df_cleaning)) * 100\n",
    "    \n",
    "    df_nulos_actual = pd.DataFrame({\n",
    "        'Columna': df_cleaning.columns,\n",
    "        'Nulos': nulos_actual_counts,\n",
    "        'Porcentaje_Nulos': nulos_actual_percentage,\n",
    "        'Tipo_Dato': df_cleaning.dtypes # Añadir tipo de dato para contexto\n",
    "    }).reset_index(drop=True) # Asegurar que el índice sea simple\n",
    "    \n",
    "    # Mostrar solo columnas que AÚN tienen nulos, ordenadas de mayor a menor porcentaje\n",
    "    df_nulos_actual_sorted = df_nulos_actual[df_nulos_actual['Nulos'] > 0].sort_values(\n",
    "        by='Porcentaje_Nulos', ascending=False\n",
    "    )\n",
    "    \n",
    "    if not df_nulos_actual_sorted.empty:\n",
    "        print(\"Cantidad y porcentaje de valores nulos ACTUALES por columna (solo columnas con nulos):\")\n",
    "        print(df_nulos_actual_sorted.to_markdown(index=False))\n",
    "        logging.info(\"Tabla de estado actual de nulos por columna generada y mostrada.\")\n",
    "    else:\n",
    "        print(\"¡Excelente! No se encontraron valores nulos en el DataFrame df_cleaning en este momento.\")\n",
    "        logging.info(\"No se encontraron valores nulos en df_cleaning actualmente.\")\n",
    "else:\n",
    "    logging.warning(\"El DataFrame df_cleaning está vacío. No se pueden calcular los nulos.\")\n",
    "    print(\"El DataFrame df_cleaning está vacío.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1028,
   "id": "1cb87f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna 'country_code' eliminada de df_cleaning.\n"
     ]
    }
   ],
   "source": [
    "if 'country_code' in df_cleaning.columns:\n",
    "    df_cleaning.drop(columns=['country_code'], inplace=True)\n",
    "    print(\"Columna 'country_code' eliminada de df_cleaning.\")\n",
    "else:\n",
    "    print(\"La columna 'country_code' ya no existe en df_cleaning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1029,
   "id": "eb387398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 102058 entries, 0 to 102057\n",
      "Data columns (total 24 columns):\n",
      " #   Column                            Non-Null Count   Dtype         \n",
      "---  ------                            --------------   -----         \n",
      " 0   id                                102058 non-null  int64         \n",
      " 1   name                              102058 non-null  object        \n",
      " 2   host_id                           102058 non-null  int64         \n",
      " 3   neighbourhood_group               102058 non-null  category      \n",
      " 4   neighbourhood                     102058 non-null  category      \n",
      " 5   lat                               102058 non-null  float64       \n",
      " 6   long                              102058 non-null  float64       \n",
      " 7   country                           102058 non-null  category      \n",
      " 8   cancellation_policy               102058 non-null  category      \n",
      " 9   room_type                         102058 non-null  category      \n",
      " 10  construction_year                 102058 non-null  Int64         \n",
      " 11  price                             102058 non-null  float64       \n",
      " 12  service_fee                       102058 non-null  float64       \n",
      " 13  minimum_nights                    102058 non-null  Int64         \n",
      " 14  number_of_reviews                 102058 non-null  Int64         \n",
      " 15  last_review                       102058 non-null  datetime64[ns]\n",
      " 16  reviews_per_month                 102058 non-null  float64       \n",
      " 17  review_rate_number                102058 non-null  category      \n",
      " 18  calculated_host_listings_count    102058 non-null  Int64         \n",
      " 19  availability_365                  102058 non-null  Int64         \n",
      " 20  is_host_unconfirmed               102058 non-null  bool          \n",
      " 21  is_host_verified                  102058 non-null  bool          \n",
      " 22  is_instant_bookable_false_policy  102058 non-null  bool          \n",
      " 23  is_instant_bookable_true_policy   102058 non-null  bool          \n",
      "dtypes: Int64(5), bool(4), category(6), datetime64[ns](1), float64(5), int64(2), object(1)\n",
      "memory usage: 12.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_cleaning.info() # Mostrar información del DataFrame después de las imputaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1030,
   "id": "b4990049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'name', 'host_id', 'neighbourhood_group', 'neighbourhood', 'lat', 'long', 'country', 'cancellation_policy', 'room_type', 'construction_year', 'price', 'service_fee', 'minimum_nights', 'number_of_reviews', 'last_review', 'reviews_per_month', 'review_rate_number', 'calculated_host_listings_count', 'availability_365', 'is_host_unconfirmed', 'is_host_verified', 'is_instant_bookable_false_policy', 'is_instant_bookable_true_policy']\n"
     ]
    }
   ],
   "source": [
    "print(df_cleaning.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1031,
   "id": "ea0460f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:21,524 - INFO - Celda 13: Preparando para subir df_cleaning a la nueva tabla 'cleaned_airbnb_listings' en PostgreSQL.\n"
     ]
    }
   ],
   "source": [
    "# Celda 13: Subida del DataFrame Limpio a PostgreSQL\n",
    "\n",
    "# Variables para la nueva tabla\n",
    "NEW_TABLE_NAME = 'cleaned_airbnb_listings'\n",
    "logging.info(f\"Celda 13: Preparando para subir df_cleaning a la nueva tabla '{NEW_TABLE_NAME}' en PostgreSQL.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1032,
   "id": "d2d02b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:21,554 - INFO - Celda 13.1: Definiendo esquema y creando la tabla vacía 'cleaned_airbnb_listings'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Celda 13.1: Creando Tabla Vacía 'cleaned_airbnb_listings' ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:  la base de datos «airbnb» tiene una discordancia de versión de ordenamiento (“collation”)\n",
      "DETAIL:  La base de datos fue creada usando la versión de ordenamiento 2.31, pero el sistema operativo provee la versión 2.35.\n",
      "HINT:  Reconstruya todos los objetos en esta base de datos que usen el ordenamiento por omisión y ejecute ALTER DATABASE airbnb REFRESH COLLATION VERSION, o construya PostgreSQL con la versión correcta de la biblioteca.\n",
      "2025-05-17 02:01:21,575 - INFO - Ejecutando: DROP TABLE IF EXISTS \"cleaned_airbnb_listings\";\n",
      "2025-05-17 02:01:21,578 - INFO - Tabla 'cleaned_airbnb_listings' eliminada si existía.\n",
      "2025-05-17 02:01:21,580 - INFO - Ejecutando: \n",
      "    CREATE TABLE IF NOT EXISTS \"cleaned_airbnb_listings\" (\n",
      "        \"id\" BIGINT PRIMARY KEY, \"name\" TEXT, \"host_id\" BIGINT, \"neighbourhood_group\" TEXT, \"neighbourhood\" TEXT, \"lat\" DOUBLE PRECISION, \"long\" DOUBLE PRECISION, \"country\" TEXT, \"cancellation_policy\" TEXT, \"room_type\" TEXT, \"construction_year\" INTEGER, \"price\" DOUBLE PRECISION, \"service_fee\" DOUBLE PRECISION, \"minimum_nights\" INTEGER, \"number_of_reviews\" INTEGER, \"last_review\" TIMESTAMP WITHOUT TIME ZONE, \"reviews_per_month\" DOUBLE PRECISION, \"review_rate_number\" INTEGER, \"calculated_host_listings_count\" INTEGER, \"availability_365\" INTEGER, \"is_host_unconfirmed\" BOOLEAN, \"is_host_verified\" BOOLEAN, \"is_instant_bookable_false_policy\" BOOLEAN, \"is_instant_bookable_true_policy\" BOOLEAN\n",
      "    );\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabla 'cleaned_airbnb_listings' eliminada si existía.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:21,595 - INFO - Tabla vacía 'cleaned_airbnb_listings' creada exitosamente con el esquema definido.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabla vacía 'cleaned_airbnb_listings' creada exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# Celda 13.1: Definir Esquema y Crear Tabla Vacía en PostgreSQL\n",
    "logging.info(f\"Celda 13.1: Definiendo esquema y creando la tabla vacía '{NEW_TABLE_NAME}'.\")\n",
    "print(f\"\\n--- Celda 13.1: Creando Tabla Vacía '{NEW_TABLE_NAME}' ---\")\n",
    "\n",
    "# Asegúrate de que las credenciales de DB están cargadas (deberían estarlo desde celdas anteriores)\n",
    "# Re-establecer conexión si es necesario o si el engine se cerró.\n",
    "engine_upload = None # Definir fuera del try para el finally\n",
    "\n",
    "try:\n",
    "    if not all([POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_HOST, POSTGRES_PORT, POSTGRES_DATABASE]):\n",
    "        raise ValueError(\"Credenciales de PostgreSQL no disponibles. Asegúrate de que se cargaron.\")\n",
    "        \n",
    "    DATABASE_URL_UPLOAD = f\"postgresql+psycopg2://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DATABASE}\"\n",
    "    engine_upload = create_engine(DATABASE_URL_UPLOAD)\n",
    "    \n",
    "    # Columnas especificadas y sus tipos SQL\n",
    "    # Asegúrate que los nombres de columna coincidan con los de df_cleaning (ya normalizados)\n",
    "    schema_definition = {\n",
    "        'id': 'BIGINT PRIMARY KEY', # id como clave primaria\n",
    "        'name': 'TEXT',\n",
    "        'host_id': 'BIGINT',\n",
    "        'neighbourhood_group': 'TEXT', # o VARCHAR(255)\n",
    "        'neighbourhood': 'TEXT',       # o VARCHAR(255)\n",
    "        'lat': 'DOUBLE PRECISION',\n",
    "        'long': 'DOUBLE PRECISION',\n",
    "        'country': 'TEXT',             # o VARCHAR(255)\n",
    "        'cancellation_policy': 'TEXT', # o VARCHAR(255)\n",
    "        'room_type': 'TEXT',           # o VARCHAR(255)\n",
    "        'construction_year': 'INTEGER',\n",
    "        'price': 'DOUBLE PRECISION',   # o DECIMAL(10,2)\n",
    "        'service_fee': 'DOUBLE PRECISION', # o DECIMAL(10,2)\n",
    "        'minimum_nights': 'INTEGER',\n",
    "        'number_of_reviews': 'INTEGER',\n",
    "        'last_review': 'TIMESTAMP WITHOUT TIME ZONE',\n",
    "        'reviews_per_month': 'DOUBLE PRECISION',\n",
    "        'review_rate_number': 'INTEGER', # Asumiendo que la categoría representa 1-5\n",
    "        'calculated_host_listings_count': 'INTEGER',\n",
    "        'availability_365': 'INTEGER',\n",
    "        'is_host_unconfirmed': 'BOOLEAN',\n",
    "        'is_host_verified': 'BOOLEAN',\n",
    "        'is_instant_bookable_false_policy': 'BOOLEAN',\n",
    "        'is_instant_bookable_true_policy': 'BOOLEAN'\n",
    "    }\n",
    "    \n",
    "    # Las columnas que quieres en tu tabla\n",
    "    target_columns_for_db = [\n",
    "        'id', 'name', 'host_id', 'neighbourhood_group', 'neighbourhood', 'lat', 'long', \n",
    "        'country', 'cancellation_policy', 'room_type', 'construction_year', \n",
    "        'price', 'service_fee', 'minimum_nights', 'number_of_reviews', 'last_review', \n",
    "        'reviews_per_month', 'review_rate_number', 'calculated_host_listings_count', \n",
    "        'availability_365', 'is_host_unconfirmed', 'is_host_verified', \n",
    "        'is_instant_bookable_false_policy', 'is_instant_bookable_true_policy'\n",
    "    ]\n",
    "\n",
    "    # Construir la sentencia CREATE TABLE\n",
    "    # Usar comillas dobles para nombres de columna por si acaso, aunque los normalizados no las necesitan\n",
    "    columns_sql_definitions = []\n",
    "    for col_name in target_columns_for_db:\n",
    "        if col_name in schema_definition:\n",
    "            columns_sql_definitions.append(f'\"{col_name}\" {schema_definition[col_name]}')\n",
    "        else:\n",
    "            # Fallback si una columna no está en schema_definition (debería estarlo)\n",
    "            logging.warning(f\"Tipo SQL no definido explícitamente para '{col_name}'. Se usará TEXT por defecto si `to_sql` infiere.\")\n",
    "            # Sin embargo, la creación explícita de abajo fallará si no está. Mejor asegurarse que todas están.\n",
    "            raise KeyError(f\"Tipo SQL no definido explícitamente para la columna requerida: '{col_name}'\")\n",
    "\n",
    "\n",
    "    create_table_query = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS \"{NEW_TABLE_NAME}\" (\n",
    "        {', '.join(columns_sql_definitions)}\n",
    "    );\n",
    "    \"\"\"\n",
    "    # Usamos IF NOT EXISTS para evitar error si la tabla ya existe.\n",
    "    # Si quieres reemplazarla siempre, usa DROP TABLE IF EXISTS \"{NEW_TABLE_NAME}\"; antes.\n",
    "    # Por ahora, vamos a hacer DROP y CREATE para asegurar una tabla limpia con el esquema correcto.\n",
    "\n",
    "    drop_table_query = f'DROP TABLE IF EXISTS \"{NEW_TABLE_NAME}\";'\n",
    "\n",
    "    with engine_upload.connect() as connection:\n",
    "        trans = connection.begin()\n",
    "        try:\n",
    "            logging.info(f\"Ejecutando: {drop_table_query}\")\n",
    "            connection.execute(text(drop_table_query)) # Necesitas importar text de sqlalchemy\n",
    "            logging.info(f\"Tabla '{NEW_TABLE_NAME}' eliminada si existía.\")\n",
    "            print(f\"Tabla '{NEW_TABLE_NAME}' eliminada si existía.\")\n",
    "            \n",
    "            logging.info(f\"Ejecutando: {create_table_query}\")\n",
    "            connection.execute(text(create_table_query)) # Necesitas importar text de sqlalchemy\n",
    "            trans.commit()\n",
    "            logging.info(f\"Tabla vacía '{NEW_TABLE_NAME}' creada exitosamente con el esquema definido.\")\n",
    "            print(f\"Tabla vacía '{NEW_TABLE_NAME}' creada exitosamente.\")\n",
    "        except Exception as e_exec:\n",
    "            trans.rollback()\n",
    "            logging.error(f\"Error al crear la tabla '{NEW_TABLE_NAME}': {e_exec}\")\n",
    "            raise e_exec # Relanzar para detener la ejecución si la creación falla\n",
    "            \n",
    "    # Importar text\n",
    "    from sqlalchemy import text\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error en la Celda 13.1 (conexión o definición de tabla): {e}\")\n",
    "    print(f\"Error en la Celda 13.1: {e}\")\n",
    "# finally: # Mantener el engine abierto para la siguiente celda de inserción\n",
    "#     if engine_upload:\n",
    "#         engine_upload.dispose()\n",
    "#         logging.info(\"Conexión del motor SQLAlchemy (engine_upload) dispuesta.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1033,
   "id": "6ed8e5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:21,619 - INFO - Celda 13.2: Preparando df_cleaning para la carga a 'cleaned_airbnb_listings'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Celda 13.2: Preparando df_cleaning ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:21,642 - INFO - df_to_upload creado con 24 columnas en el orden especificado.\n",
      "/tmp/ipykernel_1305523/3137182911.py:36: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if 'review_rate_number' in df_to_upload.columns and pd.api.types.is_categorical_dtype(df_to_upload['review_rate_number']):\n",
      "2025-05-17 02:01:21,653 - INFO - Columna 'review_rate_number' (category) convertida a Int64 para la carga a SQL.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_to_upload preparado con 102058 filas y 24 columnas.\n",
      "Primeras filas de df_to_upload:\n",
      "|      id | name                               |     host_id | neighbourhood_group   | neighbourhood   |     lat |     long | country       | cancellation_policy   | room_type       |   construction_year |   price |   service_fee |   minimum_nights |   number_of_reviews | last_review         |   reviews_per_month |   review_rate_number |   calculated_host_listings_count |   availability_365 | is_host_unconfirmed   | is_host_verified   | is_instant_bookable_false_policy   | is_instant_bookable_true_policy   |\n",
      "|--------:|:-----------------------------------|------------:|:----------------------|:----------------|--------:|---------:|:--------------|:----------------------|:----------------|--------------------:|--------:|--------------:|-----------------:|--------------------:|:--------------------|--------------------:|---------------------:|---------------------------------:|-------------------:|:----------------------|:-------------------|:-----------------------------------|:----------------------------------|\n",
      "| 1001254 | Clean & quiet apt home by the park | 80014485718 | Brooklyn              | Kensington      | 40.6475 | -73.9724 | United States | strict                | Private room    |                2020 |     966 |           193 |               10 |                   9 | 2021-10-19 00:00:00 |                0.21 |                    4 |                                6 |                286 | True                  | False              | True                               | False                             |\n",
      "| 1002102 | Skylit Midtown Castle              | 52335172823 | Manhattan             | Midtown         | 40.7536 | -73.9838 | United States | moderate              | Entire home/apt |                2007 |     142 |            28 |               30 |                  45 | 2022-05-21 00:00:00 |                0.38 |                    4 |                                2 |                228 | False                 | True               | True                               | False                             |\n",
      "\n",
      "Info de df_to_upload:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 102058 entries, 0 to 102057\n",
      "Data columns (total 24 columns):\n",
      " #   Column                            Non-Null Count   Dtype         \n",
      "---  ------                            --------------   -----         \n",
      " 0   id                                102058 non-null  int64         \n",
      " 1   name                              102058 non-null  object        \n",
      " 2   host_id                           102058 non-null  int64         \n",
      " 3   neighbourhood_group               102058 non-null  category      \n",
      " 4   neighbourhood                     102058 non-null  category      \n",
      " 5   lat                               102058 non-null  float64       \n",
      " 6   long                              102058 non-null  float64       \n",
      " 7   country                           102058 non-null  category      \n",
      " 8   cancellation_policy               102058 non-null  category      \n",
      " 9   room_type                         102058 non-null  category      \n",
      " 10  construction_year                 102058 non-null  Int64         \n",
      " 11  price                             102058 non-null  float64       \n",
      " 12  service_fee                       102058 non-null  float64       \n",
      " 13  minimum_nights                    102058 non-null  Int64         \n",
      " 14  number_of_reviews                 102058 non-null  Int64         \n",
      " 15  last_review                       102058 non-null  datetime64[ns]\n",
      " 16  reviews_per_month                 102058 non-null  float64       \n",
      " 17  review_rate_number                102058 non-null  Int64         \n",
      " 18  calculated_host_listings_count    102058 non-null  Int64         \n",
      " 19  availability_365                  102058 non-null  Int64         \n",
      " 20  is_host_unconfirmed               102058 non-null  bool          \n",
      " 21  is_host_verified                  102058 non-null  bool          \n",
      " 22  is_instant_bookable_false_policy  102058 non-null  bool          \n",
      " 23  is_instant_bookable_true_policy   102058 non-null  bool          \n",
      "dtypes: Int64(6), bool(4), category(5), datetime64[ns](1), float64(5), int64(2), object(1)\n",
      "memory usage: 13.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# Celda 13.2: Preparar df_cleaning para la Carga\n",
    "logging.info(f\"Celda 13.2: Preparando df_cleaning para la carga a '{NEW_TABLE_NAME}'.\")\n",
    "print(f\"\\n--- Celda 13.2: Preparando df_cleaning ---\")\n",
    "\n",
    "if not df_cleaning.empty:\n",
    "    # Columnas especificadas para la base de datos (mismas que target_columns_for_db de la celda anterior)\n",
    "    final_columns_for_upload = [\n",
    "        'id', 'name', 'host_id', 'neighbourhood_group', 'neighbourhood', 'lat', 'long', \n",
    "        'country', 'cancellation_policy', 'room_type', 'construction_year', \n",
    "        'price', 'service_fee', 'minimum_nights', 'number_of_reviews', 'last_review', \n",
    "        'reviews_per_month', 'review_rate_number', 'calculated_host_listings_count', \n",
    "        'availability_365', 'is_host_unconfirmed', 'is_host_verified', \n",
    "        'is_instant_bookable_false_policy', 'is_instant_bookable_true_policy'\n",
    "    ]\n",
    "\n",
    "    # Verificar si todas las columnas necesarias están en df_cleaning\n",
    "    missing_cols = [col for col in final_columns_for_upload if col not in df_cleaning.columns]\n",
    "    if missing_cols:\n",
    "        logging.error(f\"Faltan las siguientes columnas en df_cleaning para la carga: {missing_cols}\")\n",
    "        print(f\"ERROR: Faltan las siguientes columnas en df_cleaning: {missing_cols}\")\n",
    "        # Detener o manejar el error apropiadamente\n",
    "        raise ValueError(f\"Columnas faltantes en df_cleaning: {missing_cols}\")\n",
    "\n",
    "    # Crear un nuevo DataFrame solo con las columnas deseadas y en el orden correcto\n",
    "    df_to_upload = df_cleaning[final_columns_for_upload].copy()\n",
    "    logging.info(f\"df_to_upload creado con {df_to_upload.shape[1]} columnas en el orden especificado.\")\n",
    "\n",
    "    # Verificación de tipos y conversiones finales si son necesarias:\n",
    "    # Pandas Int64 (nullable int) se mapea bien a BIGINT/INTEGER en SQL.\n",
    "    # Pandas float64 se mapea bien a DOUBLE PRECISION/FLOAT.\n",
    "    # Pandas datetime64[ns] se mapea bien a TIMESTAMP.\n",
    "    # Pandas bool se mapea bien a BOOLEAN.\n",
    "    # Pandas category/object/string se mapean a TEXT/VARCHAR.\n",
    "    # La columna 'review_rate_number' es category, pero sus valores son numéricos.\n",
    "    # Si to_sql tiene problemas con category de números a INTEGER SQL, la convertimos a Int64 aquí.\n",
    "    if 'review_rate_number' in df_to_upload.columns and pd.api.types.is_categorical_dtype(df_to_upload['review_rate_number']):\n",
    "        try:\n",
    "            # Intentar convertir las categorías a Int64 si representan números\n",
    "            df_to_upload['review_rate_number'] = df_to_upload['review_rate_number'].astype('Int64')\n",
    "            logging.info(\"Columna 'review_rate_number' (category) convertida a Int64 para la carga a SQL.\")\n",
    "        except Exception as e_astype:\n",
    "            logging.warning(f\"No se pudo convertir 'review_rate_number' (category) a Int64: {e_astype}. Se cargará como texto si es posible.\")\n",
    "            # Si falla, se cargará como texto si el tipo SQL es TEXT/VARCHAR. Si es INTEGER, podría fallar la carga.\n",
    "\n",
    "\n",
    "    print(f\"df_to_upload preparado con {df_to_upload.shape[0]} filas y {df_to_upload.shape[1]} columnas.\")\n",
    "    print(\"Primeras filas de df_to_upload:\")\n",
    "    print(df_to_upload.head(2).to_markdown(index=False))\n",
    "    print(\"\\nInfo de df_to_upload:\")\n",
    "    df_to_upload.info()\n",
    "\n",
    "else:\n",
    "    logging.warning(\"El DataFrame df_cleaning está vacío. No hay datos para preparar o subir.\")\n",
    "    print(\"El DataFrame df_cleaning está vacío.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1034,
   "id": "0bd88edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:21,694 - INFO - Celda 13.3: Insertando datos de df_to_upload en la tabla 'cleaned_airbnb_listings'.\n",
      "2025-05-17 02:01:21,697 - INFO - Intentando insertar 102058 filas en 'cleaned_airbnb_listings'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Celda 13.3: Insertando Datos en 'cleaned_airbnb_listings' ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:27,016 - INFO - Datos insertados exitosamente en la tabla 'cleaned_airbnb_listings'.\n",
      "2025-05-17 02:01:27,017 - INFO - Conexión del motor SQLAlchemy (engine_upload) dispuesta después de la carga.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos insertados exitosamente en la tabla 'cleaned_airbnb_listings'.\n",
      "Motor de DB (engine_upload) dispuesto.\n"
     ]
    }
   ],
   "source": [
    "# Celda 13.3: Insertar Datos en la Nueva Tabla\n",
    "logging.info(f\"Celda 13.3: Insertando datos de df_to_upload en la tabla '{NEW_TABLE_NAME}'.\")\n",
    "print(f\"\\n--- Celda 13.3: Insertando Datos en '{NEW_TABLE_NAME}' ---\")\n",
    "\n",
    "# Reutilizar engine_upload si sigue disponible y la conexión no se cerró explícitamente\n",
    "# O recrearlo si es necesario (como está arriba, se crea en Celda 13.1 y se asume disponible)\n",
    "\n",
    "if 'df_to_upload' in locals() and not df_to_upload.empty and 'engine_upload' in locals() and engine_upload is not None:\n",
    "    try:\n",
    "        logging.info(f\"Intentando insertar {len(df_to_upload)} filas en '{NEW_TABLE_NAME}'.\")\n",
    "        # Usar if_exists='append' ya que la tabla fue creada vacía.\n",
    "        # chunksize puede ayudar con la memoria para DataFrames grandes.\n",
    "        df_to_upload.to_sql(\n",
    "            name=NEW_TABLE_NAME,\n",
    "            con=engine_upload,\n",
    "            if_exists='append',\n",
    "            index=False,\n",
    "            chunksize=1000 # Ajusta según el tamaño de tu DF y memoria\n",
    "        )\n",
    "        logging.info(f\"Datos insertados exitosamente en la tabla '{NEW_TABLE_NAME}'.\")\n",
    "        print(f\"Datos insertados exitosamente en la tabla '{NEW_TABLE_NAME}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al insertar datos en la tabla '{NEW_TABLE_NAME}': {e}\")\n",
    "        print(f\"Error al insertar datos en '{NEW_TABLE_NAME}': {e}\")\n",
    "    finally:\n",
    "        if engine_upload: # Cerrar el engine después de la operación final\n",
    "            engine_upload.dispose()\n",
    "            logging.info(\"Conexión del motor SQLAlchemy (engine_upload) dispuesta después de la carga.\")\n",
    "            print(\"Motor de DB (engine_upload) dispuesto.\")\n",
    "            \n",
    "elif 'df_to_upload' not in locals() or df_to_upload.empty:\n",
    "    logging.warning(\"df_to_upload no está definido o está vacío. No se insertaron datos.\")\n",
    "    print(\"df_to_upload no está definido o está vacío. No se insertaron datos.\")\n",
    "elif 'engine_upload' not in locals() or engine_upload is None:\n",
    "    logging.warning(\"El motor de base de datos (engine_upload) no está inicializado. No se pueden insertar datos.\")\n",
    "    print(\"Motor de DB (engine_upload) no inicializado. No se pueden insertar datos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "id": "569cf74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:27,025 - INFO - Celda 13.4: Verificando las primeras filas de la tabla 'cleaned_airbnb_listings' desde PostgreSQL.\n",
      "WARNING:  la base de datos «airbnb» tiene una discordancia de versión de ordenamiento (“collation”)\n",
      "DETAIL:  La base de datos fue creada usando la versión de ordenamiento 2.31, pero el sistema operativo provee la versión 2.35.\n",
      "HINT:  Reconstruya todos los objetos en esta base de datos que usen el ordenamiento por omisión y ejecute ALTER DATABASE airbnb REFRESH COLLATION VERSION, o construya PostgreSQL con la versión correcta de la biblioteca.\n",
      "2025-05-17 02:01:27,037 - INFO - Primeras 5 filas obtenidas de 'cleaned_airbnb_listings' en PostgreSQL:\n",
      "2025-05-17 02:01:27,040 - INFO - Conexión del motor SQLAlchemy (engine_verify) dispuesta.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Celda 13.4: Verificando Datos en 'cleaned_airbnb_listings' ---\n",
      "Primeras 5 filas de la tabla 'cleaned_airbnb_listings' consultadas desde PostgreSQL:\n",
      "|      id | name                                             |     host_id | neighbourhood_group   | neighbourhood   |     lat |     long | country       | cancellation_policy   | room_type       |   construction_year |   price |   service_fee |   minimum_nights |   number_of_reviews | last_review                |   reviews_per_month |   review_rate_number |   calculated_host_listings_count |   availability_365 | is_host_unconfirmed   | is_host_verified   | is_instant_bookable_false_policy   | is_instant_bookable_true_policy   |\n",
      "|--------:|:-------------------------------------------------|------------:|:----------------------|:----------------|--------:|---------:|:--------------|:----------------------|:----------------|--------------------:|--------:|--------------:|-----------------:|--------------------:|:---------------------------|--------------------:|---------------------:|---------------------------------:|-------------------:|:----------------------|:-------------------|:-----------------------------------|:----------------------------------|\n",
      "| 1001254 | Clean & quiet apt home by the park               | 80014485718 | Brooklyn              | Kensington      | 40.6475 | -73.9724 | United States | strict                | Private room    |                2020 |     966 |           193 |               10 |                   9 | 2021-10-19 00:00:00        |             0.21    |                    4 |                                6 |                286 | True                  | False              | True                               | False                             |\n",
      "| 1002102 | Skylit Midtown Castle                            | 52335172823 | Manhattan             | Midtown         | 40.7536 | -73.9838 | United States | moderate              | Entire home/apt |                2007 |     142 |            28 |               30 |                  45 | 2022-05-21 00:00:00        |             0.38    |                    4 |                                2 |                228 | False                 | True               | True                               | False                             |\n",
      "| 1002403 | THE VILLAGE OF HARLEM....NEW YORK !              | 78829239556 | Manhattan             | Harlem          | 40.809  | -73.9419 | United States | flexible              | Private room    |                2005 |     620 |           124 |                3 |                   0 | 2019-06-10 18:47:53.419777 |             1.98936 |                    5 |                                1 |                352 | False                 | False              | False                              | True                              |\n",
      "| 1002755 | Desconocido                                      | 85098326012 | Brooklyn              | Clinton Hill    | 40.6851 | -73.9598 | United States | moderate              | Entire home/apt |                2005 |     368 |            74 |               30 |                 270 | 2019-07-05 00:00:00        |             4.64    |                    4 |                                1 |                322 | True                  | False              | False                              | True                              |\n",
      "| 1003689 | Entire Apt: Spacious Studio/Loft by central park | 92037596077 | Manhattan             | East Harlem     | 40.7985 | -73.944  | United States | moderate              | Entire home/apt |                2009 |     204 |            41 |               10 |                   9 | 2018-11-19 00:00:00        |             0.1     |                    3 |                                1 |                289 | False                 | True               | True                               | False                             |\n",
      "Motor de DB (engine_verify) dispuesto.\n"
     ]
    }
   ],
   "source": [
    "# Celda 13.4: Verificar Datos en PostgreSQL (Opcional)\n",
    "logging.info(f\"Celda 13.4: Verificando las primeras filas de la tabla '{NEW_TABLE_NAME}' desde PostgreSQL.\")\n",
    "print(f\"\\n--- Celda 13.4: Verificando Datos en '{NEW_TABLE_NAME}' ---\")\n",
    "\n",
    "engine_verify = None\n",
    "try:\n",
    "    # Se necesita un nuevo engine o reutilizar uno si no se cerró\n",
    "    if not all([POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_HOST, POSTGRES_PORT, POSTGRES_DATABASE]):\n",
    "        raise ValueError(\"Credenciales de PostgreSQL no disponibles para verificación.\")\n",
    "\n",
    "    DATABASE_URL_VERIFY = f\"postgresql+psycopg2://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DATABASE}\"\n",
    "    engine_verify = create_engine(DATABASE_URL_VERIFY)\n",
    "\n",
    "    query_verify = f'SELECT * FROM \"{NEW_TABLE_NAME}\" LIMIT 5;'\n",
    "    df_from_db_cleaned = pd.read_sql_query(text(query_verify), con=engine_verify) # Usar text()\n",
    "    \n",
    "    logging.info(f\"Primeras 5 filas obtenidas de '{NEW_TABLE_NAME}' en PostgreSQL:\")\n",
    "    print(f\"Primeras 5 filas de la tabla '{NEW_TABLE_NAME}' consultadas desde PostgreSQL:\")\n",
    "    if not df_from_db_cleaned.empty:\n",
    "        print(df_from_db_cleaned.to_markdown(index=False))\n",
    "    else:\n",
    "        logging.info(\"La consulta no devolvió filas o la tabla está vacía después de la carga.\")\n",
    "        print(\"La tabla parece estar vacía o la consulta no devolvió resultados.\")\n",
    "            \n",
    "except Exception as e:\n",
    "    logging.error(f\"Error al consultar datos desde la tabla '{NEW_TABLE_NAME}' en PostgreSQL: {e}\")\n",
    "    print(f\"Error al consultar datos desde '{NEW_TABLE_NAME}': {e}\")\n",
    "finally:\n",
    "    if engine_verify:\n",
    "        engine_verify.dispose()\n",
    "        logging.info(\"Conexión del motor SQLAlchemy (engine_verify) dispuesta.\")\n",
    "        print(\"Motor de DB (engine_verify) dispuesto.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1036,
   "id": "606ddf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 02:01:27,046 - INFO - Conexiones del motor de SQLAlchemy dispuestas (cerradas).\n"
     ]
    }
   ],
   "source": [
    "engine.dispose() # Cerrar todas las conexiones en el pool del engine\n",
    "logging.info(\"Conexiones del motor de SQLAlchemy dispuestas (cerradas).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
